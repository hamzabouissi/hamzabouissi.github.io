<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Bind9 on chown u&#43;r mind</title>
    <link>//localhost:1313/tags/bind9/</link>
    <description>Recent content in Bind9 on chown u&#43;r mind</description>
    <image>
      <title>chown u&#43;r mind</title>
      <url>//localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>//localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- 0.152.2</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 20 May 2023 16:12:38 +0100</lastBuildDate>
    <atom:link href="//localhost:1313/tags/bind9/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Deploying gitea into kubernetes with custom domain</title>
      <link>//localhost:1313/posts/deploy-gitea-k8s/</link>
      <pubDate>Sat, 20 May 2023 16:12:38 +0100</pubDate>
      <guid>//localhost:1313/posts/deploy-gitea-k8s/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Hello, lately I have been trying to deploy a custom Docker image into my local Kubernetes cluster. It turned out I needed to host my Docker image on a container registry, either Docker Hub, which is not suitable for my use case, or deploy and use a local registry. During my research, I found Gitea, which I liked as it allows me to deploy all my projects on it and also host the containers.&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p>Hello, lately I have been trying to deploy a custom Docker image into my local Kubernetes cluster. It turned out I needed to host my Docker image on a container registry, either Docker Hub, which is not suitable for my use case, or deploy and use a local registry. During my research, I found Gitea, which I liked as it allows me to deploy all my projects on it and also host the containers.</p>
<h2 id="prerequisite">Prerequisite</h2>
<pre><code>* kubernetes cluster
* external server(S3,NFS) for dynamic provisioning
* metallb installed
</code></pre>
<h2 id="create-pvc-for-nfs-server">Create PVC for NFS Server</h2>
<p>With the help of Proxmox, I created a VM and configured it as an NFS server on 192.168.1.109. To use this server in our Kubernetes cluster, we need to create a StorageClass and then create a PVC that points to that class so pods can use it.</p>
<p><img alt="pvc" loading="lazy" src="/img/pvc.png"></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">helm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --create-namespace <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --namespace nfs-provisioner <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --set nfs.server<span class="o">=</span>192.168.1.109 <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --set nfs.path<span class="o">=</span>/srv/public/nfs
</span></span></code></pre></td></tr></table>
</div>
</div><p>Then we create a PVC, linking it to the storageClassName:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">PersistentVolumeClaim</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nfs-test</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">storage.k8s.io/name</span><span class="p">:</span><span class="w"> </span><span class="l">nfs</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">storage.k8s.io/part-of</span><span class="p">:</span><span class="w"> </span><span class="l">kubernetes-complete-reference</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">accessModes</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="l">ReadWriteMany</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">storageClassName</span><span class="p">:</span><span class="w"> </span><span class="l">nfs-client</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">resources</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">requests</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">storage</span><span class="p">:</span><span class="w"> </span><span class="l">15Gi</span><span class="w">
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>If we want the deployment to store its volume in NFS, we define volumes with the argument persistentVolumeClaim = nfs-test. Here is an example:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Pod</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">task-pv-pod</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">volumes</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">task-pv-storage</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">persistentVolumeClaim</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">claimName</span><span class="p">:</span><span class="w"> </span><span class="l">nfs-test</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="l">...</span><span class="w">
</span></span></span></code></pre></td></tr></table>
</div>
</div><h2 id="deploy-gitea--postgresql--redis">Deploy Gitea + PostgreSQL + Redis</h2>
<p>To facilitate deploying resources into Kubernetes, we use Helm. With one command and changing a few values, we can deploy our resources. Before deploying, when I was reading the Gitea chart, I noticed Gitea requires PostgreSQL and Redis to be deployed alongside it to save its configs and states.</p>
<p>So let&rsquo;s create a file &lsquo;values-gitea.yaml&rsquo; and add the default chart values.</p>
<p>Then change the following values:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">persistence</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">create</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">claimName</span><span class="p">:</span><span class="w"> </span><span class="l">nfs-test</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">postgresql-ha</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">enabled</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">postgresql</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">enabled</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>I disabled the deployment of Postgres-HA because I didn&rsquo;t need it for my use case. However, if you&rsquo;re deploying for your organization where multiple users are pushing and pulling, you may keep it enabled.</p>
<p>Now to deploy the Helm release with the modifications, run:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">  helm install gitea gitea-charts/gitea --values values-gitea.yaml
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p><strong><em>NOTE:</em></strong> You need a cluster with an internet connection to pull the Docker images.</p>
</blockquote>
<blockquote>
<p><strong><em>NOTE:</em></strong> If you don&rsquo;t specify the namespace, it will choose the &lsquo;default&rsquo; namespace.</p>
</blockquote>
<p>Now, we wait until the images get pulled and deployed. You can watch the pods by running:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">  kubectl get pods -w
</span></span></code></pre></td></tr></table>
</div>
</div><p><img alt="k9s pods" loading="lazy" src="/img/k9s_screenshoot.png"></p>
<p>After all the pods are deployed, to access Gitea, we need to either open a port or create an ingress. Let&rsquo;s try the port-forwarding mechanism for now to test the app:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl port-forward service/gitea-http 3000:3000
</span></span></code></pre></td></tr></table>
</div>
</div><p>Now go to <a href="http://localhost:3000">http://localhost:3000</a> and test, you can login as gitea_admin, password: r8sA8CPHD9!bt6d</p>
<p>To access Gitea from another pod, CoreDNS provides a default resolving mechanism in the form: service_name.namespace.svc.cluster.local. In our example, the DNS for Gitea is: gitea-http.default.</p>
<h2 id="deploy-controller-nginx">Deploy Controller Nginx</h2>
<p>For testing purposes, port-forwarding may be a good solution, but if we want a more reliable solution and even attach a domain with HTTPS to the Gitea service, we need an ingress.</p>
<p>To start with ingress, an ingress controller is needed. We will choose the most popular one: nginx-ingress.</p>
<p>following this <a href="https://docs.nginx.com/nginx-ingress-controller/installation/installing-nic/installation-with-helm/">article</a>, choosing helm install, I got the below command to run:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">  helm install my-release oci://ghcr.io/nginxinc/charts/nginx-ingress --version 1.2.1
</span></span></code></pre></td></tr></table>
</div>
</div><p>If everything works as expected, you should see an ingress-controller service with an IP address from your load-balancer (MetalLB) pool of IP addresses.</p>
<p><img alt="nginx-ingress service" loading="lazy" src="/img/nginx_controller.png"></p>
<p>Great, to expose Gitea, we will change the previous chart file &lsquo;values-gitea.yaml&rsquo; values:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="w">  </span><span class="nt">ingress</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">enabled</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">className</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">hosts</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="nt">host</span><span class="p">:</span><span class="w"> </span><span class="l">gitea.homelab.local</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">paths</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span>- <span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l">/</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">pathType</span><span class="p">:</span><span class="w"> </span><span class="l">Prefix</span><span class="w">
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>here we instructed to create an ingress rule:</p>
<pre><code>- className is needed if you have multiple ingress controllers installed
- the host part tell ingress to accept any request with domain or  host header : gitea.homelab.local and forward it to gitea instance  
</code></pre>
<p>To redeploy the release with the new configuration, we run:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">helm upgrade gitea gitea-charts/gitea --values values-gitea.yaml
</span></span></code></pre></td></tr></table>
</div>
</div><p>If we check the ingresses, we can find Gitea ingress has been created. To test it, we will query the IP address of the ingress, supplying a custom host header: gitea.homelab.local.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">curl --header <span class="s1">&#39;Host: gitea.homelab.local&#39;</span> ingress_ip_address
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="deploy-bind9">Deploy bind9</h2>
<p>You may notice that accessing Gitea from a browser isn&rsquo;t possible because the local DNS server doesn&rsquo;t have knowledge of the domain: homelab.local. The solution is either to modify the /etc/hosts file or create a CT in Proxmox and host a DNS server there.</p>
<p>I went for the second option, hosting a DNS server because my homelab may require a variety of services in the future, and I want them to be mapped to a domain for all the connected devices in my network.</p>
<p>For the DNS server, Pi-hole may be the most popular option for ad-blocking and adding DNS records, but I experienced a few bugs with serving DNS, so I went with the second option: Bind9.</p>
<p>I read this <a href="https://www.digitalocean.com/community/tutorials/how-to-configure-bind-as-a-private-network-dns-server-on-ubuntu-20-04#step-2-configuring-the-primary-dns-server">article</a></p>
<p>I created a CT in proxmox and assigned a static ip 192.168.1.114, don&rsquo;t use dhcp because it may change if CT restarted. So here are my configuration</p>
<p><strong>my local ip address</strong>: 192.168.1.104, <strong>ingress ip address</strong>: 192.168.1.148</p>
<p>filename: /etc/bind/named.conf.options</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-text" data-lang="text"><span class="line"><span class="cl">acl &#34;trusted&#34; {
</span></span><span class="line"><span class="cl">  192.168.1.0/24;
</span></span><span class="line"><span class="cl">};
</span></span><span class="line"><span class="cl">options {
</span></span><span class="line"><span class="cl">  directory &#34;/var/cache/bind&#34;;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  // If there is a firewall between you and nameservers you want
</span></span><span class="line"><span class="cl">  // to talk to, you may need to fix the firewall to allow multiple
</span></span><span class="line"><span class="cl">  // ports to talk.  See http://www.kb.cert.org/vuls/id/800113
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  // If your ISP provided one or more IP addresses for stable
</span></span><span class="line"><span class="cl">  // nameservers, you probably want to use them as forwarders.
</span></span><span class="line"><span class="cl">  // Uncomment the following block, and insert the addresses replacing
</span></span><span class="line"><span class="cl">  // the all-0&#39;s placeholder.
</span></span><span class="line"><span class="cl">  recursion yes;
</span></span><span class="line"><span class="cl">  allow-recursion { trusted; };
</span></span><span class="line"><span class="cl">  listen-on { 192.168.1.114;};
</span></span><span class="line"><span class="cl">  allow-transfer { none; };
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  // forwarders {
</span></span><span class="line"><span class="cl">  //      0.0.0.0;
</span></span><span class="line"><span class="cl">  // };
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  //========================================================================
</span></span><span class="line"><span class="cl">  // If BIND logs error messages about the root key being expired,
</span></span><span class="line"><span class="cl">  // you will need to update your keys.  See https://www.isc.org/bind-keys
</span></span><span class="line"><span class="cl">  //========================================================================
</span></span><span class="line"><span class="cl">  dnssec-validation auto;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  listen-on-v6 { any; };
</span></span><span class="line"><span class="cl">};
</span></span></code></pre></td></tr></table>
</div>
</div><p>filename: /etc/bind/named.conf.local</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-text" data-lang="text"><span class="line"><span class="cl">zone &#34;homelab.local&#34; {
</span></span><span class="line"><span class="cl">    type master;
</span></span><span class="line"><span class="cl">    file &#34;/etc/bind/zones/db.homelab.local&#34;;
</span></span><span class="line"><span class="cl">};
</span></span><span class="line"><span class="cl">zone &#34;168.192.in-addr.arpa&#34; {
</span></span><span class="line"><span class="cl">    type primary;
</span></span><span class="line"><span class="cl">    file &#34;/etc/bind/zones/db.192.168&#34;;  # 192.168.0.0/24 subnet
</span></span><span class="line"><span class="cl">};
</span></span></code></pre></td></tr></table>
</div>
</div><p>filename: zones/db.homelab.local</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-text" data-lang="text"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">;
</span></span><span class="line"><span class="cl">; BIND data file for local loopback interface
</span></span><span class="line"><span class="cl">;
</span></span><span class="line"><span class="cl">$TTL    604800
</span></span><span class="line"><span class="cl">@       IN      SOA     homelab.local. admin.homelab.local. (
</span></span><span class="line"><span class="cl">                              3         ; Serial
</span></span><span class="line"><span class="cl">                         604800         ; Refresh
</span></span><span class="line"><span class="cl">                          86400         ; Retry
</span></span><span class="line"><span class="cl">                        2419200         ; Expire
</span></span><span class="line"><span class="cl">                         604800 )       ; Negative Cache TTL
</span></span><span class="line"><span class="cl">;
</span></span><span class="line"><span class="cl">; name servers - NS records
</span></span><span class="line"><span class="cl">    IN      NS      ns1.homelab.local.
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">; name servers - A records
</span></span><span class="line"><span class="cl">ns1.homelab.local.            IN      A       192.168.1.114
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">; name servers - A records
</span></span><span class="line"><span class="cl">gitea.homelab.local.          IN      A       192.168.1.148
</span></span></code></pre></td></tr></table>
</div>
</div><p>filename: /etc/bind/zones/db.168.192</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-text" data-lang="text"><span class="line"><span class="cl">;
</span></span><span class="line"><span class="cl">; BIND reverse data file for local loopback interface
</span></span><span class="line"><span class="cl">;
</span></span><span class="line"><span class="cl">$TTL    604800
</span></span><span class="line"><span class="cl">@       IN      SOA     ns1.homelab.local. admin.homelab.local. (
</span></span><span class="line"><span class="cl">                              3         ; Serial
</span></span><span class="line"><span class="cl">                         604800         ; Refresh
</span></span><span class="line"><span class="cl">                          86400         ; Retry
</span></span><span class="line"><span class="cl">                        2419200         ; Expire
</span></span><span class="line"><span class="cl">                         604800 )       ; Negative Cache TTL
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">; name servers - NS records
</span></span><span class="line"><span class="cl">      IN      NS      ns1.homelab.local.
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">; PTR Records
</span></span><span class="line"><span class="cl">114.1   IN      PTR     ns1.homelab.local.    ; 192.168.1.114
</span></span><span class="line"><span class="cl">148.1   IN      PTR     gitea.homelab.local.  ; 192.168.1.148
</span></span></code></pre></td></tr></table>
</div>
</div><p>once the the bind9 configured and it&rsquo;s working, we need to add the dns server ip address as an additional one, am using NetworkManager:</p>
<p><img alt="network_manager" loading="lazy" src="/img/network_manager_dns.png"></p>
<h2 id="add-tls">Add TLS</h2>
<p>Now, if we try to login into to the Gitea registry using the below command:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">docker login gitea.homelab.local
</span></span></code></pre></td></tr></table>
</div>
</div><p>It will return an error claiming the registry domain needs a TLS certificate. We can work around that by adding the registry domain to /etc/docker/daemon.json, but it would be more useful if we create a TLS certificate and append it to the domain.</p>
<p>We will start first by creating the cert. I chose mkcert because my first search led to it ðŸ˜„.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">mkcert gitea.homelab.local
</span></span></code></pre></td></tr></table>
</div>
</div><p>It will generate two PEM files: a public key and a private key.</p>
<p>We will create a TLS secret and append the two created files from mkcert:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl create secret tls gitea-secret <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --key gitea.homelab.local-key.pem <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --cert gitea.homelab.local.pem
</span></span></code></pre></td></tr></table>
</div>
</div><p>Finally, we append the gitea-secret into the ingress by changing the gitea-values.yaml file:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">tls</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">secretName</span><span class="p">:</span><span class="w"> </span><span class="l">gitea-secret</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">hosts</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">       </span>- <span class="l">gitea.homelab.local</span><span class="w">
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>Now, we can visit gitea.homelab.local and login to gitea registry without issues.</p>
<p><img alt="docker-login" loading="lazy" src="/img/docker_login.png"></p>
<h2 id="change-nginx-config-for-pushing-the-image">Change nginx config for pushing the image</h2>
<p>We deployed Gitea with one main purpose in mind: pushing containers to the registry. However, if we try building a local image and pushing it, you may face an error saying: &ldquo;413 Request Entity Too Large&rdquo;!</p>
<p><img alt="413_push_image" loading="lazy" src="/img/docker_push_413.png"></p>
<p>This is because by default Nginx imposes a limit of 1MB for uploading media files. To change that, we add an annotation for ingress to remove the limit:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">annotations</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">nginx.org/client-max-body-size</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;0&#34;</span><span class="w">
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>then we update the release chart</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">helm upgrade gitea gitea-charts/gitea --values values-gitea.yaml
</span></span></code></pre></td></tr></table>
</div>
</div><p>Now, we can push the image: gitlab.homelab.local/gitea_admin/app:latest</p>
<p><img alt="pushed_image" loading="lazy" src="/img/pushed_image.png"></p>
<p>if you have created another user instead of gitea_admin, you can replace it in the above command.</p>
<h2 id="add-bind9-server-in-coredns">Add Bind9 Server in CoreDNS</h2>
<p>We have done everything from deploying to adding TLS cert, but if we tried to create a deployment with the deployed image as an example</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">apps/v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">app-deployment</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">app</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">app</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">template</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">app</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">app</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">gitea.homelab.local/gitea_admin/app:latest</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>after applying the yaml, if you run kubectl describe deployment/app_name
you may notice in the events section that it&rsquo;s stating pulling the image has failed, that&rsquo;s logical because kubernetes cluster doesn&rsquo;t know about our custom domain: <strong>homelab.local</strong>.</p>
<p>So to let kubernetes DNS server: CoreDNS, acknowledge our domain we gonna need a litle tweak into the CoreDNS config</p>
<p>we run the following command to open the editor with configmap:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl edit configmap -n kube-system coredns
</span></span></code></pre></td></tr></table>
</div>
</div><p>and then we add the reference to homelab.local</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">ConfigMap</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">coredns</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">kube-system</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">data</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">Corefile</span><span class="p">:</span><span class="w"> </span><span class="p">|</span><span class="sd">
</span></span></span><span class="line"><span class="cl"><span class="sd">    .:53 {
</span></span></span><span class="line"><span class="cl"><span class="sd">        errors
</span></span></span><span class="line"><span class="cl"><span class="sd">        health
</span></span></span><span class="line"><span class="cl"><span class="sd">        kubernetes cluster.local in-addr.arpa ip6.arpa {
</span></span></span><span class="line"><span class="cl"><span class="sd">           pods insecure
</span></span></span><span class="line"><span class="cl"><span class="sd">           fallthrough in-addr.arpa ip6.arpa
</span></span></span><span class="line"><span class="cl"><span class="sd">        }
</span></span></span><span class="line"><span class="cl"><span class="sd">        prometheus :9153
</span></span></span><span class="line"><span class="cl"><span class="sd">        forward . 172.16.0.1
</span></span></span><span class="line"><span class="cl"><span class="sd">        cache 30
</span></span></span><span class="line"><span class="cl"><span class="sd">        loop
</span></span></span><span class="line"><span class="cl"><span class="sd">        reload
</span></span></span><span class="line"><span class="cl"><span class="sd">        loadbalance
</span></span></span><span class="line"><span class="cl"><span class="sd">    }
</span></span></span><span class="line"><span class="cl"><span class="sd">    homalb.local:53 {
</span></span></span><span class="line"><span class="cl"><span class="sd">        errors
</span></span></span><span class="line"><span class="cl"><span class="sd">        cache 30
</span></span></span><span class="line"><span class="cl"><span class="sd">        forward . 192.168.1.114
</span></span></span><span class="line"><span class="cl"><span class="sd">    }  </span><span class="w">
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>and for the CoreDNS to take effect, we will restart it with :</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl rollout restart -n kube-system deployment/coredns
</span></span></code></pre></td></tr></table>
</div>
</div><p>Now, to test things out you can redeploy the previous deployed yaml or just run an alpine with nslookup</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl run --image<span class="o">=</span>alpine:3.5 -it alpine-shell-1 -- nslookup gitea.homelab.local
</span></span></code></pre></td></tr></table>
</div>
</div><p>it should return the ip address of the ingress.</p>
]]></content:encoded>
    </item>
  </channel>
</rss>
