[{"content":"Introduction So that shiny day I got a project to do, and it required deploying a mongodb cluster. few searches and I found percona operator, moved into installation section and copied the helm install command.\n","permalink":"//localhost:1313/posts/etcd_went_down/","summary":"Introduction So that shiny day I got a project to do, and it required deploying a mongodb cluster. few searches and I found percona operator, moved into installation section and copied the helm install command.","title":"Oops...Etcd went down"},{"content":"Introudction Hello, lately I have been trying to deploy a custom Docker image into my local Kubernetes cluster. It turned out I needed to host my Docker image on a container registry, either Docker Hub, which is not suitable for my use case, or deploy and use a local registry. During my research, I found Gitea, which I liked as it allows me to deploy all my projects on it and also host the containers.\nPrerequisite * kubernetes cluster * external server(S3,NFS) for dynamic provisioning * metallb installed Create PVC for NFS Server With the help of Proxmox, I created a VM and configured it as an NFS server on 192.168.1.109. To use this server in our Kubernetes cluster, we need to create a StorageClass and then create a PVC that points to that class so pods can use it.\n1 2 3 4 5 helm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\ --create-namespace \\ --namespace nfs-provisioner \\ --set nfs.server=192.168.1.109 \\ --set nfs.path=/srv/public/nfs Then we create a PVC, linking it to the storageClassName:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: nfs-test labels: storage.k8s.io/name: nfs storage.k8s.io/part-of: kubernetes-complete-reference spec: accessModes: - ReadWriteMany storageClassName: nfs-client resources: requests: storage: 15Gi If we want the deployment to store its volume in NFS, we define volumes with the argument persistentVolumeClaim = nfs-test. Here is an example:\n1 2 3 4 5 6 7 8 9 10 apiVersion: v1 kind: Pod metadata: name: task-pv-pod spec: volumes: - name: task-pv-storage persistentVolumeClaim: claimName: nfs-test ... Deploy Gitea + PostgreSQL + Redis To facilitate deploying resources into Kubernetes, we use Helm. With one command and changing a few values, we can deploy our resources. Before deploying, when I was reading the Gitea chart, I noticed Gitea requires PostgreSQL and Redis to be deployed alongside it to save its configs and states.\nSo let\u0026rsquo;s create a file \u0026lsquo;values-gitea.yaml\u0026rsquo; and add the default chart values.\nThen change the following values:\n1 2 3 4 5 6 7 8 9 persistence: create: false claimName: nfs-test postgresql-ha: enabled: false postgresql: enabled: true I disabled the deployment of Postgres-HA because I didn\u0026rsquo;t need it for my use case. However, if you\u0026rsquo;re deploying for your organization where multiple users are pushing and pulling, you may keep it enabled.\nNow to deploy the Helm release with the modifications, run:\n1 helm install gitea gitea-charts/gitea --values values-gitea.yaml NOTE: You need a cluster with an internet connection to pull the Docker images.\nNOTE: If you don\u0026rsquo;t specify the namespace, it will choose the \u0026lsquo;default\u0026rsquo; namespace.\nNow, we wait until the images get pulled and deployed. You can watch the pods by running:\n1 kubectl get pods -w After all the pods are deployed, to access Gitea, we need to either open a port or create an ingress. Let\u0026rsquo;s try the port-forwarding mechanism for now to test the app:\n1 kubectl port-forward service/gitea-http 3000:3000 Now go to http://localhost:3000 and test, you can login as gitea_admin, password: r8sA8CPHD9!bt6d\nTo access Gitea from another pod, CoreDNS provides a default resolving mechanism in the form: service_name.namespace.svc.cluster.local. In our example, the DNS for Gitea is: gitea-http.default.\nDeploy Controller Nginx For testing purposes, port-forwarding may be a good solution, but if we want a more reliable solution and even attach a domain with HTTPS to the Gitea service, we need an ingress.\nTo start with ingress, an ingress controller is needed. We will choose the most popular one: nginx-ingress.\nfollowing this article, choosing helm install, I got the below command to run:\n1 helm install my-release oci://ghcr.io/nginxinc/charts/nginx-ingress --version 1.2.1 If everything works as expected, you should see an ingress-controller service with an IP address from your load-balancer (MetalLB) pool of IP addresses.\nGreat, to expose Gitea, we will change the previous chart file \u0026lsquo;values-gitea.yaml\u0026rsquo; values:\n1 2 3 4 5 6 7 8 ingress: enabled: true className: nginx hosts: - host: gitea.homelab.local paths: - path: / pathType: Prefix here we instructed to create an ingress rule:\n- className is needed if you have multiple ingress controllers installed - the host part tell ingress to accept any request with domain or host header : gitea.homelab.local and forward it to gitea instance To redeploy the release with the new configuration, we run:\n1 helm upgrade gitea gitea-charts/gitea --values values-gitea.yaml If we check the ingresses, we can find Gitea ingress has been created. To test it, we will query the IP address of the ingress, supplying a custom host header: gitea.homelab.local.\n1 curl --header \u0026#39;Host: gitea.homelab.local\u0026#39; ingress_ip_address Deploy bind9 You may notice that accessing Gitea from a browser isn\u0026rsquo;t possible because the local DNS server doesn\u0026rsquo;t have knowledge of the domain: homelab.local. The solution is either to modify the /etc/hosts file or create a CT in Proxmox and host a DNS server there.\nI went for the second option, hosting a DNS server because my homelab may require a variety of services in the future, and I want them to be mapped to a domain for all the connected devices in my network.\nFor the DNS server, Pi-hole may be the most popular option for ad-blocking and adding DNS records, but I experienced a few bugs with serving DNS, so I went with the second option: Bind9.\nI read this article\nI created a CT in proxmox and assigned a static ip 192.168.1.114, don\u0026rsquo;t use dhcp because it may change if CT restarted. So here are my configuration\nmy local ip address: 192.168.1.104, ingress ip address: 192.168.1.148\nfilename: /etc/bind/named.conf.options\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 acl \u0026#34;trusted\u0026#34; { 192.168.1.0/24; }; options { directory \u0026#34;/var/cache/bind\u0026#34;; // If there is a firewall between you and nameservers you want // to talk to, you may need to fix the firewall to allow multiple // ports to talk. See http://www.kb.cert.org/vuls/id/800113 // If your ISP provided one or more IP addresses for stable // nameservers, you probably want to use them as forwarders. // Uncomment the following block, and insert the addresses replacing // the all-0\u0026#39;s placeholder. recursion yes; allow-recursion { trusted; }; listen-on { 192.168.1.114;}; allow-transfer { none; }; // forwarders { // 0.0.0.0; // }; //======================================================================== // If BIND logs error messages about the root key being expired, // you will need to update your keys. See https://www.isc.org/bind-keys //======================================================================== dnssec-validation auto; listen-on-v6 { any; }; }; filename: /etc/bind/named.conf.local\n1 2 3 4 5 6 7 8 zone \u0026#34;homelab.local\u0026#34; { type master; file \u0026#34;/etc/bind/zones/db.homelab.local\u0026#34;; }; zone \u0026#34;168.192.in-addr.arpa\u0026#34; { type primary; file \u0026#34;/etc/bind/zones/db.192.168\u0026#34;; # 192.168.0.0/24 subnet }; filename: zones/db.homelab.local\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 ; ; BIND data file for local loopback interface ; $TTL 604800 @ IN SOA homelab.local. admin.homelab.local. ( 3 ; Serial 604800 ; Refresh 86400 ; Retry 2419200 ; Expire 604800 ) ; Negative Cache TTL ; ; name servers - NS records IN NS ns1.homelab.local. ; name servers - A records ns1.homelab.local. IN A 192.168.1.114 ; name servers - A records gitea.homelab.local. IN A 192.168.1.148 filename: /etc/bind/zones/db.168.192\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ; ; BIND reverse data file for local loopback interface ; $TTL 604800 @ IN SOA ns1.homelab.local. admin.homelab.local. ( 3 ; Serial 604800 ; Refresh 86400 ; Retry 2419200 ; Expire 604800 ) ; Negative Cache TTL ; name servers - NS records IN NS ns1.homelab.local. ; PTR Records 114.1 IN PTR ns1.homelab.local. ; 192.168.1.114 148.1 IN PTR gitea.homelab.local. ; 192.168.1.148 once the the bind9 configured and it\u0026rsquo;s working, we need to add the dns server ip address as an additional one, am using NetworkManager:\nAdd TLS Now, if we try to login into to the Gitea registry using the below command:\n1 docker login gitea.homelab.local It will return an error claiming the registry domain needs a TLS certificate. We can work around that by adding the registry domain to /etc/docker/daemon.json, but it would be more useful if we create a TLS certificate and append it to the domain.\nWe will start first by creating the cert. I chose mkcert because my first search led to it üòÑ.\n1 mkcert gitea.homelab.local It will generate two PEM files: a public key and a private key.\nWe will create a TLS secret and append the two created files from mkcert:\n1 2 3 kubectl create secret tls gitea-secret \\ --key gitea.homelab.local-key.pem \\ --cert gitea.homelab.local.pem Finally, we append the gitea-secret into the ingress by changing the gitea-values.yaml file:\n1 2 3 4 tls: - secretName: gitea-secret hosts: - gitea.homelab.local Now, we can visit gitea.homelab.local and login to gitea registry without issues.\nChange nginx config for pushing the image We deployed Gitea with one main purpose in mind: pushing containers to the registry. However, if we try building a local image and pushing it, you may face an error saying: \u0026ldquo;413 Request Entity Too Large\u0026rdquo;!\nThis is because by default Nginx imposes a limit of 1MB for uploading media files. To change that, we add an annotation for ingress to remove the limit:\n1 2 annotations: nginx.org/client-max-body-size: \u0026#34;0\u0026#34; then we update the release chart\n1 helm upgrade gitea gitea-charts/gitea --values values-gitea.yaml Now, we can push the image: gitlab.homelab.local/gitea_admin/app:latest\nif you have created another user instead of gitea_admin, you can replace it in the above command.\nAdd Bind9 Server in CoreDNS We have done everything from deploying to adding TLS cert, but if we tried to create a deployment with the deployed image as an example\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion: apps/v1 kind: Deployment metadata: name: app-deployment labels: app: app spec: selector: matchLabels: app: app template: metadata: labels: app: app spec: containers: - name: app image: gitea.homelab.local/gitea_admin/app:latest ports: - containerPort: 80 after applying the yaml, if you run kubectl describe deployment/app_name you may notice in the events section that it\u0026rsquo;s stating pulling the image has failed, that\u0026rsquo;s logical because kubernetes cluster doesn\u0026rsquo;t know about our custom domain: homelab.local.\nSo to let kubernetes DNS server: CoreDNS, acknowledge our domain we gonna need a litle tweak into the CoreDNS config\nwe run the following command to open the editor with configmap:\n1 kubectl edit configmap -n kube-system coredns and then we add the reference to homelab.local\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 apiVersion: v1 kind: ConfigMap metadata: name: coredns namespace: kube-system data: Corefile: | .:53 { errors health kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa } prometheus :9153 forward . 172.16.0.1 cache 30 loop reload loadbalance } homalb.local:53 { errors cache 30 forward . 192.168.1.114 } and for the CoreDNS to take effect, we will restart it with :\n1 kubectl rollout restart -n kube-system deployment/coredns Now, to test things out you can redeploy the previous deployed yaml or just run an alpine with nslookup\n1 kubectl run --image=alpine:3.5 -it alpine-shell-1 -- nslookup gitea.homelab.local it should return the ip address of the ingress.\n","permalink":"//localhost:1313/posts/deploy-gitea-k8s/","summary":"Introudction Hello, lately I have been trying to deploy a custom Docker image into my local Kubernetes cluster. It turned out I needed to host my Docker image on a container registry, either Docker Hub, which is not suitable for my use case, or deploy and use a local registry. During my research, I found Gitea, which I liked as it allows me to deploy all my projects on it and also host the containers.","title":"Deploying gitea into kubernetes with custom domain"},{"content":"HIDING\n","permalink":"//localhost:1313/posts/hide-your-secrets/","summary":"HIDING","title":"GCP -\u003e AWS Migration: Hide Your Secrets"},{"content":"(Knock on the door)\n(door opens)(some background noise of the company like people shatter or talk)\nYeah, Come In\nHey\nHey\nHey, my Junior friend, have a seat.\nThanks\nIt\u0026rsquo;s been 2 weeks since I assigned you the project, how it\u0026rsquo;s going?\nGood, I\u0026rsquo;m doing good, I just finished a few tasks and as you asked about my progress.\nSo, can you tell me how much progress you have made?\nI think am nearly 40%, I somehow finished creating the dev environment, so I can test with developers the performance and durability of the infrastructure.\nWhat about PostgreSQL, Redis, Configuration and Secrets Management? And I saw your email about using AWS AURORA, did you figure out the answer ?\nI deployed the database as an RDS, well I didn\u0026rsquo;t find a real benefit in using Aurora for the dev environment, and also it is somehow expensive, but maybe we can use it on the prod environment as it offers scalability. For Redis, AWS has a fully managed service called Elastic Cache with integrated monitoring service CloudWatch and the last one Configuration Management I read about the service (AWS Secret Manager) that I will use but didn\u0026rsquo;t integrate yet.\nThat seems fair, have you faced some challenges learning AWS, I heard from my fellow developers that there are a few steeping curves in understanding the documentation and the services.\nSomehow yes, as I don\u0026rsquo;t know that much about AWS and the time isn\u0026rsquo;t enough for learning and practising, I choose a tool to be abstract for me the underlying infrastructure for now, but I am willing to learn what happens in the background, so I can customize more.\nWhat\u0026rsquo;s the tool name?\nAWS Copilot (the CTO typing the name, we need to hear the keyboard typing) it\u0026rsquo;s an open source project \u0026hellip;.(the CTO interrupt saying)\nHmm, I see, but wouldn\u0026rsquo;t that add a new layer to learn, because most DevOps developers on the market will stick with IAC?\nYeah, I thought of that, I noticed that copilot works upon CloudFormation, which is an IAC tool for AWS and if you\u0026rsquo;re asking why not use Terraform with built-in modules, I can tell with this method we will narrow in the future our range of search to DevOps with terraform knowledge as with the current situation we can teach our backend developers a few commands and that\u0026rsquo;s enough to monitor the infrastructure.\nInteresting, I\u0026rsquo;m the type of technical guy, can you explain more about the solution?\nOk, can I open my laptop to show you\n(developer unpacking the laptop sound)\nYeah, Sure\nhave you thought of opening a startup (the CTO also) (CTO show some interest in the developer)\n(think few seconds) No, I don\u0026rsquo;t think I am ready, but maybe a small side hustle\nOkay, let me log into my AWS account (while typing)(the developer said) add the MFA code and here we go.\n(the sound of the CTO moving to the chair next to the developer)\nSo, here is the CloudFormation dashboard, those are the stacks that copilot deployed, you can also identify them by tags, generally they have tags copilotenvironment and copilot-application, at the end if somehow copilot didn\u0026rsquo;t meet our expectation or we couldn\u0026rsquo;t customize it further we can just modify, extend and boom. Now let me show the commands to set up and deploy the services\u0026hellip;..\nYou didn\u0026rsquo;t deploy the app Yet, right? (while interrupting)\nNot Yet\nOk (with notification sound on the phone), Continue\nTo set up infrastructure first, we need to run copilot app init then copilot env init after that copilot will ask us a few questions about the public and private subnets and the availability zone of the load balancer after we confirm that we see 3 Cloudformation stack has been created which define a few resources(VPC, Subnets, RouteTables,InternetGateway, and Cluster for Containers), finally I will be writing all the steps on notion.\nAnd the dependencies? (CTO asked)\nYeah, I did my search on deploy RDS and ElasticCache, Copilot doesn\u0026rsquo;t support either of them as a built-in command, but we can extend and create the services by ourselves and this is what I have done, the extended functionality support CloudFormation\u0026rsquo;s files\nGot It, but, hmm(take a few seconds to rephrase ), but I\u0026rsquo;m worrying if we don\u0026rsquo;t understand carefully and let copilot create the infrastructure, it may lead to creating unuseful services and you know we\u0026rsquo;re short on money.\nYeah, I understand your concern but am willing to learn more about the architecture copilot created for us by that time I can see if copilot fits our needs else I remove it and advance my skills to manage by myself the rest of the infrastructure directly with cloud formation\nThat seems fair enough, Ok, here is what you gonna do in the next few days. After you finish deploying that config management thing, deploy the application on ECS, and handle it to the developers to try and check the performance, and what about the CI/CD\nCopilot support it but didn\u0026rsquo;t take a deep look into the documentation, I can deploy the application from my terminal, so let\u0026rsquo;s tell the project manager whenever there is a pull request merged they notify me so I pull the code and push it for review\nNoted, I will tell him\n","permalink":"//localhost:1313/posts/confession/","summary":"(Knock on the door)\n(door opens)(some background noise of the company like people shatter or talk)\nYeah, Come In\nHey\nHey\nHey, my Junior friend, have a seat.\nThanks\nIt\u0026rsquo;s been 2 weeks since I assigned you the project, how it\u0026rsquo;s going?\nGood, I\u0026rsquo;m doing good, I just finished a few tasks and as you asked about my progress.\nSo, can you tell me how much progress you have made?","title":"GCP -\u003e AWS Migration: Confession"},{"content":" 3 days passes, and I\u0026rsquo;m struggling on the same bug, Am I looking at the wrong side of the window, I don\u0026rsquo;t know, I think the best way to understand is going back and examine every command line and line of code I wrote.\nSo at first, After I took the decision to use AWS copilot, I look at the task on JIRA, analyzed it carefully and saw the need to deploy application dependencies first, one of the dependencies is the database . We have been using PostgresSQL V14, so we just need the same version on the dev environment.\nI run through the documentation, to catch any command on how to deploy a database with underlying infrastructure(VPC, Subnet, Route Table, \u0026hellip;). The first command I saw\ncopilot init It fulfils the need of creating the underlying infrastructure, but it requires an application ready to deploy, but this is not the case now. A few minutes after and stumbled upon another command with the description \u0026ldquo;creates a new environment where your services will live.\u0026rdquo;\ncopilot env init When I run the above command, it asked me to run copilot app init first. And here is the output of environment creation\nFrom my understanding of the output and manifest.yml file, it seems after running copilot env deploy it will create two public and private subnets on separate regions. So I proceeded with the command, and here is the output\nI googled a few terms I didn\u0026rsquo;t understand like ECS, security groups, and DNS namespace to have basic knowledge of what happens in the background.\nThe following task was deploying the database and this is when things got trickier, now one of the features of copilot is a command to deploy storage services like database, file system\u0026hellip; etc. It supports two types of databases DynamoDB, Aurora\nAurora seems a great option as it\u0026rsquo;s fully compatible with PostgresSQL, so I tried to deploy a cluster using the following command\ncopilot storage init -n cluster -t Aurora --lifecycle environment --engine PostgreSQL At the same time, I opened Thunderbird and I messaged the CTO asking if it was ok deploying Aurora instead of an RDS. I went back to the command and I found\nCouldn\u0026rsquo;t find any workloads associated with app noteapp, try initializing one: copilot [svc/job] init .\n‚úò select a workload from noteapp : no workloads found in noteapp\nWell, the problem is obvious, I cannot deploy a storage service unless I create a service first and by service I mean containerized application.\nI walked through the documentation again, and I found a magical feature that says \u0026ldquo;Modeling Additional Environment Resources with AWS CloudFormation\u0026rdquo;, this feature gave me the ability to deploy resources on an environment based.\nThat gave me goosebumps to understand CloudFormation, as it seems crucial in the next phases. The methodology was deploying a demo architecture to get comfortable with the services and the whole flow, I deployed one of the well-known architectures which is lambda function \u0026amp; DynamoDB and here is my recap\nCloudFormation file structure consists of 3 main blocks Parameters (Optional), Resources (Required), and Outputs (Optional).\nResources block encapsulate the services we need to deploy, Each service requires 2 properties Type, Properties and each service has different Properties, an example of that for Lambda Function there are Handler , Runtime , Code properties while on DynamoDB::Table there are different properties AttributeDefinitions , KeySchema , ProvisionedThroughput Now, AWS CLI has a built-in command for managing CloudFormation files, to create resources the first time the command is\naws cloudformation create-stack --stack-name resource_stack --template-body file://cloudformation.yml --capabilities CAPABILITY_NAMED_IAM and for update\naws cloudformation update-stack --stack-name resource_stack --template-body file://cloudformation.yml --capabilities CAPABILITY_NAMED_IAM An additional cool feature of AWS CloudFormation is the built-in managing dashboard where you can see your stacks and their status\nThen I started to think about integrating CloudFormation with Copilot until I looked through the window, and it was almost dark and my back was hurting, so I took the sign and went for a little bit of social life\n","permalink":"//localhost:1313/posts/stress-swallows-you/","summary":"3 days passes, and I\u0026rsquo;m struggling on the same bug, Am I looking at the wrong side of the window, I don\u0026rsquo;t know, I think the best way to understand is going back and examine every command line and line of code I wrote.\nSo at first, After I took the decision to use AWS copilot, I look at the task on JIRA, analyzed it carefully and saw the need to deploy application dependencies first, one of the dependencies is the database .","title":"GCP -\u003e AWS Migration: Stress Swallows You"},{"content":" You know that feeling ?, when you\u0026rsquo;re escaping a bad documentation, instead crawling around searching for a solution, and you find a snippet of code on Stack Overflow or Reddit, after you copy and paste it, it doesn\u0026rsquo;t work then your mind tells you \u0026ldquo;you need to change it a little bit\u0026rdquo;.\nSo you start changing the code to solve your problem, and guess what? A hell of a lot of new terminology and ideas enter your mind, and you start to get confused. Well, I Hamza Bou Issa am in that state of mind.\nThe last time I was left in \u0026ldquo;Deploying RDS with copilot using CloudFormation\u0026rdquo;, Yeah my approach to solving the problem is the same as before, typing a bunch of keywords and questions into Google, clicking on the first few links, if Stack Overflow then I detect responses with green mark and copy code, if it\u0026rsquo;s an article, I find snippets and copy the ones that have RDS or DB on them\nI took the time to understand CloudFormation file structure and a few resource types\nAt first, I found this snippet\n# Set AWS template version AWSTemplateFormatVersion: \u0026quot;2010-09-09\u0026quot; # Set Parameters Parameters: EngineVersion: Description: PostgreSQL version. Type: String Default: \u0026quot;14.1\u0026quot; SubnetIds: Description: Subnets Type: \u0026quot;List\u0026lt;AWS::EC2::Subnet::Id\u0026gt;\u0026quot; VpcId: Description: Insert your existing VPC id here Type: String Resources: DBSubnetGroup: Type: \u0026quot;AWS::RDS::DBSubnetGroup\u0026quot; Properties: DBSubnetGroupDescription: !Ref \u0026quot;AWS::StackName\u0026quot; SubnetIds: !Ref SubnetIds DatabaseSecurityGroup: Type: \u0026quot;AWS::EC2::SecurityGroup\u0026quot; Properties: GroupDescription: The Security Group for the database instance. VpcId: !Ref VpcId SecurityGroupIngress: - IpProtocol: tcp FromPort: 5432 ToPort: 5432 DBInstance: Type: \u0026quot;AWS::RDS::DBInstance\u0026quot; Properties: AllocatedStorage: \u0026quot;30\u0026quot; DBInstanceClass: db.t4g.medium DBName: \u0026quot;postgres\u0026quot; DBSubnetGroupName: !Ref DBSubnetGroup Engine: postgres EngineVersion: !Ref EngineVersion MasterUsername: username MasterUserPassword: password StorageType: gp2 MonitoringInterval: 0 VPCSecurityGroups: - !Ref DatabaseSecurityGroup The following code will create 3 resources: DbInstance , DatabaseSecurityGroup , DBSubnetGroup , From my understanding the connection between those resources is a Database need to be created on private Subnets(DBSubnetGroup) on the other side for the database to accept connection it needs a security group( DatabaseSecurityGroup ) which should be on the same VPC as the Subnets\nNow before I paste this code into environment/addons/rds.yml, I\u0026rsquo;m going to remove the parameters as we have an alternate method of passing the SubnetIds and VpcId.\nCloudFormation gives the ability to import resources from previously created stacks with Fn::ImportValue function. In this case, after I run copilot env deploy --name test . Copilot create 2 CloudFormation stacks\nThe first stack is the interesting one, after we open on mycompany-app-test stack, we click on the Outputs panel, and it should show us the created resources with export names that can be imported on our RDS stack.\nThe two interesting export names are mycompany-app-test-PrivateSubnets , mycompany-app-test-VpcId , let\u0026rsquo;s refactor our rds.yml file and add them\n# Set AWS template version AWSTemplateFormatVersion: \u0026quot;2010-09-09\u0026quot; # Set Parameters Parameters: App: Type: String Description: Your application's name. Env: Type: String Description: The environment name your service, job, or workflow is being deployed Name: Type: String Description: The name of the service, job, or workflow being deployed. Resources: DBSubnetGroup: Type: \u0026quot;AWS::RDS::DBSubnetGroup\u0026quot; Properties: DBSubnetGroupDescription: !Ref \u0026quot;AWS::StackName\u0026quot; SubnetIds: !Split [',', { 'Fn::ImportValue': !Sub '${App}-${Env}-PrivateSubnets'}] DatabaseSecurityGroup: Type: \u0026quot;AWS::EC2::SecurityGroup\u0026quot; Properties: GroupDescription: The Security Group for the database instance. VpcId: Fn::ImportValue: !Sub '${App}-${Env}-VpcId' SecurityGroupIngress: - IpProtocol: tcp FromPort: 5432 ToPort: 5432 CidrIp: 0.0.0.0/0 DBInstance: Type: \u0026quot;AWS::RDS::DBInstance\u0026quot; Properties: AllocatedStorage: \u0026quot;30\u0026quot; DBInstanceClass: db.t4g.medium DBName: \u0026quot;postgres\u0026quot; DBSubnetGroupName: !Ref DBSubnetGroup Engine: postgres EngineVersion: \u0026quot;14.1\u0026quot; MasterUsername: username MasterUserPassword: password StorageType: gp2 MonitoringInterval: 0 VPCSecurityGroups: - !Ref DatabaseSecurityGroup As you see, I removed the previous parameters and replace them with a few parameters App , Env , Name which is copilot required add-ons parameters. Also for SubnetIds I import PrivateSubnets and split it because it must be passed as separate values\nAfter I run copilot env deploy --name test a nested stack will get created\nBut how I\u0026rsquo;m going to test if the database is working or accepting connection while it\u0026rsquo;s not reachable on the public internet, well it seems I can create an ec2 instance on the public subnet and allow connection with the security group.\nHere is the refactored code\n# Set AWS template version AWSTemplateFormatVersion: \u0026quot;2010-09-09\u0026quot; # Set Parameters Parameters: App: Type: String Description: Your application's name. Env: Type: String Description: The environment name your service, job, or workflow is being deployed to. Resources: DBSubnetGroup: Type: \u0026quot;AWS::RDS::DBSubnetGroup\u0026quot; Properties: DBSubnetGroupDescription: !Ref \u0026quot;AWS::StackName\u0026quot; SubnetIds: !Split [',', { 'Fn::ImportValue': !Sub '${App}-${Env}-PrivateSubnets' }] DatabaseSecurityGroup: Type: \u0026quot;AWS::EC2::SecurityGroup\u0026quot; Properties: GroupDescription: The Security Group for the database instance. VpcId: Fn::ImportValue: !Sub '${App}-${Env}-VpcId' SecurityGroupIngress: - IpProtocol: tcp FromPort: 5432 ToPort: 5432 CidrIp: 0.0.0.0/0 DBInstance: Type: \u0026quot;AWS::RDS::DBInstance\u0026quot; Properties: AllocatedStorage: \u0026quot;30\u0026quot; DBInstanceClass: db.t4g.medium DBName: \u0026quot;postgres\u0026quot; DBSubnetGroupName: !Ref DBSubnetGroup Engine: postgres EngineVersion: \u0026quot;14.1\u0026quot; MasterUsername: username MasterUserPassword: password StorageType: gp2 MonitoringInterval: 0 VPCSecurityGroups: - !Ref DatabaseSecurityGroup Tags: - Key: Name Value: !Sub 'copilot-${App}-${Env}' NewKeyPair: Type: 'AWS::EC2::KeyPair' Properties: KeyName: !Sub ${App}-${Env}-EC2-RDS-KEYPAIR Tags: - Key: Name Value: !Sub 'copilot-${App}-${Env}' EC2SecuityGroup: Type: \u0026quot;AWS::EC2::SecurityGroup\u0026quot; Properties: GroupDescription: The Security Group for the ec2 instance. VpcId: Fn::ImportValue: !Sub '${App}-${Env}-VpcId' SecurityGroupIngress: - IpProtocol: tcp FromPort: 22 ToPort: 22 CidrIp: 0.0.0.0/0 Tags: - Key: Name Value: !Sub 'copilot-${App}-${Env}' Ec2Instance: Type: 'AWS::EC2::Instance' Properties: ImageId: ami-05e8e219ac7e82eba InstanceType: t2.micro KeyName: !Ref NewKeyPair SubnetId: !Select [\u0026quot;0\u0026quot;,!Split [',', { 'Fn::ImportValue': !Sub '${App}-${Env}-PublicSubnets' }]] SecurityGroupIds: - !Ref EC2SecuityGroup UserData: | #!/bin/bash sudo apt update sudo apt upgrade sudo apt install postgresql postgresql-contrib Tags: - Key: Name Value: !Sub 'copilot-${App}-${Env}' Outputs: ServerPublicDNS: Description: \u0026quot;Public DNS of EC2 instance\u0026quot; Value: !GetAtt Ec2Instance.PublicDnsName DatabaseEndpoint: Description: \u0026quot;Connection endpoint for the database\u0026quot; Value: !GetAtt DBInstance.Endpoint.Address A few things to notice, I added an Ec2Instance on a public subnet, a security group that allows only ssh port (22), meanwhile, the ImageId, InstanceType property are more tied to the region you\u0026rsquo;re deploying to, I am using eu-west-3(I\u0026rsquo;m not a French guy -_-). NewKeyPair is the ssh key to log into EC2, and finally the Outputs section for getting the EC2 instance and Database connection URL\nI rerun copilot env deploy --name test , wait for the stack to update, before connecting to ec2 an ssh file must be downloaded, the ssh key pair will be saved on AWS Parameter Store, here are the following steps to download it\naws ec2 describe-key-pairs --filters Name=key-name,Values=mycompany-app-test-E The above command output.\nkey-05abb699beEXAMPLE and to save the ssh key value\naws ssm get-parameter --name /ec2/keypair/key-05abb699beEXAMPLE --with-decrypt Now, After I get the ssh file, try to connect to the server\nssh -i new-key-pair.pem ubuntu@ec2-host I try to connect to RDS\npsql -U username -d postgres -h database_host ","permalink":"//localhost:1313/posts/determination/","summary":"You know that feeling ?, when you\u0026rsquo;re escaping a bad documentation, instead crawling around searching for a solution, and you find a snippet of code on Stack Overflow or Reddit, after you copy and paste it, it doesn\u0026rsquo;t work then your mind tells you \u0026ldquo;you need to change it a little bit\u0026rdquo;.\nSo you start changing the code to solve your problem, and guess what? A hell of a lot of new terminology and ideas enter your mind, and you start to get confused.","title":"GCP -\u003e AWS Migration: Determination"},{"content":"Key points wake up with an email deciding not to go to the workspace read the task description and start searching lay on the bed again, thinking of ways to approach the problem get up and write his thoughts in his journal Search again and find copilot as a solution his alarm bumps for taking a walk It\u0026rsquo;s 8:35 AM, and the phone started vibrating with notifications, he used to receive the \u0026ldquo;Good Morning Babe\u0026rdquo; message from his girlfriend with a few newsletters emails, but this time, it was an email on his Zoho mail with the Subject: [JIRA] Yassine assigned DEV-550 to you\nHe opened his JIRA account from his phone, he was assigned a single task with the title\nHe didn\u0026rsquo;t take a deep look at the description, but he felt he wasn\u0026rsquo;t in the mood to go to the workspace, so he sent a message to the company Slack channel stating he is going to work from home today.\nTook a quick bath, eat breakfast and returned to the desk, he opened again the JIRA task and now taking a better look at the description by catching some keywords: DEV Environment, VPC, Subnet, Route Table, LoadBalancer, Database, Redis, Containers, IAC\nHe wasn\u0026rsquo;t familiar with the majority of keywords, and this was an advantage to stimulate him for learning. 50 min passes and he should take a break for 20 min as he follows The 50-Minute Focus Technique, but it doesn\u0026rsquo;t seem he cares, his eyes are filled with passion for finding the solution, he knew there is no direct tutorial or article to his problem.\nHis browser was filled with tabs of random articles with a side note app to summarise what he understood from each article\nHe lay down on the bed, taking a break and connecting the dots. The task seemed more clear now, but time was a crucial factor, he was telling himself\nI pretty understand the workflow I should take but hmmm,but 3 weeks doesn\u0026rsquo;t seem a lot of time, so let me find a tool to help me speed up the process and doesn\u0026rsquo;t hook me up into much networking details for now\nLet me take a nap for now\nIt was 3:51 PM, and this time he woke up prepared, he took his pc while lying on his bed. The goal is to find a solution to help him speed up building the infrastructure, A quick search leads him to a few tools like Terraform, Ansible, CloudFormation, Puppet, Vagrant\nBut it didn\u0026rsquo;t seem the final result, Terraform \u0026amp; CloudFormation were IAC( Infrastructure as Code) tools an abstraction over cloud provider Rest API, Ansible and Puppet were Configuration Management tools he doesn\u0026rsquo;t need for now and a vagrant is a virtualization tool which was away from his needs.\nIn the same way, he was eager to find a tool to work on top of CloudFormation, he was typing all sorts of keywords around CloudFormation and IAC tools like \u0026ldquo;Tools that use CloudFormation, AWS open source projects for building networking infrastructure, What better than CloudFormation, CloudFormation Cons, CloudFormation Alternatives\u0026rdquo;\nAs a consequence, the results were somehow near his needs, he found a few tools to compare them and decide which one to go with\nAWS CDK HashiCorp WAYPOINT AWS Copilot AWS Amplify Fissaa How extendible the tool is, How much is supported by the community if it\u0026rsquo;s open source(GitHub Starts, pull request, issues), learning curve, have additional and unique features. Those were the criteria he judged upon choosing his next tool, for better clarity he created a table where he assigned each one a score\nAfter calculating the scores, AWS Copilot won the battle by how much is can be extendible into adding CloudFormation files, the community is quite good with 2.7k stars and few issues and on the pull requests side people seems interested in developing features on it, the learning curve scored 6 because documentation was pretty clear, and they have tutorial section and AWS is investing some videos also, it scored the highest on unique features because it helps deploy storage services(S3, Aurora, DynamoDB), customizing the underlying infrastructure like Subnet, NAT, Load Balancer\u0026hellip;etc.\nIt\u0026rsquo;s 7:19 PM, and the alarm rings saying \u0026ldquo;30-min Walk\u0026rdquo;\u0026hellip;\n","permalink":"//localhost:1313/posts/the-search/","summary":"Key points wake up with an email deciding not to go to the workspace read the task description and start searching lay on the bed again, thinking of ways to approach the problem get up and write his thoughts in his journal Search again and find copilot as a solution his alarm bumps for taking a walk It\u0026rsquo;s 8:35 AM, and the phone started vibrating with notifications, he used to receive the \u0026ldquo;Good Morning Babe\u0026rdquo; message from his girlfriend with a few newsletters emails, but this time, it was an email on his Zoho mail with the Subject: [JIRA] Yassine assigned DEV-550 to you","title":"GCP -\u003e AWS Migration: The Search"},{"content":"Key Points: the boy is still an amateur, this is his first experience of him with DevOps the boy lacks some social skills the Startup CEO calls him he got the task of migrating the stack from GCP to AWS he returns confused and scared of not finding a solution then he goes back home, locks the room door and Lay on the bed imagining the way He was standing in front of the Nespresso machine, waiting for his daily caffeine fix, but his ears were inadvertently eavesdropping on the conversation of his two colleagues.\nThe page will take too long if you make that many requests to the backend\nShouldn\u0026rsquo;t we talk to the backend team to create one API to post all JSON data?\u0026quot; one colleague suggested.\nHe didn\u0026rsquo;t keep listening after he smelled the aroma of his coffee. He took his coffee and went back to his desk. As a Backend Developer, he scrolled up and down, examining the project\u0026rsquo;s code and searching for any opportunities for optimization or refactoring.\nAn hour passed, and he grew bored. He played the Spotify playlist \u0026ldquo;Lofi Electronic mix\u0026rdquo; and then slumped in his chair, wondering what life would be like after one year of professional experience. It was the smile on his face that reflected that he was moving closer to his dreams. That moment was the missing key to escaping boredom.\nWhile wandering in his imagination, he felt a hand on his shoulder. He opened his eyes and saw the CTO in front of him. He was embarrassed and suddenly fell off the chair. The CTO laughed and asked if he was okay. He replied\nYeah, all good, thanks. I was thinking about some optimization, and I was closing my eyes to avoid getting distracted\nYep, sure. Sorry for disturbing you. Can I have a moment of your time to discuss something?\u0026quot; the CTO asked.\nHe followed him to the office, sat down, and waited for the CTO to finish sending the email. He couldn\u0026rsquo;t stop asking himself if he was going to be fired or not. His heart jumped out of fear. Then the CTO said,\nI received an email from GCP (Google Cloud Platform) that they will not give us more credit after the next two months. So we thought of migrating to AWS since it gives us one year of free usage on some services. We can benefit from the wide range of services that it provides. I called you because I know you have some sort of Networking knowledge and skills as you worked with some projects on university, and I wanted to ask if you\u0026rsquo;re interested\nTo be honest with you, I dont have that much experience with AWS, but I just did basic configuration of EC2 instances, that\u0026rsquo;s all.\nHmm, Understood, So are you willing to face more challenges and develop yourself on the cloud project ?\nOkey, I think it will be a good experience to prove myself and my skills, I\u0026rsquo;m In\nOkay, sounds good. I will tell the backend senior that you moved to the DevOps team and send you the required tasks and GCP credentials\nCan I ask who is with me on the team ?\nNo one, just you\nHe left the office, feeling a mix of curiosity and fear. He packed his laptop, put on his headphones, and walked back home.\nAs he walked through the door of his apartment, he took off his shoes and fell onto the couch. His mind was racing with thoughts about the upcoming project and the opportunity that lay ahead of him. He was excited about the prospect of working with AWS and learning more about DevOps.\nHe spent the rest of the evening researching AWS services and watching tutorials on how to use them. He was determined to be well-prepared for the task ahead. As the night wore on, he grew tired and went to bed, dreaming about the possibilities that awaited him\n","permalink":"//localhost:1313/posts/the-beginning/","summary":"Key Points: the boy is still an amateur, this is his first experience of him with DevOps the boy lacks some social skills the Startup CEO calls him he got the task of migrating the stack from GCP to AWS he returns confused and scared of not finding a solution then he goes back home, locks the room door and Lay on the bed imagining the way He was standing in front of the Nespresso machine, waiting for his daily caffeine fix, but his ears were inadvertently eavesdropping on the conversation of his two colleagues.","title":"GCP -\u003e AWS Migration: The Beginning"},{"content":"Introduction Deploy Previews allow you and your team to experience changes to any part of your site without having to publish them to production.\nWith a deploy previews feature you and your teammates can see the changes of every pull request you make without merging it, this will reduce the burden of rolling back the environment when bugs happen as you can review the changes before.\nIn this tutorial, you‚Äôll learn about creating a CI pipeline with CodeBuild that gets triggered on every pull request creation or update, for every build we host react build folder on an S3 bucket and serve it with Cloudfront, finally after merging the pull request, we delete the build folder from S3.\nIn the end, this tutorial will expand your knowledge of AWS Services and help you speed up your development and deployment process.\nBefore starting, here is a playlist to enjoy reading with\nTo accomplish this tutorial, you‚Äôll need:\nan AWS account with administrative permissions an AWS CLI with a minimum version of 2.4.6 a runnable React app hosted on GitHub basic knowledge of YAML file structure clone this simple react app Creating CI Pipeline To build a resilient system, you need to add tests on your code then run them locally and on every push to version control. In this step, you will build a CI pipeline that triggers every pull request creation or update on your GitHub repository, the pipeline will run the desired tests and return a badge containing the status of the tests to your GitHub.\nCreate the file pull-request.yml in your text editor:\nnano pull-request.yml Add the following CloudFormation code to the file, which builds a CodeBuildProject that defines a CI pipeline with GitHub integration:\nResources: CodeBuildProject: Type: AWS::CodeBuild::Project Properties: Name: FrontBuildOnPull ServiceRole: !GetAtt BuildProjectRole.Arn LogsConfig: CloudWatchLogs: GroupName: !Ref BuildLogGroup Status: ENABLED StreamName: front_pull_request EncryptionKey: \u0026quot;alias/aws/s3\u0026quot; Artifacts: Type: S3 Location: !Ref ArtifactBucket Name: FrontPullRequest OverrideArtifactName: true EncryptionDisabled: true Environment: Type: LINUX_CONTAINER ComputeType: BUILD_GENERAL1_MEDIUM Image: aws/codebuild/amazonlinux2-x86_64-standard:4.0 Source: Type: GITHUB Location: \u0026quot;https://github.com/projectX/repoY\u0026quot; ReportBuildStatus: true Triggers: BuildType: BUILD Webhook: true FilterGroups: - - Type: EVENT Pattern: PULL_REQUEST_CREATED,PULL_REQUEST_UPDATED Visibility: PUBLIC_READ ResourceAccessRole: !Ref PublicReadRole I will briefly explain the CloudFormation template structure, so you can get a general understanding of CloudFormation files that guides you through the next steps.\nThe majority of CloudFormation template files will contain Parameters(Optional) and Resources(Required), Outputs(Optional), each block on the Resources is a Service, and every Service contains Type and Properties. The type is referred to as AWS Service or a Custom Function and each Type has a different set of properties.\nLet‚Äôs analyze the above Code to understand better. Here we have a Resources block that contains three Services. The CodeBuildProject refers to AWS::CodeBuild::Project which is an AWS Service for CodeBuild. The CodeBuild has a set of properties, you can find them here properties, I will explain each property functionality and why we‚Äôre adding it.\nGive CodeBuild the required permissions For CodeBuild to work we need to assign the ServiceRole an IAM Role to give CodeBuild the right to access other AWS services like secrets, S3, and logs.\n... ServiceRole: !GetAtt BuildProjectRole.Arn ... ... BuildProjectRole is a role that assumes the principal ‚Äúcodebuild.amazonaws.com‚Äù the permission to use a set of services on our behalf of us, more reading about AWS::IAM::ROLE\nAfter, we need to ask ourselves what services CodeBuild must have access to and what we want to access on each of the services:\nCodebuild needs to create and update the test reports. CodeBuild needs to store the CI process logs inside a logs group. CodeBuild needs to access the secrets manager to get some secret credentials ex: CYPRESS_KEY. - CodeBuild needs to get and store React build folder on S3. Now after we defined our requirements, we create BuildProjectPolicy which is an AWS::IAM::Policy for BuildProjectRole that contains the list of statements, and each statement is composed of a set of actions.\nBuildProjectRole: Type: AWS::IAM::Role Properties: AssumeRolePolicyDocument: Version: '2012-10-17' Statement: - Effect: Allow Principal: Service: - codebuild.amazonaws.com Action: - sts:AssumeRole BuildProjectPolicy: Type: AWS::IAM::Policy DependsOn: BuildProjectRole Properties: PolicyName: !Sub ${AWS::StackName}-CodeBuildPolicy PolicyDocument: Version: '2012-10-17' Statement: - Effect: Allow Action: - codebuild:CreateReportGroup - codebuild:CreateReport - codebuild:UpdateReport - codebuild:BatchPutTestCases - codebuild:BatchPutCodeCoverages # Create and update test report Resource: \u0026quot;*\u0026quot; - Effect: Allow Action: - s3:PutObject - s3:GetObject - s3:GetObjectVersion # Get and store React build folder on S3 Resource: \u0026quot;*\u0026quot; - Effect: Allow Action: - logs:CreateLogGroup - logs:CreateLogStream - logs:PutLogEvents # Store the CI process logs inside a logs group Resource: arn:aws:logs:*:*:* - Effect: Allow Action: - secretsmanager:* # Get secret creedentials ex: CYPRESS_KEY Resource: \u0026quot;*\u0026quot; Roles: - !Ref BuildProjectRole Access and Store CodeBuild Logs Logs are an important key of DevOps because it adds the visibility aspect to the infrastructure, so gather logs as much as you can. In our case, to enable logs for CodeBuild project we create LogsConfig property.\n... ... LogsConfig: CloudWatchLogs: GroupName: !Ref BuildLogGroup Status: ENABLED StreamName: front_pull_request ... ... Now, we need to create a LogGroup to which CodeBuild pushes logs to\nBuildLogGroup: Type: AWS::Logs::LogGroup Properties: LogGroupName: frontend_build_on_pull RetentionInDays: 7 RetentionInDays is the period after which the logs expire.\nAccess and Store React Builds After successfully building and testing the project, we should store the build folder on S3 so we can access it again when the developer wants to see his changes. So we create an Artifacts property that stores build on an S3 bucket called ‚ÄúArtifactBucket‚Äù\n... ... Artifacts: Type: S3 Location: !Ref ArtifactBucket Name: FrontPullRequest OverrideArtifactName: true EncryptionDisabled: true ... ... OverrideArtifactName we use to customize the path where we store the builds.\nEncryptionDisabled we disabled it because we don‚Äôt need custom encryption in our case, we will be using the default one instead.\nArtifactBucket: Type: 'AWS::S3::Bucket' DeletionPolicy: Delete Properties: BucketName: \u0026quot;some-random-unique-bucket-name\u0026quot; BucketEncryption: ServerSideEncryptionConfiguration: - ServerSideEncryptionByDefault: SSEAlgorithm: AES256 PublicAccessBlockConfiguration: BlockPublicAcls: true BlockPublicPolicy: true IgnorePublicAcls: true RestrictPublicBuckets: true ArtifactBucket is a resource block that refers to AWS::S3::Bucket, also we should add some properties to restrict public access to build folders and define how the data must be encrypted when it gets stored.\nName: you need to change to your unique bucket name, the name must be unique across all AWS buckets.\nBucketEncryption we use Amazon S3 server-side encryption which uses 256-bit Advanced Encryption Standard(AES256).\nPublicAccessBlockConfiguration this property blocks all public access to the public and ignores any policy that allows public visibility of the bucket.\nCodeBuild CI Environment For the CI to work, you must provide information about the CI environment. A build environment represents a combination of the operating system, programming language runtime, and tools that CodeBuild uses to run a build.\n... ... Environment: Type: LINUX_CONTAINER ComputeType: BUILD_GENERAL1_MEDIUM Image: aws/codebuild/amazonlinux2-x86_64-standard:4.0 ... Type this is the OS type, which is Linux for our case.\nComputeType: this provides the CodeBuild with information about the computing resources it‚Äôs allowed to use (7 GB Memory,4vCPU,128GB Disk ).\nImage: A docker image to use during the build which provides the environment with a set of tools and runtimes, here is the documentation for better details\nCodeBuild Trigger and Repo Linking To fire up the build process whenever a pull request is created or updated, you need to add two properties: Source, Triggers\n... ... Source: Type: GITHUB Location: \u0026quot;https://github.com/projectX/repoY\u0026quot; ReportBuildStatus: true Triggers: BuildType: BUILD Webhook: true FilterGroups: - - Type: EVENT Pattern: PULL_REQUEST_CREATED,PULL_REQUEST_UPDATED ... ... Source defines the type of version control and the location, you need to change the Location property to your desired repository, on the other side to report the build status back to version control(Github) you turn ReportBuildStatus to true. Triggers specify webhooks that trigger The AWS CodeBuild build.\nTo allow Codebuild access to your repository, you need to create a personal access token and pass it to CodeBuild so it can access your repo on your behalf, This can be done in two steps:\n1- Generate a personal token from your GitHub account:\nVisit the Personal access tokens Github page, then select ‚ÄúFull control of private repositories‚Äù and ‚ÄúFull control of repository hooks‚Äù and click Generate token.\n2- Pass down the generated token to AWS Codebuild credentials: Run the following command to generate an import-source-credentials.json file on your local :\naws codebuild import-source-credentials --generate-cli-skeleton After, we need to modify import-source-credentials.json and fill it with your credentials:\nauth_type: PERSONAL_ACCESS_TOKEN\nserverType: GITHUB\nusername: your GitHub username\ntoken: the generated token from the previous step.\naws codebuild import-source-credentials \u0026ndash;cli-input-json file://import-source-credentials.json\nTo test that your command runs successfully, run the following command to list your CodeBuild credentials:\naws codebuild list-source-credentials Now when CodeBuild starts running it will search in your source code for a file named buildspec.yml, by definition :\nA buildspec is a collection of build commands and related settings, in YAML format, that CodeBuild uses to run a build.\nHere is an example of buildspec.yml from our demo app :\nversion: 0.2 phases: install: commands: - npm install pre_build: on-failure: ABORT commands: - echo \u0026quot;run some pre-check\u0026quot; #- npm run format:check #- npm run lint:check build: on-failure: ABORT commands: - echo \u0026quot;run tests\u0026quot; # - npm run-script start \u0026amp; npx wait-on http://localhost:3000 # - npx cypress run --record --key ${CYPRESS_KEY} post_build: commands: - | npm run build artifacts: # include all files required to run application # we include only the static build files files: - '**/*' name: $CODEBUILD_WEBHOOK_TRIGGER base-directory: 'dist' Our CI contains different phases (install, pre_build ‚Ä¶etc), finally, post_build will run when all the previous steps exited successfully.\npost_build will create a dist folder and then CodeBuild will upload the dist folder to S3 under CODEBUILD_WEBHOOK_TRIGGER path that will be /pr/{pr_number}.\nFor example, if a pull request gets created with the number 1, then after CI builds successfully the build folder will get stored on ArtifactBucket under path /pr/1/.\nCI logs Public Visibility To Users The following property gives developers access to see build logs without having an AWS account :\n... ... Visibility: PUBLIC_READ ResourceAccessRole: !Ref PublicReadRole ... then the PublicReadRole must allow access to logs and S3\nPublicReadRole: Type: AWS::IAM::Role Properties: AssumeRolePolicyDocument: Statement: - Action: ['sts:AssumeRole'] Effect: Allow Principal: Service: [codebuild.amazonaws.com] Version: '2012-10-17' Path: / PublicReadPolicy: Type: 'AWS::IAM::Policy' Properties: PolicyName: PublicBuildPolicy PolicyDocument: Version: \u0026quot;2012-10-17\u0026quot; Statement: - Effect: Allow Action: - \u0026quot;logs:GetLogEvents\u0026quot; Resource: - !Sub \u0026quot;arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:${BuildLogGroup}:*\u0026quot; - Effect: Allow Action: - \u0026quot;s3:GetObject\u0026quot; - \u0026quot;s3:GetObjectVersion\u0026quot; Resource: - Fn::Sub: - \u0026quot;${ArtifcatArn}/*\u0026quot; - {ArtifcatArn: !GetAtt ArtifactBucket.Arn} Roles: - !Ref PublicReadRole Sum It Up After you understand the full picture of CodeBuild Service and its dependency, you can edit pull-request.yml:\nnano pull-request.yml replace its content with the following code\nResources: CodeBuildProject: Type: AWS::CodeBuild::Project Properties: Name: FrontBuildOnPull ServiceRole: !GetAtt BuildProjectRole.Arn LogsConfig: CloudWatchLogs: GroupName: !Ref BuildLogGroup Status: ENABLED StreamName: front_pull_request EncryptionKey: \u0026quot;alias/aws/s3\u0026quot; Artifacts: Type: S3 Location: !Ref ArtifactBucket Name: FrontPullRequest OverrideArtifactName: true EncryptionDisabled: true Environment: Type: LINUX_CONTAINER ComputeType: BUILD_GENERAL1_MEDIUM Image: aws/codebuild/amazonlinux2-x86_64-standard:4.0 Source: Type: GITHUB Location: \u0026quot;https://github.com/projectX/repoY\u0026quot; ReportBuildStatus: true Triggers: BuildType: BUILD Webhook: true FilterGroups: - - Type: EVENT Pattern: PULL_REQUEST_CREATED,PULL_REQUEST_UPDATED Visibility: PUBLIC_READ ResourceAccessRole: !Ref PublicReadRole BuildProjectRole: Type: AWS::IAM::Role Properties: AssumeRolePolicyDocument: Version: '2012-10-17' Statement: - Effect: Allow Principal: Service: - codebuild.amazonaws.com Action: - sts:AssumeRole BuildProjectPolicy: Type: AWS::IAM::Policy DependsOn: BuildProjectRole Properties: PolicyName: !Sub ${AWS::StackName}-CodeBuildPolicy PolicyDocument: Version: '2012-10-17' Statement: - Effect: Allow Action: - codebuild:CreateReportGroup - codebuild:CreateReport - codebuild:UpdateReport - codebuild:BatchPutTestCases - codebuild:BatchPutCodeCoverages # Create and update test report Resource: \u0026quot;*\u0026quot; - Effect: Allow Action: - s3:PutObject - s3:GetObject - s3:GetObjectVersion # Get and store React build folder on S3 Resource: \u0026quot;*\u0026quot; - Effect: Allow Action: - logs:CreateLogGroup - logs:CreateLogStream - logs:PutLogEvents # Store the CI process logs inside a logs group Resource: arn:aws:logs:*:*:* - Effect: Allow Action: - secretsmanager:* # Get secret creedentials ex: CYPRESS_KEY Resource: \u0026quot;*\u0026quot; Roles: - !Ref BuildProjectRole BuildLogGroup: Type: AWS::Logs::LogGroup Properties: LogGroupName: frontend_build_on_pull RetentionInDays: 7 ArtifactBucket: Type: 'AWS::S3::Bucket' DeletionPolicy: Delete Properties: BucketName: \u0026quot;some-random-unique-bucket-name\u0026quot; BucketEncryption: ServerSideEncryptionConfiguration: - ServerSideEncryptionByDefault: SSEAlgorithm: AES256 PublicAccessBlockConfiguration: BlockPublicAcls: true BlockPublicPolicy: true IgnorePublicAcls: true RestrictPublicBuckets: true PublicReadRole: Type: AWS::IAM::Role Properties: AssumeRolePolicyDocument: Statement: - Action: ['sts:AssumeRole'] Effect: Allow Principal: Service: [codebuild.amazonaws.com] Version: '2012-10-17' Path: / PublicReadPolicy: Type: 'AWS::IAM::Policy' Properties: PolicyName: PublicBuildPolicy PolicyDocument: Version: \u0026quot;2012-10-17\u0026quot; Statement: - Effect: Allow Action: - \u0026quot;logs:GetLogEvents\u0026quot; Resource: - !Sub \u0026quot;arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:${BuildLogGroup}:*\u0026quot; - Effect: Allow Action: - \u0026quot;s3:GetObject\u0026quot; - \u0026quot;s3:GetObjectVersion\u0026quot; Resource: - Fn::Sub: - \u0026quot;${ArtifcatArn}/*\u0026quot; - {ArtifcatArn: !GetAtt ArtifactBucket.Arn} Roles: - !Ref PublicReadRole To test our code, move into the pull-request.yml file path and run\naws cloudformation create-stack --stack-name pull-request-preview-stack --template-body file://pull-request.yml --capabilities CAPABILITY_NAMED_IAM Now visit Cloudformation Console and you should see a stack with the name pull-request-preview-stack and its status CREATE_IN_PROGRESS or CREATE_COMPLETE\nServing Content With CloudFront CloudFront is a content delivery network(CDN), CDN can speed up the serving of your static content by caching in the edges that are near you, it is less expensive and it allows you to add a TLS certificate(HTTP) and domain to your static content.\nOn the other side, we don‚Äôt need all of these things but what we want from CloudFront is Lambda@Edge which will allow us to redirect requests by the header to right S3 bucket folder. To explain more, let‚Äôs suppose we made 2 pull-request by numbers 121,122, after 2 of them are tested and built successfully, they will get stored in a folder named pr on the Artifact Bucket with the following structure:\npr/\n121/\nassets index.html ‚Ä¶ ‚Ä¶ 122/\nassets index.html ‚Ä¶ ‚Ä¶ Now when Lambda@Edge comes to play, every request that comes to the CloudFront host(which is a random subdomain under CloudFront xxxxx.cloudfront.net) with a pull-request header and value 122 will be redirected to the folder ‚Äú122‚Äù to serve ‚Äú122‚Äù pull request contents.\nIn this section, you will learn how to serve S3 content from a CDN and customize users requests by different criteria like (header,query-params‚Ä¶etc) and also build a trigger whenever a build runs successfully, it clears the cache of a specific pull request on the CDN for the new contents.\nCreating CloudFront Let‚Äôs edit pull-request.yml\nnano pull-request.yml and add a CloudFront Distribution Service that will primarily serve index.html from ArtifactBucket\nDistribution: Type: AWS::CloudFront::Distribution Properties: DistributionConfig: Origins: - DomainName: !Sub ${ArtifactBucket}.s3.${AWS::Region}.amazonaws.com Id: S3Origin S3OriginConfig: OriginAccessIdentity: !Sub origin-access-identity/cloudfront/${OriginAccessIdentity} Enabled: true DefaultRootObject: index.html Logging: Bucket: !Sub ${DistributionBucket}.s3.${AWS::Region}.amazonaws.com DefaultCacheBehavior: TargetOriginId: S3Origin CachePolicyId: !Ref DistributionCachingPolicy ViewerProtocolPolicy: https-only PriceClass: PriceClass_100 Cloudfront has 4 major properties that we will explain briefly\nOrigins: This property refers to where CloudFront retrieves the content, we will be adding only Artifact Bucket, also there is a sub-property OriginAccessIdentity that restricts access to ArtifactBucket bucket only from CloudFront.\nDefaultRootObject: The default file that CloudFront will render from the bucket\nPriceClass: To how many regions should our content get cached? we choose PriceClass_100 that‚Äôs the minimum value because we don‚Äôt care about that.\nLogging: We should collect access logs, So we store them in a bucket ‚ÄúDistributionBucket‚Äù\nDefaultCacheBehavior is a required property so we will add it, but we don‚Äôt need any caching behavior from CloudFront to test the pull requests. DefaultCacheBehavior contains 3 required attributes:\nTargetOriginId points to one of our Origins which is S3Origin the only origin we have.\nViewerProtocolPolicy controls whether the user is redirected to HTTPS or uses HTTP or both, we chose https-only.\nCachePolicyId , is an important property because we need to allow CloudFront to pass certain headers to Lambda@Edge and the available managed cache policies don‚Äôt allow that, so creating a custom cache policy will help. here is the code for the custom cache policy that allows passing the pull-request header to Lambda@Edge.\nDistributionCachingPolicy: Type: AWS::CloudFront::CachePolicy Properties: CachePolicyConfig: Comment: \u0026ldquo;Allow pass of header and query params to cloudfront\u0026rdquo; DefaultTTL: 86400 MaxTTL: 31536000 MinTTL: 80400 Name: CacheForHeaderAndQuery ParametersInCacheKeyAndForwardedToOrigin: CookiesConfig: CookieBehavior: none EnableAcceptEncodingGzip: false HeadersConfig: HeaderBehavior: whitelist Headers: - pull-request QueryStringsConfig: QueryStringBehavior: all\nNow the problem we‚Äôre facing is that our pull requests content is stored under the PR folder so we cannot serve them as Cloudfront doesn‚Äôt allow dynamic selection of contents. So the solution is rerouting the user request before it reaches the S3 bucket and choosing the right path depending on the custom header(pull-request) he passes on the request.\nChoose The Right Path with Lambda@Edge To add Lambda@Edge to CloudFront we will modify DefaultCacheBehavior to look like this:\nDefaultCacheBehavior: TargetOriginId: S3Origin ViewerProtocolPolicy: redirect-to-https CachePolicyId: !Ref DistributionCachingPolicy LambdaFunctionAssociations: - EventType: 'origin-request' LambdaFunctionARN: !Ref VersionedLambdaFunction Let‚Äôs create the lambda function, the programming language will be python :\nLambdaFunction: Type: 'AWS::Lambda::Function' Properties: FunctionName: \u0026quot;cloudfront_lambda\u0026quot; Code: ZipFile: !Sub | import re import boto3 def handler(event, context): client = boto3.client(\u0026quot;s3\u0026quot;) bucket = \u0026quot;front-end-preview-bucket\u0026quot; response = client.list_objects_v2(Bucket=bucket,Prefix=\u0026quot;pr/\u0026quot;,Delimiter=\u0026quot;/\u0026quot;) prs = [p['Prefix'].split('/')[1] for p in response['CommonPrefixes']] request = event['Records'][0]['cf']['request'] print(request) headers = request['headers'] pr = headers.get('pull-request',None) if pr is None: print(f\u0026quot;{pr} not found\u0026quot;) response = { 'status': '404', 'statusDescription': 'No Found', } return response pr = pr[0]['value'] if not pr.isdigit() or not (pr in prs): print(f\u0026quot;{pr} not good\u0026quot;) response = { 'status': '404', 'statusDescription': 'No Found', } return response pr = int(pr) request['uri'] = f\u0026quot;/pr/{pr}{request['uri']}\u0026quot; print(request) return request Handler: 'index.handler' MemorySize: 128 Role: !GetAtt 'LambdaRole.Arn' Runtime: 'python3.9' Timeout: 5 LambdaRole: Type: 'AWS::IAM::Role' Properties: AssumeRolePolicyDocument: Version: '2012-10-17' Statement: - Effect: Allow Principal: Service: - 'lambda.amazonaws.com' - 'edgelambda.amazonaws.com' Action: 'sts:AssumeRole' Policies: - PolicyName: FetchContentFromBucket PolicyDocument: Version: \u0026quot;2012-10-17\u0026quot; Statement: - Effect: Allow Action: - \u0026quot;s3:GetObject\u0026quot; - \u0026quot;s3:ListBucket\u0026quot; - \u0026quot;s3:GetObjectVersion\u0026quot; Resource: - Fn::Sub: - \u0026quot;${ArtifcatArn}/*\u0026quot; - {ArtifcatArn: !GetAtt ArtifactBucket.Arn} ManagedPolicyArns: - 'arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole' - \u0026quot;arn:aws:iam::aws:policy/service-role/AWSLambdaVPCAccessExecutionRole\u0026quot; To summarize what function do, we fetch all current pull requests that are stored on the bucket and whenever a user makes a request, we check the header(pull-request) value and see if it‚Äôs available, if yes we rewrite the request URI to ‚Äú/pr/{pr}{request[‚Äòuri‚Äô]}‚Äù, if no we return 404 response.\nAlso, we assign Lambda different policies, some are managed policies like :\nAWSLambdaBasicExecutionRole AWSLambdaVPCAccessExecutionRole and others are custom like FetchContentFromBucket to query React build folder from ArtifactBucket.\nA Little Demo After understanding what each service does, we update our pull-request.yml file with the above code. then we update cloud formation with the following command:\naws cloudformation update-stack --stack-name pull-request-preview-stack --template-body file://pull-request.yml --capabilities CAPABILITY_NAMED_IAM Now visit Cloudformation Console and you should see a stack with the name pull-request-preview-stack and its status UPDATE_IN_PROGRESS or UPDATE_COMPLETE.\nTo get the CloudFront endpoint Click on pull-request-preview-stack on Cloudformation Console and Click Outputs\nWhat‚Äôs missing now, is creating a pull request in our repository. first, we need a new branch\ngit switch -c feature/test_preview then let‚Äôs make some changes and finally add, commit, push\ngit add . git commit -m \u0026quot;it's getting darker :black:\u0026quot; git push --set-upstream origin feature/test_preview Now let‚Äôs wait until the tests pass successfully üëÄ\nIf you visit the CloudFront Endpoint you will get a 404 page, this happens because you didn‚Äôt pass a pull-request number as a header. So to achieve this we need to install a chrome extension ‚Äúmobheader‚Äù\nyou replace 271 with your pull request number that you find on the Github pull request page\nRefresh now, Taddaaaa üéä\nNow, Let\u0026rsquo;s suppose that you made a pull request and requested one of your teammates to make a review for you and he reclaimed something buggy is happening, so you went to investigate the issue and solved it and now you‚Äôre pushing the updates. After your CI builds and tests successfully, you visit the CloudFront URL and you find the bug still exists, why ?!.\nWell it‚Äôs because CloudFront caches the build folder on its servers and you need to invalidate the cache from the servers then CloudFront will request the files again from S3\nSo the approach will be creating a Lambda Function that gets triggered whenever the builds run successfully, The function takes CloudFront DistributionId as an environment variable and make an invalidation request to /pr/{pr_number} subfolder\nEventCloudFrontLambda: Type: 'AWS::Events::Rule' Properties: Description: Invalidate Cloudfront after a successful build State: ENABLED EventPattern: source: - aws.codebuild detail-type: - CodeBuild Build State Change detail: build-status: - SUCCEEDED project-name: - !Ref CodeBuildProject Targets: - Arn: !GetAtt InvalidateCloudFront.Arn Id: \u0026quot;TargetFunctionV1\u0026quot; InvalidateCloudFront: Type: 'AWS::Lambda::Function' Properties: FunctionName: \u0026quot;invalidate_cloudfront_from_codebuild_lambda\u0026quot; Environment: Variables: DistributionId: !GetAtt Distribution.Id Code: ZipFile: !Sub | import boto3 import uuid import os def handler(event, context): print(event) artifict = event['detail']['additional-information']['artifact']['location'] pr = artifict.split(\u0026quot;/\u0026quot;)[2] distribution_id = os.getenv(\u0026quot;DistributionId\u0026quot;) print(distribution_id) client = boto3.client(\u0026quot;cloudfront\u0026quot;) response = client.create_invalidation( DistributionId=distribution_id, InvalidationBatch={ 'Paths': { 'Quantity': 1, 'Items': [ f'/pr/{pr}/*', ] }, 'CallerReference': str(uuid.uuid4()) } ) return {\u0026quot;status\u0026quot;:200} Handler: 'index.handler' MemorySize: 128 Role: !GetAtt 'LambdaRole.Arn' Runtime: 'python3.9' Timeout: 15 InvalidateCloudFrontLogs: Type: AWS::Logs::LogGroup DependsOn: InvalidateCloudFront Properties: LogGroupName: !Sub \u0026quot;/aws/lambda/${InvalidateCloudFront}\u0026quot; RetentionInDays: 7 PermissionForEventsToInvokeLambda: Type: AWS::Lambda::Permission Properties: FunctionName: !Ref InvalidateCloudFront Action: \u0026quot;lambda:InvokeFunction\u0026quot; Principal: \u0026quot;events.amazonaws.com\u0026quot; SourceArn: !GetAtt EventCloudFrontLambda.Arn Test Updating The Pull-request Code Let‚Äôs make some changes, if you cloned my repository you can change the index.json file\nand replace ‚ÄúHello World War 3! with ‚ÄúHello World Peace‚Äù\nand let‚Äôs wait for the builds to run successfully and recheck again our preview\nChallenge For You As our pull request creates a directory on S3, it is a waste of storage and money if we leave the directory on S3 after merging the pull request.\nSo the challenge will be creating a GitHub action workflow that will delete the folder from S3 after merging the pull request. one of the requirements is using AWS OpenID Connect.\nDon‚Äôt hesitate to email me, It will be a pleasure for me to review your work üòä\nSummary In this article, we walked into different AWS Services(CodeBuild, Cloudfront,S3), we understand the mechanism of AWS IAM finally we learned how to create and deploy our services with Cloudformation\nThanks for your time, stay tuned for new articles.\n","permalink":"//localhost:1313/posts/deployment-preview-with-aws-cloudfront/","summary":"Introduction Deploy Previews allow you and your team to experience changes to any part of your site without having to publish them to production.\nWith a deploy previews feature you and your teammates can see the changes of every pull request you make without merging it, this will reduce the burden of rolling back the environment when bugs happen as you can review the changes before.\nIn this tutorial, you‚Äôll learn about creating a CI pipeline with CodeBuild that gets triggered on every pull request creation or update, for every build we host react build folder on an S3 bucket and serve it with Cloudfront, finally after merging the pull request, we delete the build folder from S3.","title":"Deployment Preview with AWS CloudFront"}]