[{"content":"Introduction In this article, we’re going to replace the Cognito Service, I choose the Kratos and Oathkeeper from Ory as alternative.\nThe main functionalities of Congito here, is offering a way to sign in, sign up with or without SSO, email verification and use Sesssion Web Token for Frontend authorization.\nHere is the full architecture of authentication and authorization\nDon\u0026rsquo;t worry if didnt understand the architecture, we will dig deeper in the next headlines\nKratos Kratos is an identity and user management service, we will use it to authenticate user in, verify the email and generate Session Web token to pass it to authorization service later.\nOne feature that others will consider it a downside is Kratos doesn’t come with an UI, but Ory have created a separate repo. The repo has a dockerfile to build the image, it stay to us to create a helm chart.\nThe installation of kratos is handled by a helm chart, but we need to change few values first:\nKratos needs a database to store its state, so we supply a connection string of our private database, I created the user kratos earlier.\n1 2 3 config: ... dsn: postgresql://kratos:kratos@cluster-pg-rw.cnpg-system.svc.cluster.local:5432/kratos For sending emails, a separate service is created alongside the kratos api, to get it working we add the SMTP credentials we have, I use mailgun as it provides an easy interface and integration mechanism with python, the from_address property here is the address that’s will appear in your inbox when you receive an email.\n1 2 3 4 5 config: courier: smtp: connection_uri: xxxxxxxx from_address: no-reply@enkinineveh.space Flows property defines each stage like login, registration, verification and errors, because we’re using kratos UI all those pages will be under the root url like registration will be under: /registration.\n1 2 3 4 5 6 7 8 9 10 11 12 flows: error: ui_url: https://kratos.enkinineveh.space/error login: ui_url: https://kratos.enkinineveh.space/login verification: enabled: true ui_url: https://kratos.enkinineveh.space/verification registration: ui_url: https://kratos.enkinineveh.space/registration settings: ui_url: https://kratos.enkinineveh.space/settings We said before we need to send a verification email, and prevent unverified users from login by using this hook, so we modify the login to adjust the use case\n1 2 3 4 5 6 flows: login: ui_url: https://kratos.enkinineveh.space/login after: hooks: - hook: require_verified_address We will not enable ingress, because kratos-ui requires both services to be deployed under the same host, but unfortunately kratos-api helm chart doesn’t enable this, that’s why we will offload this to kratos-ui chart.\nThe domain we’re going to host the kratos-ui in is kratos.enkinineveh.space and the kratos api under /app path, then we pass environment variable to specify kratos-app url, CSRF_COOKIE_NAME and two random secret values\nkratos-ui values.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 ingress: enabled: true className: \u0026#34;nginx\u0026#34; annotations: nginx.org/rewrites: \u0026#34;serviceName=kratos-public rewrite=/\u0026#34; hosts: - host: kratos.enkinineveh.space paths: - path: / pathType: Prefix backend: service: name: kratos-ui-charts port: number: 3000 - path: /app pathType: Prefix backend: service: name: kratos-public port: number: 80 tls: - secretName: enkinineveh.space-tls-prod hosts: - kratos.enkinineveh.space ... secret: name: app-env-secret env: secret: CSRF_COOKIE_NAME: \u0026#34;__HOST-kratos.enkinineveh.space\u0026#34; COOKIE_SECRET: \u0026#34;e482c93bc1e9eab3cd65105128b0615accecceeff100c541b06b1698bca6a497\u0026#34; CSRF_COOKIE_SECRET: \u0026#34;a22736d7cefe107b2bf0a156984d9af35e32ff642290c23e5c054ca10ccde879\u0026#34; DANGEROUSLY_DISABLE_SECURE_CSRF_COOKIES: \u0026#34;false\u0026#34; KRATOS_PUBLIC_URL: \u0026#34;https://kratos.enkinineveh.space/app\u0026#34; KRATOS_ADMIN_URL: \u0026#34;http://kratos-admin.auth\u0026#34; To keep clean seperation, I created a seperate auth folder, Now let\u0026rsquo;s deploy both services and test it out:\n1 2 3 4 5 6 7 8 9 releases: - name: kratos chart: ory/kratos namespace: exam values: - kratos/kratos-values.yaml - name: kratos-ui chart: ./kratos-ui-node/charts namespace: exam 1 helmfile apply and here it goes, let’s head into registration to create a user, then if we try to login, it\u0026rsquo;ll prevent us, so we must verify the user first using the verification form.\nYour browser does not support the video tag. Next thing is deploying oathkeeper and sharing this session to authenticate the user.\nOathkeeper Oathkeeper has two components API and Proxy, we will pick the proxy service to live in front of the exam-gen and exam-taking UI.\nThe Oathkeeper Proxy service has an access list to decide which request should be allowed or deny it. For example we can enable a cookie_session on url exam-generate-frontend that reference exam-generate internal upstream service\noathkeeper values.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 oathkeeper: # -- The ORY Oathkeeper configuration. For a full list of available settings, check: # https://github.com/ory/oathkeeper/blob/master/docs/config.yaml accessRules: | [ { \u0026#34;id\u0026#34;: \u0026#34;exam-generation-rule\u0026#34;, \u0026#34;upstream\u0026#34;: { \u0026#34;url\u0026#34;: \u0026#34;http://exam-generation-frontend-charts:8501\u0026#34; }, \u0026#34;match\u0026#34;: { \u0026#34;url\u0026#34;: \u0026#34;https://exam-generate-frontend.enkinineveh.space/\u0026lt;.*\u0026gt;\u0026#34;, \u0026#34;methods\u0026#34;: [ \u0026#34;GET\u0026#34;, \u0026#34;POST\u0026#34;, \u0026#34;OPTIONS\u0026#34;, \u0026#34;PUT\u0026#34;, \u0026#34;PATCH\u0026#34; ] }, \u0026#34;authenticators\u0026#34;: [ { \u0026#34;handler\u0026#34;: \u0026#34;cookie_session\u0026#34; } ], \u0026#34;authorizer\u0026#34;: { \u0026#34;handler\u0026#34;: \u0026#34;allow\u0026#34; }, \u0026#34;mutators\u0026#34;: [{ \u0026#34;handler\u0026#34;: \u0026#34;header\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;headers\u0026#34;: { \u0026#34;X-USER-EMAIL\u0026#34;: \u0026#34;{{ print .Extra.identity }}\u0026#34; } } }], \u0026#34;errors\u0026#34;:[ { \u0026#34;handler\u0026#34;:\u0026#34;redirect\u0026#34; } ] }, also we saw in previous article that the front-take exam needs the authenticated user’s email.\nFrom the oathkeeper documentation we can leverage that with the “header” mutator, but I tried it many times and couldn\u0026rsquo;t get it to work. If you have any idea how to make it work, I will be pleased to talk.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 { \u0026#34;id\u0026#34;: \u0026#34;exam-taking-rule\u0026#34;, \u0026#34;upstream\u0026#34;: { \u0026#34;url\u0026#34;: \u0026#34;http://exam-taking-frontend-charts:8501\u0026#34; }, \u0026#34;match\u0026#34;: { \u0026#34;url\u0026#34;: \u0026#34;https://exam-taking-frontend.enkinineveh.space/\u0026lt;.*\u0026gt;\u0026#34;, \u0026#34;methods\u0026#34;: [ \u0026#34;GET\u0026#34;, \u0026#34;POST\u0026#34;, \u0026#34;OPTIONS\u0026#34;, \u0026#34;PUT\u0026#34;, \u0026#34;PATCH\u0026#34;, \u0026#34;HEAD\u0026#34; ] }, \u0026#34;authenticators\u0026#34;: [ { \u0026#34;handler\u0026#34;: \u0026#34;cookie_session\u0026#34; } ], \u0026#34;authorizer\u0026#34;: { \u0026#34;handler\u0026#34;: \u0026#34;allow\u0026#34; }, \u0026#34;mutators\u0026#34;: [{ \u0026#34;handler\u0026#34;: \u0026#34;header\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;headers\u0026#34;: { \u0026#34;X-USER-EMAIL\u0026#34;: \u0026#34;{{ print .Extra.identity }}\u0026#34; } } }], \u0026#34;errors\u0026#34;: [ { \u0026#34;handler\u0026#34;:\u0026#34;redirect\u0026#34; } ] } But you may ask yourself if oathkeeper and kratos are two separate services , how can oathkeeper verify the kratos cookie_session ?\nWell kratos API provides the URL: /sessions/whoami to verify cookies, so when adding the cookie_session authenticator, we pass that url with few extra parameters\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 authenticators: cookie_session: enabled: true config: check_session_url: http://kratos-public.exam/sessions/whoami # this reference kratos internal url forward_http_headers: - Cookie - X-USER-EMAIL preserve_path: true extra_from: \u0026#34;@this\u0026#34; # kratos will be configured to put the subject from the IdP here subject_from: \u0026#34;identity.traits.email\u0026#34; only: - ory_kratos_session and one last thing in the configuration side is telling oathkeeper to redirect unauthorised requests to kratos login page and the return_to_query_param property is for redirecting user back to UI after successfully login\n1 2 3 4 5 6 7 8 9 10 errors: handlers: redirect: enabled: true config: to: https://kratos.enkinineveh.space/login return_to_query_param: \u0026#34;return_to\u0026#34; when: - error: - unauthorized and we dont\u0026rsquo;t forget to add the frontend domain into kratos allowed_return_urls, so requests cannot get blocked\n1 2 3 4 5 selfservice: allowed_return_urls: - https://kratos.enkinineveh.space/ - https://exam-taking-frontend.enkinineveh.space/ - https://exam-generate-frontend.enkinineveh.space/ Proxy service will be deployed in front of the UI apps, so we update the ingress of the both frontend apps to redirect traffic to oathkeeper-proxy dns and othkeeper proxy will decide based on the access rules if the request should be allowed or not.\nexam-taking-app values.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 hosts: - host: exam-taking-frontend.enkinineveh.space paths: - path: / pathType: ImplementationSpecific backend: service: name: oathkeeper-proxy port: number: 4455 - path: /_stcore/stream pathType: ImplementationSpecific backend: service: name: exam-taking-frontend-charts port: number: 8501 exam-generate-app values.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 hosts: - host: exam-generate-frontend.enkinineveh.space paths: - path: / pathType: ImplementationSpecific backend: service: name: oathkeeper-proxy port: number: 4455 - path: /_stcore/stream pathType: ImplementationSpecific backend: service: name: exam-generation-frontend-charts port: number: 8501 here is a demo for authorization\nYour browser does not support the video tag. After registration \u0026amp; Emailing PostSignUp Function We nearly implement all the functionalities Cognito provides in this architecture but there is one missing feature, which is calling a PostSignUp function after registration to subscribe the educator to an Amazon Simple Notification Service , but in our case, we call PostSignUp to add a educator email in subscribers table so then the email-fn retrieve the email to send notifications to.\nWe start by creating the function, it should serve two main roles: intercepting the event and persisting the email inside a dynamodb table. The handler function will get the email from event['traits']['email'] as this the format kratos webhook use, then we created a mongo client and inserted the email into MONGO_TABLE_NAME .\nSo here the code of main.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import os from pymongo import MongoClient def create_subscriber(email:str) -\u0026gt; list[str]: # Initialize DynamoDB table table_name = os.getenv(\u0026#34;MONGO_TABLE_NAME\u0026#34;) client = MongoClient(os.getenv(\u0026#34;MONGO_URI\u0026#34;)) db = client[\u0026#34;exams\u0026#34;] collection = db[table_name] subscribers = collection.insert_one({\u0026#34;email\u0026#34;:email}) return subscribers def handler(event,context): email = event[\u0026#39;traits\u0026#39;][\u0026#39;email\u0026#39;] create_subscriber(email) Wrap it inside FastAPI, build the image and push it into the registry. Then we reference the image and add the environment variables to values.yaml as always:\n1 2 3 4 5 6 7 8 9 10 11 image: repository: gitea.enkinineveh.space/gitea_admin/post-signup-fn pullPolicy: IfNotPresent # Overrides the image tag whose default is the chart appVersion. tag: \u0026#34;v1\u0026#34; env: normal: MONGO_URI: \u0026#34;mongodb://databaseAdmin:sHWKYbXRalmNExTMiYr@my-cluster-name-rs0.mongo.svc.cluster.local/admin?replicaSet=rs0\u0026amp;ssl=false\u0026#34; MONGO_TABLE_NAME: \u0026#34;subscribers\u0026#34; After deploying the chart, we retrive the service URL:\n1 2 3 4 kubectl get kservice -n exam post-signup-fn-charts NAME URL LATESTCREATED LATESTREADY READY REASON post-signup-fn-charts http://post-signup-fn-charts.exam.svc.cluster.local post-signup-fn-charts-00001 post-signup-fn-charts-00001 True The other part of the puzzle is configuring kratos webhook to send educator only email to this functions, but how shall we identity educator emails from the student ones ?.\nThe answer to this question is adding another choice field contains user types, either: educator or student. To achieve this, we will leverage the \u0026ldquo;custom identity schema\u0026rdquo; kratos provides, we already implemented the concept in previous step but we didn\u0026rsquo;t talk about it.\nIdentity schema is the form you\u0026rsquo;re seeing when you access the registration page, by default it comes with just email, password, fullanme , but we can customize it, to do that we change identity.schemas property. one side note is the identity schema use json-schema to create and validate forms\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 { \u0026#34;$id\u0026#34;: \u0026#34;https://schemas.ory.sh/presets/kratos/identity.email.schema.json\u0026#34;, \u0026#34;$schema\u0026#34;: \u0026#34;http://json-schema.org/draft-07/schema#\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Person\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;traits\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;email\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;email\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;E-Mail\u0026#34;, \u0026#34;ory.sh/kratos\u0026#34;: { \u0026#34;credentials\u0026#34;: { \u0026#34;password\u0026#34;: { \u0026#34;identifier\u0026#34;: true } }, \u0026#34;recovery\u0026#34;: { \u0026#34;via\u0026#34;: \u0026#34;email\u0026#34; }, \u0026#34;verification\u0026#34;: { \u0026#34;via\u0026#34;: \u0026#34;email\u0026#34; } } }, \u0026#34;fullname\u0026#34;:{ \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Full Name\u0026#34; }, \u0026#34;phone_number\u0026#34;:{ \u0026#34;type\u0026#34;:\u0026#34;number\u0026#34;, \u0026#34;title\u0026#34;:\u0026#34;Phone Number\u0026#34; }, \u0026#34;user_type\u0026#34;: { # we add this field \u0026#34;title\u0026#34;:\u0026#34;User Type\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;enum\u0026#34;: [\u0026#34;student\u0026#34;, \u0026#34;educator\u0026#34;], \u0026#34;default\u0026#34;: \u0026#34;student\u0026#34; } }, \u0026#34;required\u0026#34;: [ \u0026#34;email\u0026#34;, \u0026#34;fullname\u0026#34;, \u0026#34;phone_number\u0026#34;, \u0026#34;user_type\u0026#34; ], \u0026#34;additionalProperties\u0026#34;: false } } } Now we encode(base64) the following schema and pass it to identity.schemas:\n1 2 3 4 5 identity: default_schema_id: default schemas: - id: default url: base64://ew0KICAiJGlkIjogImh0dHBzOi8vc2NoZW1hcy5vcnkuc2gvcHJlc2V0cy9rcmF0b3MvaWRlbnRpdHkuZW1haWwuc2NoZW1hLmpzb24iLA0KICAiJHNjaGVtYSI6ICJodHRwOi8vanNvbi1zY2hlbWEub3JnL2RyYWZ0LTA3L3NjaGVtYSMiLA0KICAidGl0bGUiOiAiUGVyc29uIiwNCiAgInR5cGUiOiAib2JqZWN0IiwNCiAgInByb3BlcnRpZXMiOiB7DQogICAgInRyYWl0cyI6IHsNCiAgICAgICJ0eXBlIjogIm9iamVjdCIsDQogICAgICAicHJvcGVydGllcyI6IHsNCiAgICAgICAgImVtYWlsIjogew0KICAgICAgICAgICJ0eXBlIjogInN0cmluZyIsDQogICAgICAgICAgImZvcm1hdCI6ICJlbWFpbCIsDQogICAgICAgICAgInRpdGxlIjogIkUtTWFpbCIsDQogICAgICAgICAgIm9yeS5zaC9rcmF0b3MiOiB7DQogICAgICAgICAgICAiY3JlZGVudGlhbHMiOiB7DQogICAgICAgICAgICAgICJwYXNzd29yZCI6IHsNCiAgICAgICAgICAgICAgICAiaWRlbnRpZmllciI6IHRydWUNCiAgICAgICAgICAgICAgfQ0KICAgICAgICAgICAgfSwNCiAgICAgICAgICAgICJyZWNvdmVyeSI6IHsNCiAgICAgICAgICAgICAgInZpYSI6ICJlbWFpbCINCiAgICAgICAgICAgIH0sDQogICAgICAgICAgICAidmVyaWZpY2F0aW9uIjogew0KICAgICAgICAgICAgICAidmlhIjogImVtYWlsIg0KICAgICAgICAgICAgfQ0KICAgICAgICAgIH0NCiAgICAgICAgfSwNCiAgICAgICAgImZ1bGxuYW1lIjp7DQogICAgICAgICAgInR5cGUiOiJzdHJpbmciLA0KICAgICAgICAgICJ0aXRsZSI6ICJGdWxsIE5hbWUiDQogICAgICAgIH0sDQogICAgICAgICJwaG9uZV9udW1iZXIiOnsNCiAgICAgICAgICAidHlwZSI6Im51bWJlciIsDQogICAgICAgICAgInRpdGxlIjoiUGhvbmUgTnVtYmVyIg0KICAgICAgICB9LA0KICAgICAgICAidXNlcl90eXBlIjogew0KICAgICAgICAgICJ0aXRsZSI6IlVzZXIgVHlwZSIsDQogICAgICAgICAgInR5cGUiOiAic3RyaW5nIiwNCiAgICAgICAgICAiZW51bSI6IFsic3R1ZGVudCIsICJlZHVjYXRvciJdLCANCiAgICAgICAgICAiZGVmYXVsdCI6ICJzdHVkZW50Ig0KICAgICAgICB9DQogICAgICB9LA0KICAgICAgInJlcXVpcmVkIjogWw0KICAgICAgICAiZW1haWwiLA0KICAgICAgICAiZnVsbG5hbWUiLA0KICAgICAgICAicGhvbmVfbnVtYmVyIiwidXNlcl90eXBlIg0KICAgICAgXSwNCiAgICAgICJhZGRpdGlvbmFsUHJvcGVydGllcyI6IGZhbHNlDQogICAgfQ0KICB9DQp9 here is the new form:\nGetting back to the webhook configuration, we will set an after registration webhook\n1 2 3 4 5 6 7 8 9 10 11 registration: ui_url: https://kratos.enkinineveh.space/registration after: hooks: - hook: web_hook config: url: http://post-signup-fn-charts.exam.svc.cluster.local body: base64://empty_for_now method: \u0026#34;POST\u0026#34; can_interrupt: false emit_analytics_event: false you may notice we didn\u0026rsquo;t supply the body, based on the documentation, kratos use jsonnet to define the logic.\nThe code will intercept the event and pass only educator emails\n1 2 3 4 5 6 7 8 9 function(ctx) if ctx.identity.traits.user_type == \u0026#34;student\u0026#34; then error \u0026#34;cancel\u0026#34; else { \u0026#34;traits\u0026#34;: { \u0026#34;email\u0026#34;: ctx.identity.traits.email } } Now we encode it as base64 and pass it to the body param\n1 2 3 config: url: http://post-signup-fn-charts.exam.svc.cluster.local body: base64://ZnVuY3Rpb24oY3R4KQ0KaWYgY3R4LmlkZW50aXR5LnRyYWl0cy51c2VyX3R5cGUgPT0gInN0dWRlbnQiIHRoZW4NCiAgZXJyb3IgImNhbmNlbCINCmVsc2UNCiAgew0KICAgICJ0cmFpdHMiOiB7DQogICAgICAiZW1haWwiOiBjdHguaWRlbnRpdHkudHJhaXRzLmVtYWlsDQogICAgfQ0KICB9 We update the release and test the registration process for an educator account. if the registration ran successfully we will see a document has been created on subscribers table.\nYour browser does not support the video tag. Sending Email Can you guess the last missing element of the puzzle ? It\u0026rsquo;s the emailing part, Sending an email when the exam is generated or the student has answered the questions. the knative-sevice will get triggered whenever a message reaches the email-topic we created on the previous steps.\nthe main.py in the knative service will be composed of 2 functions:\nget_subscribers: to retrieve emails from the subscribers table. send_email: will decode the event message and send it to the list of emails we have so here is the code\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 from email.mime.text import MIMEText from email.message import EmailMessage import smtplib from requests import HTTPError import requests from pymongo import MongoClient import os API_KEY = os.environ[\u0026#39;MAILGUN_API_KEY\u0026#39;] def get_subscribers() -\u0026gt; list[str]: # Initialize DynamoDB table table_name = os.getenv(\u0026#34;MONGO_TABLE_NAME\u0026#34;) client = MongoClient(os.getenv(\u0026#34;MONGO_URI\u0026#34;)) db = client[\u0026#34;exams\u0026#34;] collection = db[table_name] subscribers = collection.find() return list(e[\u0026#39;email\u0026#39;] for e in subscribers ) def send_mail(subs,subject,body): requests.post( \u0026#34;https://api.mailgun.net/v3/xxxxxxx.mailgun.org/messages\u0026#34;, auth=(\u0026#34;api\u0026#34;, API_KEY), data={ \u0026#34;from\u0026#34;: \u0026#34;Excited User \u0026lt;no-reply@enkinineveh.space\u0026gt;\u0026#34;, \u0026#34;to\u0026#34;: subs, \u0026#34;subject\u0026#34;: subject, \u0026#34;text\u0026#34;: body, }, ) def main(event, context): body = event.decode() subs = get_subscribers() send_mail(subs,\u0026#34;HELLO\u0026#34;, body) finally build and push the image and the reference it in values.yaml in addtiontion to passing the enviornment variables.\n1 2 3 4 5 6 7 env: normal: BOOTSTRAP_SERVER: kafka-cluster-kafka-bootstrap.strimzi.svc:9092 MAILGUN_API_KEY: xxxxx MONGO_TABLE_NAME: subscribers MONGO_URI: \u0026#34;mongodb://databaseAdmin:sHWKYbXRalmNExTMiYr@my-cluster-name-rs0.mongo.svc.cluster.local/admin?replicaSet=rs0\u0026amp;ssl=false\u0026#34; TOPIC_ARN: email-topic We clearly done setting up everything, I will test the whole process from registring user and receiving verification code, generating an exam and receiving the email when it\u0026rsquo;s ready, answering the questions and getting the scoreboard mail.\nYour browser does not support the video tag. Summary and That\u0026rsquo;s it, we transformed the architecture into a pure open source project.\n","permalink":"//localhost:1313/posts/transform-aws-5/","summary":"Introduction In this article, we’re going to replace the Cognito Service, I choose the Kratos and Oathkeeper from Ory as alternative.\nThe main functionalities of Congito here, is offering a way to sign in, sign up with or without SSO, email verification and use Sesssion Web Token for Frontend authorization.\nHere is the full architecture of authentication and authorization\nDon\u0026rsquo;t worry if didnt understand the architecture, we will dig deeper in the next headlines","title":"Transform AWS Exam Generator Architecture to Open Source Part #5: Authentication and Emailing"},{"content":"Brief Description In this article, we create the passing part of the exam, the architecture is composed of:\nkubernetes service for Taking Exam UI. Knative service for taking exams. MongoDb for storing student answers. KafkaConnect to capture changed data on MongoDb and move it to a KafkaTopic. Knative service for calculating the scoreboard and send it into a topic. We will follow the same approach in the previous article, we tackle dependency-free services first. the the UI app will require the knative service and mongo to work, so we start with knative service and mongo and then we move into UI and finally the Kafka connect integration with mongodb and knative.\nKnative Fn We download the function take_exam.py file, create a FastAPI wrapper but this time there is no event instead the function is called directly from the UI.\nIf we inspect the take_exam.py code, we can see it’s expecting a queryStringParameters dict inside the event and inside it a string called: object_name which will be referencing the exam file.\nSo here is the app.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 @app.get(\u0026#34;/\u0026#34;) def intercept_event(object_name:Union[str,None]=None): if object_name: event = { \u0026#34;queryStringParameters\u0026#34;: { \u0026#34;object_name\u0026#34;: object_name } } else: event = { \u0026#34;queryStringParameters\u0026#34;: {} } result = lambda_handler(event=event,context={}) return result Cool, now we build, push\n1 docker buildx build -t gitea.enkinineveh.space/gitea_admin/exam-take-fn:v1 . --push and reference the image and we don’t forget the Minio environment values.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 image: repository: gitea.enkinineveh.space/gitea_admin/exam-take-fn pullPolicy: IfNotPresent # Overrides the image tag whose default is the chart appVersion. tag: \u0026#34;v1\u0026#34; ... env: normal: BUCKET_NAME: \u0026#34;exams\u0026#34; secret: AWS_ACCESS_KEY_ID: \u0026#34;minio\u0026#34; AWS_SECRET_ACCESS_KEY: \u0026#34;minio123\u0026#34; AWS_ENDPOINT_URL_S3: \u0026#34;http://minions-hl.minio-tenant:9000\u0026#34; After adding the helm chart to helmfile release and apply it, we inspect the knative service to get the ingress URL, which will be used by the frontend later.\n1 2 3 4 kubectl get kservice exam-taking-fn -n exam NAME URL LATESTCREATED LATESTREADY READY REASON exam-taking-fn http://exam-taking-fn.exam.svc.cluster.local exam-taking-fn-00001 exam-taking-fn-00001 True Mongodb Next part is Mongodb, we will use the percona operator for managing the MongoDb cluster\nfirst start by installing the chart\n1 2 3 4 5 6 7 8 9 repositories: ... - name: percona url: https://percona.github.io/percona-helm-charts/ releases: ... - name: percona-operator chart: percona/psmdb-operator namespace: mongo Then we copy paste the following script to deploy a Cluster with 3 replicas across nodes “kubernetes.io/hostname” and a 1Gb of storage for now:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion: psmdb.percona.com/v1 kind: PerconaServerMongoDB metadata: name: mongo-cluster namespace: mongo spec: image: percona/percona-server-mongodb:4.4.6-8 replsets: - name: rs0 size: 3 volumeSpec: persistentVolumeClaim: resources: requests: storage: 1Gi Rerun helmfile apply, wait a few minutes and here the cluster status is ready, and the endpoint for kubernetes internal access.\nBut for database access we need credentials, which will be found on a secret named mongodb-cluster-secret.\nFrontend UI app As both dependencies: exam-take function and mongodb are created, we’re able to deploy the exam-take frontend now.\nHaving a look at the frontend code, the application needs a:\nAPI_GATEWAY_URL which is in our case the knative service url we got before:\n1 2 3 env: normal: API_GATEWAY_URL: \u0026#34;http://exam-taking-fn.exam.svc.cluster.local\u0026#34; 1 API_GATEWAY_URL = os.getenv(\u0026#39;API_GATEWAY_URL\u0026#39;) A token to decode the authenticated user’s email, we will escape this part for now as it requires an authenticated service, we will pass a static email for every student.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # headers = _get_websocket_headers() # token = headers.get(\u0026#39;X-Amzn-Oidc-Data\u0026#39;) # parts = token.split(\u0026#39;.\u0026#39;) # if len(parts) \u0026gt; 1: # payload = parts[1] # # Decode the payload # decoded_bytes = base64.urlsafe_b64decode(payload + \u0026#39;==\u0026#39;) # Padding just in case # decoded_str = decoded_bytes.decode(\u0026#39;utf-8\u0026#39;) # decoded_payload = json.loads(decoded_str) # # Extract the email # email = decoded_payload.get(\u0026#39;email\u0026#39;, \u0026#39;Email not found\u0026#39;) # print(email) # else: # print(\u0026#34;Invalid token\u0026#34;) email = \u0026#34;tunis@gmail.com\u0026#34; And to save the answers, a Dynamodb was used. We replaced the code with MongoClient and passed the host and table name as environment variables.\nquiz.py\n1 2 3 4 5 6 7 8 9 10 11 from pymongo import MongoClient ... def save_quiz_results(data): # Initialize DynamoDB table table_name = os.getenv(\u0026#39;MONGO_TABLE_NAME\u0026#39;) client = MongoClient(os.getenv(\u0026#34;MONGO_URI\u0026#34;)) db = client[\u0026#39;exams\u0026#39;] collection = db[table_name] collection.insert_one(data) ... values.yaml\n1 2 3 4 env: normal: MONGO_URI: \u0026#34;mongodb://databaseAdmin:sHWKYbXRalmNExTMiYr@my-cluster-name-rs0.mongo.svc.cluster.local/admin?replicaSet=rs0\u0026amp;ssl=false\u0026#34; MONGO_TABLE_NAME: \u0026#34;score\u0026#34; We add the pymongo package in the requirements.txt file, then build and push.\nWe do the same as the generation front-end app, reference the image, change port, enable ingress, specify websocket service and done\n1 docker buildx build -t gitea.enkinineveh.space/gitea_admin/exam-take-frontend . --push 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 image: repository: gitea.enkinineveh.space/gitea_admin/exam-take-frontend pullPolicy: IfNotPresent # Overrides the image tag whose default is the chart appVersion. tag: \u0026#34;v1\u0026#34; service: type: ClusterIP port: 8501 ingress: enabled: true className: \u0026#34;nginx\u0026#34; annotations: nginx.org/proxy-connect-timeout: \u0026#34;3600s\u0026#34; nginx.org/proxy-read-timeout: \u0026#34;3600s\u0026#34; nginx.org/client-max-body-size: \u0026#34;4m\u0026#34; nginx.org/proxy-buffering: \u0026#34;false\u0026#34; nginx.org/websocket-services: exam-taking-frontend-charts hosts: - host: exam-taking-frontend.enkinineveh.space paths: - path: / pathType: ImplementationSpecific backend: service: name: exam-taking-frontend-charts port: number: 8501 tls: - secretName: enkinineveh.space-tls-prod hosts: - exam-taking-frontend.enkinineveh.space Add the chart into helmfile, apply it\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 releases: .... - name: exam-generation-frontend chart: ./frontend/exam-generation-app/charts namespace: \u0026#34;exam\u0026#34; - name: exam-taking-frontend # we added this chart: ./frontend/exam-taking-app/charts namespace: \u0026#34;exam\u0026#34; - name: exam-generation-fn chart: ./knative/ExamGenFn/charts namespace: \u0026#34;exam\u0026#34; - name: exam-taking-fn # we added this chart: ./knative/ExamTakeFn/charts namespace: \u0026#34;exam\u0026#34; and we can access the UI Now. Now we try the whole thing, remove past files and start, upload the supposed test pdf file, wait for generation, check the taking-exam app, we see it listed the exam, we click, and it generates a form, we answer and voila.\nFinally let’s check the mongodb table for the answers, we list documents inside the score table, everything looks good.\nhere is a demo for more visual pleasing experience\nYour browser does not support the video tag. Kafka Connect \u0026amp; Function One of the reasons I choose Kafka besides its seamless integration with knative, is the ability to capture data changes (CDC) from other sources and forward it to topics, this ability is called Kafka Connect.\nKafka Connect is a tool for scalably and reliably streaming data between Apache Kafka® and other data systems. If we want to capture data the time it gets added into Mongodb, we must configure the Kafka Connect to use the Mongodb Connector which internally listens for Mongodb CDC.\nKafka Connect will be deployed as a separate Cluster but under the same kubernetes cluster of course\nHere is the yaml kafka-connect.yaml for creating the cluster:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion: kafka.strimzi.io/v1beta2 kind: KafkaConnect metadata: name: connect-cluster namespace: strimzi annotations: strimzi.io/use-connector-resources: \u0026#34;true\u0026#34; spec: replicas: 1 image: gitea.enkinineveh.space/gitea_admin/connect-debezium:v2 bootstrapServers: kafka-cluster-kafka-bootstrap.strimzi.svc:9092 config: group.id: connect-cluster offset.storage.topic: connect-cluster-offsets config.storage.topic: connect-cluster-configs status.storage.topic: connect-cluster-status logging: type: inline loggers: connect.root.logger.level: \u0026#34;INFO\u0026#34; bootstrapServers for connecting to the Kafka server. group.id unique id for defining group of workers And those config are internal topic for managing connector and task status, configuration, storage data And the most important property is image, the image must be from strimzi/Kafka, and it should add the mongodb connector under plugins folder to be used later by connectors, I deployed my own image , you can find the code in the repo. Setting use-connector-resources to true enables KafkaConnectors to create, remove, and reconfigure connectors The last interesting part now, is adding the connector:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion: kafka.strimzi.io/v1beta2 kind: KafkaConnector metadata: name: mongodb-source-connector-second namespace: strimzi labels: strimzi.io/cluster: connect-cluster spec: class: io.debezium.connector.mongodb.MongoDbConnector tasksMax: 2 config: mongodb.connection.string: mongodb://databaseAdmin:sHWKYbXRalmNExTMiYr@my-cluster-name-rs0.mongo.svc.cluster.local/admin?replicaSet=rs0\u0026amp;ssl=false topic.prefix: \u0026#34;mongo-trigger-topic\u0026#34; database.include.list: \u0026#34;exams\u0026#34; collection.include.list: \u0026#34;score\u0026#34; We specify the class for MongoDbConnector plugin, and the last properties are needed for mongodb, here the topic prefix is “mongo-trigger-topic”, but we must create a topic with following name, mongo-trigger-topic.exam.score, why?\nBecause KafkaConnector use this format: prefix.db.colletion to forward events, means if we add record in score collection the topic name should, \u0026lt;topic.prefix\u0026gt;.exam.score\nSo here is the code for topic creation:\n1 2 3 4 5 6 7 8 9 10 apiVersion: kafka.strimzi.io/v1beta1 kind: KafkaTopic metadata: name: mongo-trigger-topic.exams.score namespace: strimzi labels: strimzi.io/cluster: \u0026#34;kafka-cluster\u0026#34; spec: partitions: 2 One thing to notice is the event format that will be forwarded from kafka connect, here is an example of the format:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 { \u0026#34;payload\u0026#34;: { \u0026#34;before\u0026#34;: null, \u0026#34;after\u0026#34;: \u0026#34;{\\\u0026#34;_id\\\u0026#34;: {\\\u0026#34;$oid\\\u0026#34;: \\\u0026#34;66a279b8f98a9d0379570575\\\u0026#34;},\\\u0026#34;email\\\u0026#34;: \\\u0026#34;tunisia@gmail.com\\\u0026#34;,\\\u0026#34;score\\\u0026#34;: 0,\\\u0026#34;result\\\u0026#34;: \\\u0026#34;failed\\\u0026#34;,\\\u0026#34;details\\\u0026#34;: [{\\\u0026#34;question\\\u0026#34;: \\\u0026#34;Which country of those countries located in balkans ?\\\u0026#34;,\\\u0026#34;user_answer\\\u0026#34;: \\\u0026#34;Germany\\\u0026#34;,\\\u0026#34;correct_answer\\\u0026#34;: \\\u0026#34;Romania\\\u0026#34;,\\\u0026#34;is_correct\\\u0026#34;: false}]}\u0026#34;, \u0026#34;updateDescription\u0026#34;: null, \u0026#34;source\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;2.7.0.Final\u0026#34;, \u0026#34;connector\u0026#34;: \u0026#34;mongodb\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;email-topic\u0026#34;, \u0026#34;ts_ms\u0026#34;: 1721924024000, \u0026#34;snapshot\u0026#34;: \u0026#34;false\u0026#34;, \u0026#34;db\u0026#34;: \u0026#34;exams\u0026#34;, \u0026#34;sequence\u0026#34;: null, \u0026#34;ts_us\u0026#34;: 1721924024000000, \u0026#34;ts_ns\u0026#34;: 1721924024000000000, \u0026#34;collection\u0026#34;: \u0026#34;score\u0026#34;, \u0026#34;ord\u0026#34;: 1, \u0026#34;lsid\u0026#34;: null, \u0026#34;txnNumber\u0026#34;: null, \u0026#34;wallTime\u0026#34;: null }, \u0026#34;op\u0026#34;: \u0026#34;c\u0026#34;, \u0026#34;ts_ms\u0026#34;: 1721924024568, \u0026#34;transaction\u0026#34;: null } } as we\u0026rsquo;re adding a new data the before property is empty and the after should contains the persisted data.\nFrom the architecture you may notice we need a function to consume the stored events in mongo-trigger-topic.exam.score. The code is using SNS and DynamoDb we replace them with Kafka and mongo. We start by initialising the KafkaProducer for sending to email Topic. The dynamodb_to_json function will be replaced by mongodb_to_json.\n1 2 3 4 5 6 7 8 9 10 ... from kafka import KafkaProducer producer = KafkaProducer(bootstrap_servers=os.environ[\u0026#34;KAFKA_BOOTSTRAP_SERVER\u0026#34;]) # Utility function to convert DynamoDB item to regular JSON def mongodb_to_json(mongo_item): return json.loads(mongo_item) ... Then we remove the for loop as knative doesn’t use batching stream and replaceit with a code to get event data from ‘after’ property and at the end we send messsage to Email Topic.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 ... def lambda_handler(event, context): topic_arn = os.environ[\u0026#39;EMAIL_TOPIC_ARN\u0026#39;] image = event[\u0026#39;payload\u0026#39;][\u0026#39;after\u0026#39;] # Convert DynamoDB JSON to regular JSON item_json = mongodb_to_json(image) # Format the message as a score card message = format_score_card(item_json) try: producer.send(topic_arn, message.encode()) except Exception as e: print(f\u0026#34;Error sending Kafka notification: {e}\u0026#34;) raise return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(\u0026#39;Lambda executed successfully!\u0026#39;) } ... as the Kafka Connector will persist the added data into a topic, we should create a KafkaSource to consume it and forward it to the mongo function.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: sources.knative.dev/v1beta1 kind: KafkaSource metadata: name: mongo-source spec: consumerGroup: kafka-group bootstrapServers: - {{ .Values.env.normal.KAFKA_BOOTSTRAP_SERVER }} topics: - {{ .Values.env.normal.SOURCE_TOPIC_ARN }} consumers: 2 sink: ref: apiVersion: serving.knative.dev/v1 kind: Service name: {{ include \u0026#34;charts.fullname\u0026#34; . }} and finally pass environment variables to helm values.yaml\n1 2 3 4 5 6 ... env: normal: KAFKA_BOOTSTRAP_SERVER: kafka-cluster-kafka-bootstrap.strimzi.svc:9092 SOURCE_TOPIC_ARN: mongo-trigger-topic.exams.score EMAIL_TOPIC_ARN: email-topic Now let’s test the entire process, we first open a side terminal for watching email-topic data. Visit the frontend app, select an exam, answer the questions, wait for second and here is the data sent to email-topic.\nYour browser does not support the video tag. Conclusion Now, we finished both parts, the generation and the taking of the exam parts, we will move into securing the access to authenticated accounts and later send notification to educator about student scores.\n","permalink":"//localhost:1313/posts/transform-aws-4/","summary":"Brief Description In this article, we create the passing part of the exam, the architecture is composed of:\nkubernetes service for Taking Exam UI. Knative service for taking exams. MongoDb for storing student answers. KafkaConnect to capture changed data on MongoDb and move it to a KafkaTopic. Knative service for calculating the scoreboard and send it into a topic. We will follow the same approach in the previous article, we tackle dependency-free services first.","title":"Transform AWS Exam Generator Architecture to Open Source Part #4: Exam Passing"},{"content":"Brief description In this part we should talk about:\nKafka topics and difference of deploying between zookeeper and kraft. Create Minio cluster and k8s jobs for adding event notification Knative installation and the invocation with Kafka topics. Hosting of generate-exam frontend in k8s services with ingress, subdomain and reference the WebSocket service. Current Stack I have a K8s cluster composed of three nodes (1 master, 2 control plane) with Talos as the running OS, MetalLB deployed as a load balancer combined with Nginx (nginx.io) as an ingress controller.\nIngress hosts that we will see are mapped to my Cloudflare domain(enkinineveh.space) secured with TLS certs generated using cert-manager and letsencrypt. One side note is that am using reflector to share TLS secret into other namespaces.\nHelm is my favorite tool for deploying resources, Helmfile in case of deploying multiple charts, and k9s for navigating resources.\nGitea is my baby GitHub and CNPG is the default database operator.\nIntroduction The processing phase is divided into 2 parts: the generation and the passing of the exam.\nThe generation part is composed of 5 key elements: frontend UI, Minio, Kafka topics, Knative, Bedrock.\nWhen you create an architecture, you must tackle the dependency-free component first, here as an example, for Minio to send notifications it needs a destination in our case it means a topic, and for Knative to get triggered it also needs a source, a topic also, so that leads to Kafka being the first element to create followed by Minio and Knative.\nFor installing packages in Kubernetes, Helm is my love language, but to deploy multiple charts together helmfile is the get-go, because it assembles charts and deploys them as a single stack.\nKafka Now to deploy a Kafka cluster, we need an operator to handle all the heavy and tedious tasks of managing the cluster, which will lead us into Strimzi.\nStrimzi offers a way to run an Apache Kafka cluster on Kubernetes in various deployment configurations, before deploying we must understand the different architecture Kafka comes with.\nKafka have two deployment methods: kraft and zookeeper, the zookeeper method was creating a separate controller for managing metadata that can leads into burden or latency during heavy load environment, in the other side kraft managed to introduce a new method of handling the metadata within Kafka itself.\nFrom my experience, as we do not have that much of workload, deploying Kafka with zookeeper is sufficient, but for more up to date approach we will deploy the cluster with kraft mode\nInternally Kafka is composed of multiple components, which are: producer, consumer, broker, topics and partitions.\nTo understand the components better, let’s take the generation phase as an example.\nProducers are the event emitter, like Minio bucket notification.\nConsumers are the event receiver, Knative function is the case, as it consumes the events the moment they reach the topic.\nBroker is the connection point between the producer and the consumer, topics on the other side are subcomponents of the broker. As best practice each topic should handle a single goal.\nNow Let’s start with installing the operator first, I found the documentation highly informative to starts with.\nInstalling the operator with kraft mode, will require other feature gates like: KafkaNodePool, UseKraft and UnidirectionalTopicOperator, we add those values into featureGate property in strimzi-values.yaml file\n1 featureGates: +UseKRaft,+KafkaNodePools,+UnidirectionalTopicOperator we create a a repository and a release inside the helmfile.yaml\n1 2 3 4 5 6 7 8 9 repositories: - name: kafka-operator url: https://strimzi.io/charts releases: - name: kafka-operator chart: kafka-opertor/strimzi-kafka-operator values: - ./kafka-values.yaml and then run\n1 helmfile apply Now our operator is ready, we continue into deploying KafkaNodePool and Kraft Cluster\nKafkaNodePool is an essential part for the kraft mode cluster, because it defines a set of node pools to install kafka cluster on and has multiple configurations like number of replicas, roles of nodes, storage configuration, resource requirements, etc…\nHere is the yaml file:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: kafka.strimzi.io/v1beta2 kind: KafkaNodePool metadata: name: broker_controller labels: strimzi.io/cluster: kafka-cluster spec: replicas: 3 roles: - broker - controller storage: type: jbod volumes: - id: 0 type: persistent-claim size: 10Gi deleteClaim: false We defined 3 nodes, a storage of 10Gi for the whole pool and each node have 2 roles broker and controller.\nGood, let’s initialise the kraft cluster on those nodes, here is the yaml:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 apiVersion: kafka.strimzi.io/v1beta2 kind: Kafka metadata: name: kafka-cluster namespace: strimzi annotations: strimzi.io/node-pools: enabled strimzi.io/kraft: enabled spec: kafka: version: 3.7.1 # The replicas field is required by the Kafka CRD schema while the KafkaNodePools feature gate is in alpha phase. # But it will be ignored when Kafka Node Pools are used replicas: 3 listeners: - name: plain port: 9092 type: internal tls: false - name: tls port: 9093 type: internal tls: true config: offsets.topic.replication.factor: 3 transaction.state.log.replication.factor: 3 transaction.state.log.min.isr: 2 default.replication.factor: 3 min.insync.replicas: 2 # The storage field is required by the Kafka CRD schema while the KafkaNodePools feature gate is in alpha phase. # But it will be ignored when Kafka Node Pools are used storage: type: jbod volumes: - id: 0 type: persistent-claim size: 10Gi deleteClaim: false # The ZooKeeper section is required by the Kafka CRD schema while the UseKRaft feature gate is in alpha phase. # But it will be ignored when running in KRaft mode zookeeper: replicas: 3 storage: type: persistent-claim size: 10Gi deleteClaim: false entityOperator: userOperator: {} topicOperator: {} topicOperator and userOperator are for managing topics and user, Zookeeper definition is needed but it will be ignored later, and cluster is exposing two ports for TLS/Non-TLS Connection.\nAs we talked before the Minio Cluster will require a destination topic for sending bucket notifications(events), let’s create a one:\n1 2 3 4 5 6 7 8 9 10 apiVersion: kafka.strimzi.io/v1beta1 kind: KafkaTopic metadata: name: exam-generator-topic labels: strimzi.io/cluster: \u0026#34;kafka-cluster\u0026#34; spec: partitions: 3 replicas: 1 I found the 1 replicas with 3 partition is a good combination for latency and throughput(parallelism)\nMinio \u0026amp; Bucket Notification The Minio Cluster Deployment process is the same as Kafka, We install operator first:\n1 2 3 4 5 6 7 8 9 repositories: ... - name: minio-operator url: https://operator.min.io releases: ... - name: minio-operator chart: minio-operator/operator namespace: minio-operator Then we create a Minio Tenant (same as cluster), start by downloading the default values.yaml and changing few properties to match our needs:\nFirst, add a bucket named exams to get created when the tenant initialised: 1 2 3 buckets: - name: exams objectLock: false Change pools.server=1 because a standalone server is enough As the Minio will be accessed internally in Kubernetes, we disable ingress and if we need to access the console we port-forward the service. Secrets: we use simple access key and secret key in my case: minio and minio123 then add the chart and values to helmfile:\n1 2 3 4 5 6 7 - name: minio-tenant chart: minio-operator/tenant namespace: minio-tenant values: - ./minio-tenant-values.yaml needs: - minio-operator We check the pods, and here is the tenant deployed.\nTo test things out Let’s forward the console port and access it.\n1 kpf -n Minio-operator services/console 9090:9090 It asked for the JWT, which will be retrieved with:\n1 kubectl get secret/console-sa-secret -n minio-operator -o JSON| jq -r \u0026#39;.data.token\u0026#39; | base64 -d Cool, the tenant is working, and the bucket was created successfully\nBut we’re missing the notification side when uploading an object. To add Kafka endpoint as the notification destination, we will use the configuration command: mc\nHere is the configuration script:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # First Part mc config host add dc-minio http://minions-hl.minio-tenant:9000 minio minio123; mc admin config get dc-minio notify_kafka; mc admin config set dc-minio notify_kafka:primary \\\\ brokers=\u0026#34;kafka-cluster-kafka-bootstrap.strimzi.svc:9092\u0026#34; \\\\ topic=\u0026#34;exam-generator-topic\u0026#34; \\\\ tls_skip_verify=true \\\\ enabled=on; mc admin service restart dc-minio; echo \u0026#34;finish restart\u0026#34;; sleep 10s; # Second Part mc event add --event \u0026#34;put\u0026#34; --prefix exams dc-minio/exams arn:minio:sqs::primary:kafka ; echo \u0026#34;setup event bridging\u0026#34;; The first part is adding the Minio host, Kafka bootstrap server and destination topic.\nThe second part is adding the event hook for the “put” command targeted for exams folders inside exams buckets.\nWe wrap the above code into a Kubernetes job and run it.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 apiVersion: batch/v1 kind: Job metadata: name: configure-bucket-notification spec: template: spec: containers: - name: minio-client image: minio/mc command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;] args: - | mc config host add dc-minio http://minions-hl.minio-tenant:9000 minio minio123; mc admin config get dc-minio notify_kafka; mc admin config set dc-minio notify_kafka:primary \\ brokers=\u0026#34;kafka-cluster-kafka-bootstrap.strimzi.svc:9092\u0026#34; \\ topic=\u0026#34;exam-generator-topic\u0026#34; \\ tls_skip_verify=true \\ enabled=on; mc admin service restart dc-minio; echo \u0026#34;finish restart\u0026#34;; sleep 10s; mc event add --event \u0026#34;put\u0026#34; --prefix exams dc-minio/exams arn:minio:sqs::primary:kafka ; echo \u0026#34;setup event bridging\u0026#34;; restartPolicy: OnFailure Let’s test things out, we start a Kafka consumer for the exam-generator-topic topic, then we try uploading a sample file, we wait and an ObjectCreated event will appear.\nYour browser does not support the video tag. you may notice it’s S3 compatible, that will help us for not changing the code of Knative function to adapt Minio events.\nKnative \u0026amp; KafkaSource The Knative architecture holds multiple components, so we will give a general overview of the architecture without going into much details. Knative is divided into two major components: serving and eventing.\nThe serving part handles managing serverless workload inside the cluster, here is the request flow of HTTP requests to an application which is running on Knative Serving.\nThe eventing part is a collection of APIs that enable you to use an event-driven architecture with your applications. This part handles event listening on brokers and delivering it to SINKS (knative services)\nAs we finished explaining the core architecture, let’s move into installing Knative. The problem is that Knative doesn’t have a helm chart, so we will create one.\nFirst to install the Serving part, two yaml files are needed: serving-crds, serving-core\nLet’s create a helm chart with:\n1 helm create knative/core Delete all the files in templates folders except _helpers.tpl then download the two yaml files and place them inside templates folder.\n1 wget https://github.com/knative/serving/releases/download/knative-v1.15.2/serving-crds.yaml -o serving-crds.yaml 1 wget https://github.com/knative/serving/releases/download/knative-v1.15.2/serving-core.yaml -o serving-core.yaml Need another file to integrate knative with istio\n1 wget https://github.com/knative/net-istio/releases/download/knative-v1.15.1/net-istio.yaml -o net-istio.yaml Good, let’s add the chart to helmfile and install it.\n1 2 - name: knative-core chart: ./knative/core After installation, the above components will be created, and an ingress-gateway for serving Knative services(functions).\nEach Knative service must have a DNS to map to, am using Cloudflare to manage my domain. We retrieve the Istio ingress gateway IP address\n1 kubectl --namespace istio-system get service istio-ingressgateway and create an A record referencing “kn-function” subdomain\nThen we tell knative to serve services with the new domain\n1 2 3 4 kubectl patch configmap/config-domain \\ --namespace knative-serving \\ --type merge \\ --patch \u0026#39;{\u0026#34;data\u0026#34;:{\u0026#34;kn-functions.enkinineveh.space\u0026#34;:\u0026#34;\u0026#34;}}\u0026#39; Good, the serving component is ready, let\u0026rsquo;s move into the event part.\nThe event part is the same as the serving with few added steps, We first install the 2 yaml files: CRDs and Core, then we move into Knative source for Apache Kafka.\nThe KafkaSource reads messages stored in an existing Apache Kafka topics, and sends those messages as CloudEvents through HTTP to its configured sink(function)\nKafkaSource composed of controller and data plane.\n1 wget https://github.com/knative-extensions/eventing-Kafka-broker/releases/download/knative-v1.15.0/eventing-Kafka-controller.yaml -o eventing-kafka-broker.yaml 1 wget https://github.com/knative-extensions/eventing-kafka-broker/releases/download/knative-v1.15.1/eventing-kafka-source.yaml -o eventing-kafka-source.yaml For testing purpose now, we can deploy the below code to create an event display service for displaying received events and KafkaSource to link the topic with Knative service.\nservice.yaml\n1 2 3 4 5 6 7 8 9 10 apiVersion: serving.knative.dev/v1 kind: Service metadata: name: event-display namespace: exam spec: template: spec: containers: - image: gcr.io/knative-releases/knative.dev/eventing/cmd/event_display kafka-source.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: sources.knative.dev/v1beta1 kind: KafkaSource metadata: name: kafka-source namespace: exam spec: consumerGroup: kafka-group bootstrapServers: - kafka-cluster-kafka-bootstrap.strimzi.svc:909 topics: - exam-generator-topic consumers: 2 sink: ref: apiVersion: serving.knative.dev/v1 kind: Service name: event-display namespace: exam The source needs the Kafka server connection URL, topic and the service name to send events to.\nKnative Service Building But instead of an event display service, we need our exam generated service, luckily the AWS project has the code of the service.\nWe download the file, and we can see it’s using SNS and bedrock; two packages aren’t essential. Because SNS will be replaced by Kafka, and bedrock will be removed as we will use a static question for simplicity\n1 2 3 4 5 ... # sns = boto3.client(\u0026#34;sns\u0026#34;) producer = KafkaProducer(bootstrap_servers=os.environ[\u0026#34;KAFKA_BOOTSTRAP_SERVER\u0026#34;]) # bedrock = HelperBedrock() ... and because we removed bedrock, we will pass a bunch of static questions\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 ... # response = bedrock.get_response(file, template_instruction) # response = bedrock.get_response(response, template_formatted) # json_exam = file_helper.convert_to_json_in_memory(response) output = io.StringIO() # Write the JSON data to the in-memory stream questions = [ { \u0026#34;question\u0026#34;: \u0026#34;Which country of those countries located in balkans ?\u0026#34;, \u0026#34;options\u0026#34;: [\u0026#34;Romania\u0026#34;, \u0026#34;Germany\u0026#34;, \u0026#34;Croatia\u0026#34;, \u0026#34;Czech\u0026#34;], \u0026#34;correct_answer\u0026#34;: \u0026#34;Romania\u0026#34;, }, { \u0026#34;question\u0026#34;: \u0026#34;Where Tunisia is located?\u0026#34;, \u0026#34;options\u0026#34;: [\u0026#34;South America\u0026#34;, \u0026#34;Asia\u0026#34;, \u0026#34;Africa\u0026#34;, \u0026#34;Oceania\u0026#34;], \u0026#34;correct_answer\u0026#34;: \u0026#34;Africa\u0026#34;, }, ] json.dump(questions, output) # To ensure the content is in the buffer, seek the pointer back to the start of the stream output.seek(0) ... One requirement for the code to work is wrapping it inside an API, FastAPI will get the job done.\nDefine a single endpoint that receives the event, converts it into JSON and passes it to the main function.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 from typing import Any, Union import logging from main import main LOG = logging.getLogger(__name__) LOG.info(\u0026#34;API is starting up\u0026#34;) from fastapi import FastAPI,Request app = FastAPI() @app.get(\u0026#34;/health\u0026#34;) def health(): return {\u0026#34;Hello\u0026#34;: \u0026#34;World\u0026#34;} @app.post(\u0026#34;/\u0026#34;) async def intercept_event(request:Request): event = await request.json() return main(event,None) The service will need a container image to run, so we create a Dockerfile and requirements.txt for referencing package dependencies like FastAPI,Kafka..etc.\nDockerfile\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # FROM python:3.10 # WORKDIR /code # COPY ./requirements.txt /code/requirements.txt # RUN pip install --no-cache-dir -r /code/requirements.txt # COPY ./ /code/app # CMD [\u0026#34;fastapi\u0026#34;, \u0026#34;run\u0026#34;, \u0026#34;app/app.py\u0026#34;, \u0026#34;--port\u0026#34;, \u0026#34;80\u0026#34;] requiremtents.txt\n1 2 3 4 5 6 7 8 9 10 11 12 boto3 fastapi httptools kafka-python langchain langchain-community pdfminer.six pip-chill python-dotenv uvloop watchfiles websockets Now, we build the image, push into the registry and reference it inside the Knative service yaml.\n1 docker buildx build -t gitea.enkinineveh.space/gitea_admin/exam-gen-fn:v1 . --push We add the environment variables and secrets inside the value.yaml and create a helper to pass values as key,value into service.yaml and other values will be referenced directly.\nThe service will be accessed internally by the front, so there is no need for exposing it publicly, we can disable access outside of cluster by adding the following label:\n1 2 3 4 5 metadata: ... ... labels: networking.knative.dev/visibility: cluster-local and the last missing resource KafkaSource will invoke the knative service when a new event reach the topic, let\u0026rsquo;s create one\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion: sources.knative.dev/v1beta1 kind: KafkaSource metadata: name: kafka-source spec: consumerGroup: kafka-group bootstrapServers: - {{ .Values.env.normal.KAFKA_BOOTSTRAP_SERVER }} topics: - {{ .Values.env.normal.SOURCE_TOPIC_ARN }} consumers: 2 sink: ref: apiVersion: serving.knative.dev/v1 kind: Service name: {{ include \u0026#34;charts.fullname\u0026#34; . }} Cool, now redeploy the service\n1 helmfile apply Go to Console, upload a file, we notice a pod has been created from the knative service, wait for a few seconds and a new folder “question_bank” will be created with a JSON file holding the questions.\nGeneration Frontend Here comes the UI part, we copy the 3 files code from the repo and paste them over.\nWe will leave the code the same, so let’s build the image and push it to the registry\n1 docker buildx build –t gitea.enkinineveh.space/gitea_admin/exam_generator_front_end . --push We initialise helm chart and update the values to adjust service needs:\nReference the image inside the values.yaml\n1 2 3 4 5 image: repository: gitea.enkinineveh.space/gitea_admin/exam-take-frontend pullPolicy: IfNotPresent # Overrides the image tag whose default is the chart appVersion. tag: \u0026#34;v1\u0026#34; the service should listen on port 8501\n1 2 service: port: 8501 to access UI outside of kubernetes, we enable ingress, pass the host we want. One thing I noticed is that streamlit uses websocket for communication, so by adding this label, we tell nginx the websocket service it should use in case of websocket requests.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ingress: enabled: true className: \u0026#34;nginx\u0026#34; annotations: nginx.org/proxy-connect-timeout: \u0026#34;3600s\u0026#34; nginx.org/proxy-read-timeout: \u0026#34;3600s\u0026#34; nginx.org/client-max-body-size: \u0026#34;4m\u0026#34; nginx.org/proxy-buffering: \u0026#34;false\u0026#34; nginx.org/websocket-services: exam-taking-frontend-charts hosts: - host: exam-taking-frontend.enkinineveh.space paths: - path: / pathType: ImplementationSpecific backend: service: name: exam-taking-frontend-charts port: number: 8501 tls: - secretName: enkinineveh.space-tls-prod hosts: - exam-taking-frontend.enkinineveh.space And finally create the environment and secret variable, we don’t forget the helper\nvalues.yaml\n1 2 3 4 5 env: normal: API_GATEWAY_URL: \u0026#34;http://exam-taking-fn-exam-take-fn.default.svc.cluster.local\u0026#34; MONGO_URI: \u0026#34;mongodb://databaseAdmin:sHWKYbXRalmNExTMiYr@my-cluster-name-rs0.mongo.svc.cluster.local/admin?replicaSet=rs0\u0026amp;ssl=false\u0026#34; MONGO_TABLE_NAME: \u0026#34;score\u0026#34; _helper.tpl\n1 2 3 4 5 6 {{- define \u0026#34;helpers.list-env-variables\u0026#34;}} {{- range $key, $val := .Values.env.normal }} - name: {{ $key }} value: {{ $val }} {{- end}} {{- end }} Pass it in helmfile, apply the chart\n1 2 3 4 5 releases: ... - name: exam-generation-frontend chart: ./frontend/exam-generation-app/charts namespace: \u0026#34;exam\u0026#34; 1 helmfile apply we visit the host, and here is the Exam Generation UI\nWe Upload another test file and get the green mark for successful upload.\nSummary As we finished the generation part of the architecture, the next step will focus on adding \u0026ldquo;taking-exam\u0026rdquo; part.\n","permalink":"//localhost:1313/posts/transform-aws-3/","summary":"Brief description In this part we should talk about:\nKafka topics and difference of deploying between zookeeper and kraft. Create Minio cluster and k8s jobs for adding event notification Knative installation and the invocation with Kafka topics. Hosting of generate-exam frontend in k8s services with ingress, subdomain and reference the WebSocket service. Current Stack I have a K8s cluster composed of three nodes (1 master, 2 control plane) with Talos as the running OS, MetalLB deployed as a load balancer combined with Nginx (nginx.","title":"Transform AWS Exam Generator Architecture to Open Source Part #3: Exam Generation"},{"content":"Replacing services Phase In this article, we will pick this AWS architecture: “a serverless exam generator application for educator,” analyse it and find an open-source alternative solution for each service AWS provides, so if you are interested keep it up if you want to know more.\nTo give a context of how architecture works: it starts with the educator reaching AWS Cognito to create an account either using social media account or a simple email and password. A successful account registration will subscribe the user into SNS to receive notifications.\nThen he continues into accessing a front-end application created by python Streamlit hosted on ECS to upload his pdf lecture, when the lecture gets persisted, a lambda function will get invoked to process the persisted file and generate an exam with help of the AI, in this case bedrock.\nThe exam gets generated and persisted into another folder in the same bucket, lambda will then send a message to user SNS subscription saying, “the exam is ready.”\nIn other side the students will register for an account, visit the front-end for passing the exam, after they finish answering the questions, the user result will get persisted on a DynamoDB table, and with the help of DynamoDB ability of streaming data, a lambda function will get triggered to calculate the scoreboard of users and send a message to teacher SNS subscription.\nNow, after we get a general overview of the architecture let us move into the fun part “replacing the AWS services with open-source ones”.\nHere list of the used services and their alternatives:\nAWS Cognito, to add user signup and sign-in features, also it supports social media integration. A suitable alternative is two open-source projects developed by Ory: Kratos for handling sign-in and signup features with social media integration, while Oathkeeper for authenticating and authorizing incoming requests. ECS, Elastic container service it offers easy mechanism for deploying container to the cloud, a replacement for that is Kubernetes services because we are hosting our stack on k8s S3, for storing PDF files, a well-known alternative is Minio, an object storage system and guess what? It has S3 compatibility, so we don’t have to change the code that interact with S3 Lambda, a serverless function that will be spawned in case of an event or a direct communication with API Gateway, A known project for the serverless community is Knative, integrated with Istio and Kafka will get the job for us. API Gateway for the direct communication with lambda, there are many tools out there that can serve Knative traffic like Kong, ambassador but as we installed “Istio”, the Istio gateway will be enough to get the job. DynamoDB a fully managed, serverless, key-value NoSQL database, the main case of its existence is storing data and using CDC(change data capture) to invoke a lambda function for processing new insert data, as of this requirements any NoSQL database can get the job done, so we will use mongo managed with Percona operator. Bedrock, fully managed service that offers a choice of high-performing foundation models (FMs), for this one, am going to ignore it and use a static data. SNS fully managed Pub/Sub service for A2A and A2P messaging, its core functionality in this architecture is sending emails, so I replaced it with Knative function and Mailgun and then used a MongoDB table(subscribers) for storing educator’s email after signing up. here is the final version of the architecture\nAs we finish the task of replacing the services, we will move into the practical side of the implementation.\nThe next articles will be divided into 3 parts: generation of the exam, passing the exam, authentication \u0026amp; notification.\n","permalink":"//localhost:1313/posts/transform-aws-2/","summary":"Replacing services Phase In this article, we will pick this AWS architecture: “a serverless exam generator application for educator,” analyse it and find an open-source alternative solution for each service AWS provides, so if you are interested keep it up if you want to know more.\nTo give a context of how architecture works: it starts with the educator reaching AWS Cognito to create an account either using social media account or a simple email and password.","title":"Transform AWS Exam Generator Architecture to Open Source Part #2: Research and Planning"},{"content":"Introduction Have you thought of creating an AWS architecture but with open-source projects?\nIn these articles, we will challenge ourselves and transform this AWS architecture: a serverless exam generator application for educators.\nThe solution enables educators to instantly create curriculum-aligned assessments with minimal effort. Students can take personalised quizzes and get immediate feedback on their performance.\nWe will transform and replace each service varying from Cognito, Lambda, DynamoDb, fargate…etc with its open-source counterpart and host it, where? guessed right, on a Kubernetes cluster.\nIf you liked the idea, keep it up for next articles.\n","permalink":"//localhost:1313/posts/transform-aws-1/","summary":"Introduction Have you thought of creating an AWS architecture but with open-source projects?\nIn these articles, we will challenge ourselves and transform this AWS architecture: a serverless exam generator application for educators.\nThe solution enables educators to instantly create curriculum-aligned assessments with minimal effort. Students can take personalised quizzes and get immediate feedback on their performance.\nWe will transform and replace each service varying from Cognito, Lambda, DynamoDb, fargate…etc with its open-source counterpart and host it, where?","title":"Transform AWS Exam Generator Architecture to Open Source Part #1: Introduction"},{"content":"Introduction It\u0026rsquo;s been a nearly 3 months on my journey of learning kubernetes,\u0026hellip;.\nOne day I came across an architecture of AWS that includes AWS Cognito and ECS, if you have worked with AWS before, you would know that Cognito is a hosted authentication service which handles OAuth2/OIDC for you, To put it in a simpler way, it handles authentication and authorization to your AWS resources and provides different techniques to authenticate variying from Github, Google, etc.\nThen I asked myself wouldn\u0026rsquo;t there be an open source alternative to that? Something I can play with and maybe apply to my customers\u0026rsquo; on-premise architecture?\nWell, I bet it exists.\nOpen Source Solution It seems the open source community has a lot to offer in the identity management area. A quick Google search or ChatGPT prompt \u0026ldquo;open source identity \u0026amp; access management\u0026rdquo; will display quite a few interesting products. The list is:\nKeycloak Zitadel FusionAuth Ory products\nMy decision was to go with \u0026ldquo;Ory open source products\u0026rdquo;. I quite liked their approach of decoupling the functionalities into separate services (Hydra, Kratos, Oathkeeper) so even if you don\u0026rsquo;t like one of their products, you can replace it with another project. The documentation was quite clear but needed some improvements, and the community was kind of active on both Slack and GitHub.\nhere is a diagram of ory products\nExplain Different Terminologies I have built as a backend developer an authentication and authorization mechanism integrating google, github ..etc, but this time it was different because am delegating those features into services.\nI will be honest with you, learning the different terminologies (Identity management, authorization server, OIDC,Oauth2, policy as code, opaque vs jwt) wasn\u0026rsquo;t that smooth, it took me a lot of time searching through articles and asking chatGPT, until I got my final summary that goes like:\nOpenId: Is an authentication protocol, it allows an app to log into your social account, getting the desired information(email,username,image) without sharing your secret credentials(password). Oauth2: After proving user identity, user need somehow sort of permission of what kind of resource have access to, that part will be taken by Authorization Server(Oauth2). OpenId Connect (OIDC): A simple identity layer on top of the OAuth2 protocol, which allows clients to verify the identity of the end-user based on the authentication performed by an authorization server, as well as to obtain basic profile information about the end-user. Token(Opaque vs JWT): opaque are stored in database and when decoding they don\u0026rsquo;t contains any information, JWT in the otherside in it\u0026rsquo;s form is an user information encrypted with secret key, we use JWT for access tokens and opaque for refresh ones. JWK:A JSON data structure that represents a cryptographic key, used in cryptographic operations such as encryption and digital signatures. those keys used for validating JWT tokens Authorization server: This component will handle bothi OpenID and Oauth2 protocols Identity management: this entity will handle authentication and identifying user identity Policy Enforcement: This tool will work as proxy in front the app we trying to authenticate/authorize to, It intecept request, validate with authorizattion and then decide if user is allowed or denied. grant_types: there are 2 different methods authorization_code and client_credentials First-party app: An application that is developed by the same organization that owns the API or resource server. It is trusted by the resource owner. Third-party An application developed by an external organization that seeks permission to access resources or APIs owned by a different organization. Current Stack I have a K8s cluster composed of three nodes (1 master, 2 control plane) with Talos as the running OS, MetalLB deployed as a load balancer combined with Nginx (nginx.io) as an ingress controller.\nIngress hosts that we will see are mapped to my Cloudflare domain(enkinineveh.space) secured with TLS certs generated using cert-manager and letsencrypt. One side note is that am using reflector to share TLS secret into other namespaces.\nHelm is my favorite tool for deploying resources, Helmfile in case of deploying multiple charts, and k9s for navigating resources.\nGitea is my baby GitHub and CNPG is the default database.\nMini Project To better understand the usability of the Ory products, we will be deploying a mini project composed of:\nA private location service that returns user location. Publicly accessible backend to serve the frontend and connect to private services for fetching data. Simple frontend for displaying information. Some considerations on Authentication concerns:\nUser registration form must be composed of: Full Name, Email, Phone Number, Password. Email is unique. Social login using Google or GitHub will be a plus. One thing to note here is that we want to redirect users to the registration form in case one of the above fields is not present on the social platform. Some considerations on Authorization concerns:\nRequests to the location service need \u0026rsquo;location:read\u0026rsquo; scope to get authorized. We need two different clients: one for internal communication M2M, the other for communication between backend and frontend involving the user but without consent as the frontend is a first-party app. One beneficial step will be passing the authorized user_id as a custom header. We may provide third parties the ability to create OAuth2 apps. For the moment, we create one for them, but we want the same seamless experience as other platforms, including consent, scopes, and allowing those apps to access the location service directly as it may become public in the future. Kratos + Custom UI Introduction At first, when a user tries to access a protected resource, they need to have an account or sign up for a new one and then get a token to send to the protected resource for authorizing their request.\nDuring the phase of login/registration, an identity management system must exist to handle all that, so say welcome to Kratos.\nKratos is an identity management service that will handle all sorts of authentication methods, including login, registration, email verification, password reset, social login, etc.\nBut Kratos doesn\u0026rsquo;t come with a UI. Instead, it offers integration with existing UIs. You can create your pages with whatever language you like and then just reference the URLs in the Kratos config/selfservice/flows.\nI\u0026rsquo;m by no means a frontend developer, so I will use this open source UI project developed by Ory. It has the majority of the functionalities that I need and we will add a couple of changes later.\nDeploying kratos has 2 components:\nPublic for serving UI. Admin for managing identities (users) and modifying Kratos configuration (this component should be kept private for internal resources). To facilitate deployment, we will leverage a Helm chart. A quick Helm install will configure and deploy our resources. Ory has a quite good Helm chart. We will download the chart and make a couple of modifications.\nFirst, we will expose the public API, and the obvious approach is using Ingress, so we will change the following code under ingress:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ingress: public: enabled: true className: \u0026#34;nginx\u0026#34; hosts: - host: kratos.enkinineveh.space paths: - path: / pathType: Prefix backend: service: name: kratos-public port: number: 80 tls: - secretName: enkinineveh.space-tls-prod hosts: - kratos.enkinineveh.space Second, kratos needs a database to store identities so we will update \u0026ldquo;config\u0026rdquo;:\n1 2 config: dsn: postgresql://kratos:kratos@cluster-pg-rw.cnpg-system.svc.cluster.local:5432/kratos I created the user and db manually, take note the host is mapped to a local instance(my pg).\nall things configured, now let\u0026rsquo;s deploy the chart with modified values\n1 2 kubectl create namespace auth helm upgrade --install kratos -n auth -f kratos/kratos-values.yaml ory/kratos To experience full potential of kratos, we need to integrate the UI part. We clone the project, build it and push to a registry. either you deploy it to remote a registry like docker hub or a local one, am using gitea so that will more secure and easy for me.\n1 2 docker buildx build -t gitea.enkinineveh.space/gitea_admin/kratos-ui:v1 . docker push gitea.enkinineveh.space/gitea_admin/kratos-ui:v1 Now the docker image is ready, let\u0026rsquo;s initialize a local helm chart for the UI.\n1 helm create charts move into the values.yaml file and change the repository to the desired image in my case is gitea.enkinineveh.space/gitea_admin/kratos-ui, also don\u0026rsquo;t the forget the tag: v1\nyou may notice we didn\u0026rsquo;t add a domain or an ingress to access the UI, because for the UI to work it needs to be deployed under the same domain as kratos public, so our approach will be updating the previous chart of kratos and add another path for the kratos-ui.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 ingress: public: enabled: true className: \u0026#34;nginx\u0026#34; annotations: nginx.org/rewrites: \u0026#34;serviceName=kratos-public rewrite=/\u0026#34; hosts: - host: kratos.enkinineveh.space paths: - path: / pathType: Prefix backend: service: name: kratos-ui-service port: number: 80 - path: /app pathType: Prefix backend: service: name: kratos-public port: number: 80 tls: - secretName: enkinineveh.space-tls-prod hosts: - kratos.enkinineveh.space You can see I created another path \u0026ldquo;/app\u0026rdquo; for kratos-public, and leave the default \u0026ldquo;/\u0026rdquo; for kratos-ui.\nOne thing to notice is the annotation, I rewrite requests going to /app to / on kratos-public which means if I request /app/login on kratos.enkinineveh.space it will be forwared as /login in kratos-public.\nAnother thing to do is kratos need to reference the deployed UI, the known approach is updating kratos/config/flows by adding the different paths, in our case it will be like this:\n1 2 3 4 5 6 7 8 9 10 11 12 flows: error: ui_url: https://kratos.enkinineveh.space/error login: ui_url: https://kratos.enkinineveh.space/login verification: enabled: false ui_url: https://kratos.enkinineveh.space/verification registration: ui_url: https://kratos.enkinineveh.space/registration settings: ui_url: https://kratos.enkinineveh.space/settings Cool, everything is configured let\u0026rsquo;s install the chart but this time using helmfile as we have 2 charts to maintain\n1 2 3 4 5 6 7 8 9 10 11 12 13 helmDefaults: createNamespace: false releases: - name: kratos-ui chart: ./kratos-ui/charts namespace: auth - name: kratos chart: ory/kratos namespace: auth values: - kratos/kratos-values.yaml needs: - kratos-ui To apply the changes and test the UI run helmfile apply\nIdentity After the UI get deployed we will try create an identity, we can use the private api for admin service, but for this case we will leverage the UI part.\nHeads up into kratos.enkinineveh.space and click on the signup button, registration form will appear contains email and password and if notice the url appended flow query params(was added by kratos itself).\nWe will extend the form and add 2 other fields(full_name, phone_number) by updating the identity.schemas param, here is the updated version:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 { \u0026#34;$id\u0026#34;: \u0026#34;https://schemas.ory.sh/presets/kratos/identity.email.schema.json\u0026#34;, \u0026#34;$schema\u0026#34;: \u0026#34;http://json-schema.org/draft-07/schema#\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;User\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;traits\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;email\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;email\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;E-Mail\u0026#34;, \u0026#34;ory.sh/kratos\u0026#34;: { \u0026#34;credentials\u0026#34;: { \u0026#34;password\u0026#34;: { \u0026#34;identifier\u0026#34;: true } }, \u0026#34;recovery\u0026#34;: { \u0026#34;via\u0026#34;: \u0026#34;email\u0026#34; }, \u0026#34;verification\u0026#34;: { \u0026#34;via\u0026#34;: \u0026#34;email\u0026#34; } } }, \u0026#34;full_name\u0026#34;:{ \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;, \u0026#34;title\u0026#34;:\u0026#34;Full Name\u0026#34; }, \u0026#34;phone_number\u0026#34;:{ \u0026#34;type\u0026#34;:\u0026#34;string\u0026#34;, \u0026#34;title\u0026#34;:\u0026#34;Phone Number\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;tel\u0026#34; } }, \u0026#34;required\u0026#34;: [ \u0026#34;email\u0026#34;, \u0026#34;full_name\u0026#34;, \u0026#34;phone_number\u0026#34; ], \u0026#34;additionalProperties\u0026#34;: false } } } 1 2 3 4 5 identity: default_schema_id: default schemas: - id: default url: base64://above_file_in_base64 We run helmfile apply to update kratos release, and then if we visit again the registration page we will notice the form get updated.\nAdd OIDC(Github) Sometimes you get lazy and don\u0026rsquo;t want to type your email and use another random forgettable password, instead you choose to login with an your social accounts.\nAs an example let\u0026rsquo;s add github signin feature, we will need a github oauth app with read:user and user:email scopes but creating this is outside of this tutorial scope, you can follow github documentation and return back with github oauth2 app credentials.\nAfter you get the app, you added it under selfservice/methods/oidc/config/provider[0], here is an example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 selfservice: methods: oidc: config: providers: - id: github # this is `\u0026lt;provider-id\u0026gt;` in the Authorization callback URL. DO NOT CHANGE IT ONCE SET! provider: github client_id: client_id # Replace this with the Client ID client_secret: client_secret # Replace this with the Client secret issuer_url: https://api.github.com mapper_url: \u0026#34;base64://bG9jYWwgY2xhaW1zID0gc3RkLmV4dFZhcignY2xhaW1zJyk7Cgp7CiAgaWRlbnRpdHk6IHsKICAgIHRyYWl0czogewogICAgICBlbWFpbDogY2xhaW1zLmVtYWlsLAogICAgICB1c2VybmFtZTogY2xhaW1zLm5hbWUsCiAgICAgIHBob25lX251bWJlcjoxMjMKICAgIH0sCiAgfSwKfQ==\u0026#34; scope: - read:user - user:email requested_claims: id_token: email: essential: true full_name: essential: true phone_number: essential: true enabled: true the mapper_url is a jsonet file encoded base64, it extract the user information return by oidc and pass to kratos for creating identity\n1 2 3 4 5 6 7 8 9 10 local claims = std.extVar(\u0026#39;claims\u0026#39;); { identity: { traits: { email: claims.email, full_name: claims.name }, }, } you may notice the phone_nubmer isn\u0026rsquo;t passed from github, in that case kratos will return the user to registration to complete the missing field. and then we change kratos-ui to allow it to show the social-login button, here is the added code:\n1 2 3 4 5 6 7 8 export async function addIdentity(identityId: string) { return await sdk.identity .getIdentity({ id: identityId, includeCredential: [\u0026#34;oidc\u0026#34;] }) .then(({ data }) =\u0026gt; data) } export default { ...sdk, addIdentity } build the image and push it with a new tag: v2, then update the image value on kratos-ui chart and finally run helmfile apply to update the releases.\nVoila, you can see now Login/Register with Github.\nHydra Introduction After identifying the user, he may need to access some of our protected resources, that\u0026rsquo;s when he need to talk to an authorization server to give him token to access with it.\nHydra will act as an authorization server here. It generates, validates, and revokes tokens. Hydra and Kratos integrate well.\nLike Kratos, Hydra contains 2 services: public and admin. Another interesting component is hydra Maester. It is a Kubernetes controller that provides CRD for simplifying the creation of OAuth2 clients, but we will ignore it in this case as we will create clients manually with the Hydra CLI.\nDeploying We will copy the same methodalogy for deploying kratos,start by downloading hydra chart values and do a couple of modifications to enable ingress and integrate with kratos,..etc.\nfor ingress we will enable just the public service with following code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ingress: public: enabled: true className: \u0026#34;nginx\u0026#34; annotations: kubernetes.io/ingress.class: nginx hosts: - host: hydra.enkinineveh.space paths: - path: / pathType: ImplementationSpecific tls: - secretName: enkinineveh.space-tls-prod hosts: - hydra.enkinineveh.space also hydra will need a database for storing it\u0026rsquo;s own state, under hydra.config we will add dsn parameter and allow the the migration to create database and the related tables:\n1 2 3 4 5 6 hydra: config: dsn: postgres://hydra:hydra@cluster-pg-rw.cnpg-system:5432/hydra automigration: enabled: true Going forward to kratos integration part, there a property under config called \u0026lsquo;urls\u0026rsquo; where we will provide a set of links referencing the login, registration, logout, consent(for showing user scopes that he will accept on) and identity_provider is the url\n1 2 3 4 5 6 7 8 9 urls: self: issuer: https://hydra.enkinineveh.space/ login: https://kratos.enkinineveh.space/login registration: https://kratos.enkinineveh.space/registration consent: https://kratos.enkinineveh.space/consent logout: https://kratos.enkinineveh.space/logout identity_provider: url: https://kratos.enkinineveh.space/app self.issuer is important here as the created token will contains an iss field in claims\nby default hydra will use opaque tokens, we will change that to JWT self defined tokens with following code:\n1 2 3 4 strategies: access_token: jwt jwt: scope_claim: list finally, we need create a secret key for encrypting user credentials, tokens,etc\u0026hellip;\n1 2 3 secrets: system: - YU03TmVqdzdjNUc4WVhtVTVtQ0RJb1FodXdhc1JsVW8= Now we\u0026rsquo;are ready to deploy, we will update helmfile and run helmfile apply:\n1 2 3 4 5 - name: hydra chart: ory/hydra namespace: auth values: - hydra/hydra-values.yaml Quick look at k9s, you will notice 3 pods two are active and the last one is terminated as it was initiazed by k8s job to run migration for db. Creating Oauth2 Clients Returning back to the project requirements, for the first use case, it enforces us into creating two different OAuth2 clients with a clean separation of concerns. That means, one for internal communication and the other one between the frontend and backend. In fact, each client must have minimal scopes and a targeted audience. For the second use case, we must create another client to handle connecting directly to the location service.\nCreating clients with Hydra is a straightforward action. We can use the admin service or the Hydra CLI if you have it installed. I wanted to change the method a bit, so I went for installing the CLI and had some tinkering with it.\nbecause hydra-admin cannot be accessed outside of cluster, I will port-forward to localhost:4445\n1 kpf -n auth services/hydra-admin 4445:4445 1 2 3 hydra create oauth2-client --name frontend-backend-client --audience backend-service\\ --endpoint http://localhost:4445 --grant-type authorization_code,refresh_token \\ --response-type code --redirect-uri http://localhost:8000/oauth-redirect --scope offline_access,openid --skip-consent true --skip-logout-consent true \\ --token-endpoint-auth-method client_secret_post 1 2 hydra create oauth2-client --name internal-communication --audience internal-service --endpoint http://localhost:4445 --grant-type client_credentials \\ --token-endpoint-auth-method client_secret_basic --scope offline_access,openid,location:read 1 2 3 hydra create oauth2-client --name third-party --audience backend-service\\ --endpoint http://localhost:4445 --grant-type authorization_code,refresh_token \\ --response-type code --redirect-uri http://localhost:8000/oauth-redirect --scope offline_access,openid,location:read\\ --token-endpoint-auth-method client_secret_post The first client will settle between the frontend and backend. After the user registers, they don\u0026rsquo;t need to approve a certain consent because they are dealing with their own data through our trusted app. Then an authorization code will be sent to the redirect URL in the frontend and it will be sent back to the backend for access and refresh token exchange.\nThe second client will handle the authorization internally, meaning machine-to-machine communication. It contains the location scope to call the Location service.\nThe latter will be provided to third parties to get authorized for accessing user data.\nHere is a demo of the current setup.\nOathkeeper Introduction Token doesn\u0026rsquo;t have a value if there is no validation entity, so from the project requirements, we need to enforce access to the location service to only JWTs with the \u0026rsquo;location\u0026rsquo;scope. That being said, there are two goals we need to achieve:\nValidate tokens when the service called internaly. Validate tokens When location service become public and a third party needs an access to. Oathkeeper is a PEP (Policy Enforcement Point) composed of two services: proxy and API.\nThe proxy service can sit in front of the protected resource, intercepting requests and then deciding based on given rules if the request should be allowed or denied. In this project, we set the redirect ingress request to the Oathkeeper proxy and then Oathkeeper decides based on the rules if it should allow or deny.\nDeploying Let\u0026rsquo;s use the same strategy we did with the previous services (Hydra, Kratos). Start by downloading the values.yaml file and make a couple of modifications to adjust for our needs.\nWe will keep services private and instead pass to Nginx a private domain: oathkeeper-proxy.\nOathkeeper has different handlers for authentication, authorization, mutation, and errors. To enable a specific handler, you need to define it under oathkeeper.config.\nOur case requires adding authentication using JWT, authorization, logging errors as JSON, and regex for parsing rules. So here is the added code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 access_rules: matching_strategy: regexp repositories: - file:///etc/rules/access-rules.json authenticators: jwt: enabled: true config: jwks_urls: - https://hydra.enkinineveh.space/.well-known/jwks.json scope_strategy: hierarchic jwks_max_wait: 5s authorizers: allow: enabled: true errors: handlers: json: enabled: true config: {} finally update helmfile:\n1 2 3 4 5 6 7 - name: oathkeeper chart: ory/oathkeeper namespace: auth values: - oathkeeper/oathkeeper-values.yaml needs: - kratos Rules For beginning, we will write our first rule to restrict access to the backend from the frontend side, updating oathkeeper.accessRules:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 oathkeeper: accessRules: [ { \u0026#34;id\u0026#34;: \u0026#34;backend-rule\u0026#34;, \u0026#34;upstream\u0026#34;: { \u0026#34;url\u0026#34;: \u0026#34;http://backend-app-charts.auth\u0026#34; }, \u0026#34;match\u0026#34;: { \u0026#34;url\u0026#34;: \u0026#34;http://backend-test.enkinineveh.space/\u0026lt;.*\u0026gt;\u0026#34;, \u0026#34;methods\u0026#34;: [ \u0026#34;GET\u0026#34;, \u0026#34;POST\u0026#34;, \u0026#34;OPTIONS\u0026#34;, \u0026#34;PUT\u0026#34;, \u0026#34;PATCH\u0026#34; ] }, \u0026#34;authenticators\u0026#34;: [ { \u0026#34;handler\u0026#34;: \u0026#34;jwt\u0026#34; } ], \u0026#34;authorizer\u0026#34;: { \u0026#34;handler\u0026#34;: \u0026#34;allow\u0026#34; }, \u0026#34;errors\u0026#34;:[ { \u0026#34;handler\u0026#34;:\u0026#34;json\u0026#34; } ] } ] and then update backend ingress to reference the oathkeeper API , here is how:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 ingress: enabled: true className: \u0026#34;nginx\u0026#34; annotations: hosts: - host: backend-test.enkinineveh.space paths: - path: / pathType: Prefix backend: service: name: oathkeeper-proxy port: number: 4455 Here we told nginx to forward request to oathkeeper proxy. Now let\u0026rsquo;s test the whole authentication, here is a quick demo\nYour browser does not support the video tag. Mutation One handler I didn\u0026rsquo;t mention before is mutation, the project requires we pass user_id as a header to the targeted service(backend,location) that\u0026rsquo;s where mutation handler gonna work. Let\u0026rsquo;s update the oathkeeper config\n1 2 3 4 5 6 # Global configuration file oathkeeper.yml mutators: header: # Set enabled to true if the authenticator should be enabled and false to disable the authenticator. Defaults to false. enabled: true then we change the rules on oathkeeper.accessRules and oathkeeper.config to add mutation\n1 2 3 4 5 6 7 8 9 10 11 12 { \u0026#34;mutators\u0026#34;: [ { \u0026#34;handler\u0026#34;: \u0026#34;header\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;headers\u0026#34;: { \u0026#34;USER_ID\u0026#34;: \u0026#34;{{ print .Subject }}\u0026#34;, } } } ] } 1 2 3 4 5 6 7 8 oathkeeper: config: mutators: header: enabled: true config: headers: USER_ID: \u0026#34;{{ print .Subject }}\u0026#34; We can get token and request backend-test.enkinineveh.space/headers we will see USER_ID was passed\nIstio Authorization One last requirement the project needs is for the backend to get an access_token with the \u0026ldquo;location\u0026rdquo; scope and then request the location service.\nTo force JWT authentication in the internal services, we need a service mesh. The location deployment needs to be placed inside a service mesh and tracked by a sidecar. We\u0026rsquo;re going to choose Istio as a service mesh. The steps to integrate this feature are:\n1- Add Oathkeeper Rule to handle location service access\n2- Install the sidecar by labeling the deployment.\n3- Use Istio meshconfig to configure an external authorizer.\n4- Create an authorization policy to use the declared external authorizer and restrict deployment.\nhere is the access rule to force JWT with \u0026ldquo;location:read\u0026rdquo; scope to access location service\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 { \u0026#34;id\u0026#34;: \u0026#34;location-rule\u0026#34;, \u0026#34;match\u0026#34;: { \u0026#34;url\u0026#34;: \u0026#34;http://location-app-charts.auth/\u0026lt;.*\u0026gt;\u0026#34;, \u0026#34;methods\u0026#34;: [ \u0026#34;GET\u0026#34;, \u0026#34;POST\u0026#34;, \u0026#34;OPTIONS\u0026#34;, \u0026#34;PUT\u0026#34;, \u0026#34;PATCH\u0026#34;, \u0026#34;HEAD\u0026#34; ] }, \u0026#34;authenticators\u0026#34;: [ { \u0026#34;handler\u0026#34;: \u0026#34;jwt\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;scope_strategy\u0026#34;: \u0026#34;hierarchic\u0026#34;, \u0026#34;required_scope\u0026#34;: [\u0026#34;location:read\u0026#34;] } } ], \u0026#34;authorizer\u0026#34;: { \u0026#34;handler\u0026#34;: \u0026#34;allow\u0026#34; }, \u0026#34;mutators\u0026#34;: [{ \u0026#34;handler\u0026#34;: \u0026#34;header\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;headers\u0026#34;: { \u0026#34;USER_ID\u0026#34;: \u0026#34;{{ print .Subject }}\u0026#34; } } }], \u0026#34;errors\u0026#34;:[ { \u0026#34;handler\u0026#34;:\u0026#34;json\u0026#34; } ] } I will not show you how to install Istio because this is outside the article\u0026rsquo;s scope, but you can follow their documentation; it is easy and comprehensive.\nAfter you install Istio, to add the Istio sidecar alongside the \u0026rsquo;location deployment\u0026rsquo;, we edit the podLabels on the location chart:\n1 2 podLabels: sidecar.istio.io/inject: \u0026#34;true\u0026#34; Next, let\u0026rsquo;s declare the Istio external authorizer, which will call the /decisions endpoint on the Oathkeeper API to decide if requests are allowed or denied. For example, suppose the backend wants to call the Location service with the URL: http://location.auth/user_location. Then the Istio sidecar will intercept this request and forward it to the Oathkeeper API with the path \u0026ldquo;/decisions/user_location\u0026rdquo; and the host header \u0026ldquo;location.auth\u0026rdquo;. So let\u0026rsquo;s edit the Istio ConfigMap and add the Oathkeeper external provider:\n1 2 3 4 5 6 7 8 9 10 extensionProviders: - name: \u0026#34;ext-authz\u0026#34; envoyExtAuthzhHttp: service: \u0026#34;oathkeepr-api.auth.svc.cluster.local\u0026#34; port: 4456 timeout: 10s failOpen: false statusOnError: \u0026#34;500\u0026#34; pathPrefix: /decisions includeRequestHeadersInCheck: [\u0026#34;authoirzation\u0026#34;] Finally, create an AuthorizationPolicy to use the declared external authorizer and restrict deployment:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: location-oathkeeper namespace: auth spec: selector: matchLabels: app.kubernetes.io/instance: location-app action: CUSTOM provider: name: ext-authz rules: - {} With these steps, the Istio sidecar will intercept requests to the location service and validate them using Oathkeeper, ensuring that only requests with valid JWT tokens and the correct scopes are allowed.\nYour browser does not support the video tag. Third Party Client If we need to put location service public and let third parties access it, the steps to achieve this are:\ndelete Authorization Policy enable ingress on service change access rule match.url to ingress host and that\u0026rsquo;s all, you can create a client and handle it to a third party to access your service directly\nConclusion I appreciate your time for reading this.\nReferences github_project\n","permalink":"//localhost:1313/posts/guardians_of_hell/","summary":"Introduction It\u0026rsquo;s been a nearly 3 months on my journey of learning kubernetes,\u0026hellip;.\nOne day I came across an architecture of AWS that includes AWS Cognito and ECS, if you have worked with AWS before, you would know that Cognito is a hosted authentication service which handles OAuth2/OIDC for you, To put it in a simpler way, it handles authentication and authorization to your AWS resources and provides different techniques to authenticate variying from Github, Google, etc.","title":"Guardians of hell: hydra kratos oathkeeper"},{"content":"Introduction with 1-Mistake On that shiny day, I got a project that required deploying a mongodb cluster, After a few searches, I found percona Operator, moved into installation section and copied the helm install command.\nAfter installing the required charts, I noticed that the pods weren\u0026rsquo;t in \u0026ldquo;running\u0026rdquo; state, so as a civilized kubernetes developer I ran \u0026ldquo;kubectl describe pod_name -n namespace\u0026rdquo;, and it turned out the problem was mongodb cluster requires either 3 or 5 nodes\nThat\u0026rsquo;s easy right ? !\u0026lsquo;am using proxmox for my on-prem VMs and talos for my kubernetes OS, therefore I created a new VM with talos as The OS, disabled DHCP and added custom IP address corresponding to 192.168.1.116. Then, to add the VM a new \u0026ldquo;worker\u0026rdquo; node we use Talos magic wand:\n1 talosctl apply-config --insecure --nodes 192.168.1.116 --file worker.yaml But life always hides a few surprises for you , I mistakenly ran another command which added the new VM as a controller plane. The result was having two etcd members fighting for their survivals, but our old master \u0026ldquo;kubernetes\u0026rdquo; wasn\u0026rsquo;t happy with the result because both etcd instance went down with their kube-apiserver instances\n2-Mistake Because I am so smart, I thought the two controller nodes contradicted with etcd\u0026rsquo;s happy state, so I searched for a solution that led me into either removing the newly created node or adding a new one to balance the cluster number, And hell yeah, Iam removing the second idiot VM.\nThe node deleted from \u0026ldquo;proxmox\u0026rdquo;,then I thought the cluster will return to healthy state and I will kiss my \u0026ldquo;IT\u0026rdquo; girlfriend saying \u0026ldquo;we\u0026rsquo;re back to normal\u0026rdquo; making her think I was mad at her for no reason, hmm but life surprises you once again, my dear.\nThis time, etcd remained in an unhealthy state, claiming it couldn\u0026rsquo;t find the joined node 192.168.1.116\nNOTE: you can run \u0026rsquo;talosctl -n 192.168.1.110 dmesg\u0026rsquo; to view node logs .\nI thought, I saw a talosctl command that invokes a members list and I said to myself if the \u0026ldquo;list\u0026rdquo; subcommand exists, then the remove or delete one will exist also, Well it was there of course, but with a different name: \u0026ldquo;remove-member\u0026rdquo;. however, it didn\u0026rsquo;t work, etcd wasn\u0026rsquo;t responding to my requests even the command : \u0026ldquo;talosctl members list\u0026rdquo; wasn\u0026rsquo;t showing anything.\nSolution: Edit the Etcd Snapshotted DB After long hours of reading Github issues, walking on the beach and talking with friends about rap songs I realized there was no solution other than to reset the controller node along with the etcd data directory.\nWhile reading the documentation on Talos \u0026ldquo;Disator Recovery\u0026rdquo;, I was made aware of the snapshot idea but wasn\u0026rsquo;t thinking outside the box. Until I thought of editing the etcd database, talosctl didn\u0026rsquo;t have a built-in command for this kind of operation so I went for snapshotting the database and inspecting it to see what I can edit there to remove the call for the our beloved dead node.\nLet\u0026rsquo;s start with taking a snapshot, there are two commands referenced in the documentation but we will go with latter because etcd is unreachable\n1 talosctl -n 192.168.1.110 cp /var/lib/etcd/member/snap/db . I ran the \u0026lsquo;file\u0026rsquo; command to check file type which returned: data, hmm well this isn\u0026rsquo;t enough linux, thanks for your time, On my second search on google I found the bbolt file type and there is this tool bbolt for inspecting bbolt databases, Cool now we playing our cards right.\nAfter a few tries, I found a bucket called \u0026ldquo;members\u0026rdquo;\n1 2 3 4 5 6 7 8 9 10 11 bbolt buckets db alarm auth authRoles authUsers cluster key lease members \u0026lt;- this one members_removed meta hmm, I procceded into list members bucket keys and inspecting each value\n1 2 3 bbolt keys db members 3cf1b5e76f18a513 920c1b791dddb17e 1 2 bbolt get db members 3cf1b5e76f18a513 {\u0026#34;id\u0026#34;:4391491117268903187,\u0026#34;peerURLs\u0026#34;:[\u0026#34;https://192.168.1.110:2380\u0026#34;],\u0026#34;name\u0026#34;:\u0026#34;talos-zrr-lqe\u0026#34;} 1 2 bbolt get db members 920c1b791dddb17e {\u0026#34;id\u0026#34;:10523816636264067454,\u0026#34;peerURLs\u0026#34;:[\u0026#34;https://192.168.1.116:2380\u0026#34;],\u0026#34;isLearner\u0026#34;:true} here we go Kogoro Mouri, I found the culprit this member with key 920c1b791dddb17e must be deleted, so let\u0026rsquo;s call the POLICE(ChatGPT) to exceel him out. We asked ChatGPT for deleting the key 920c1b791dddb17e from members buckets\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 package main import ( \u0026#34;log\u0026#34; \u0026#34;go.etcd.io/bbolt\u0026#34; ) func main() { // Open the BoltDB database file db, err := bbolt.Open(\u0026#34;db\u0026#34;, 0600, nil) if err != nil { log.Fatal(err) } defer db.Close() // The key we want to delete key := []byte(\u0026#34;920c1b791dddb17e\u0026#34;) // Update the database to delete the key from the \u0026#34;members\u0026#34; bucket err = db.Update(func(tx *bbolt.Tx) error { // Get the bucket bucket := tx.Bucket([]byte(\u0026#34;members\u0026#34;)) if bucket == nil { return bbolt.ErrBucketNotFound } // Delete the key return bucket.Delete(key) }) if err != nil { log.Fatalf(\u0026#34;Could not delete key: %v\u0026#34;, err) } else { log.Println(\u0026#34;Key deleted successfully\u0026#34;) } } Now, we can reset the etcd node and then recover from the backup db\n1 talosctl -n 192.168.1.110 reset --graceful=false --reboot --system-labels-to-wipe=EPHEMERAL wait until the node become in preparing mode and run:\n1 talosctl -n \u0026lt;IP\u0026gt; bootstrap --recover-from=./db.snapshot --recover-skip-hash-check finally run, \u0026lsquo;kubectl get nodes -o wide\u0026rsquo; and you should see your nodes\n\u0026lsquo;kubectl get pods\u0026rsquo; to check your cluster previous state returned to normal\n","permalink":"//localhost:1313/posts/etcd_went_down/","summary":"Introduction with 1-Mistake On that shiny day, I got a project that required deploying a mongodb cluster, After a few searches, I found percona Operator, moved into installation section and copied the helm install command.\nAfter installing the required charts, I noticed that the pods weren\u0026rsquo;t in \u0026ldquo;running\u0026rdquo; state, so as a civilized kubernetes developer I ran \u0026ldquo;kubectl describe pod_name -n namespace\u0026rdquo;, and it turned out the problem was mongodb cluster requires either 3 or 5 nodes","title":"Oops...Etcd went down"},{"content":"Introduction Hello, lately I have been trying to deploy a custom Docker image into my local Kubernetes cluster. It turned out I needed to host my Docker image on a container registry, either Docker Hub, which is not suitable for my use case, or deploy and use a local registry. During my research, I found Gitea, which I liked as it allows me to deploy all my projects on it and also host the containers.\nPrerequisite * kubernetes cluster * external server(S3,NFS) for dynamic provisioning * metallb installed Create PVC for NFS Server With the help of Proxmox, I created a VM and configured it as an NFS server on 192.168.1.109. To use this server in our Kubernetes cluster, we need to create a StorageClass and then create a PVC that points to that class so pods can use it.\n1 2 3 4 5 helm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\ --create-namespace \\ --namespace nfs-provisioner \\ --set nfs.server=192.168.1.109 \\ --set nfs.path=/srv/public/nfs Then we create a PVC, linking it to the storageClassName:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: nfs-test labels: storage.k8s.io/name: nfs storage.k8s.io/part-of: kubernetes-complete-reference spec: accessModes: - ReadWriteMany storageClassName: nfs-client resources: requests: storage: 15Gi If we want the deployment to store its volume in NFS, we define volumes with the argument persistentVolumeClaim = nfs-test. Here is an example:\n1 2 3 4 5 6 7 8 9 10 apiVersion: v1 kind: Pod metadata: name: task-pv-pod spec: volumes: - name: task-pv-storage persistentVolumeClaim: claimName: nfs-test ... Deploy Gitea + PostgreSQL + Redis To facilitate deploying resources into Kubernetes, we use Helm. With one command and changing a few values, we can deploy our resources. Before deploying, when I was reading the Gitea chart, I noticed Gitea requires PostgreSQL and Redis to be deployed alongside it to save its configs and states.\nSo let\u0026rsquo;s create a file \u0026lsquo;values-gitea.yaml\u0026rsquo; and add the default chart values.\nThen change the following values:\n1 2 3 4 5 6 7 8 9 persistence: create: false claimName: nfs-test postgresql-ha: enabled: false postgresql: enabled: true I disabled the deployment of Postgres-HA because I didn\u0026rsquo;t need it for my use case. However, if you\u0026rsquo;re deploying for your organization where multiple users are pushing and pulling, you may keep it enabled.\nNow to deploy the Helm release with the modifications, run:\n1 helm install gitea gitea-charts/gitea --values values-gitea.yaml NOTE: You need a cluster with an internet connection to pull the Docker images.\nNOTE: If you don\u0026rsquo;t specify the namespace, it will choose the \u0026lsquo;default\u0026rsquo; namespace.\nNow, we wait until the images get pulled and deployed. You can watch the pods by running:\n1 kubectl get pods -w After all the pods are deployed, to access Gitea, we need to either open a port or create an ingress. Let\u0026rsquo;s try the port-forwarding mechanism for now to test the app:\n1 kubectl port-forward service/gitea-http 3000:3000 Now go to http://localhost:3000 and test, you can login as gitea_admin, password: r8sA8CPHD9!bt6d\nTo access Gitea from another pod, CoreDNS provides a default resolving mechanism in the form: service_name.namespace.svc.cluster.local. In our example, the DNS for Gitea is: gitea-http.default.\nDeploy Controller Nginx For testing purposes, port-forwarding may be a good solution, but if we want a more reliable solution and even attach a domain with HTTPS to the Gitea service, we need an ingress.\nTo start with ingress, an ingress controller is needed. We will choose the most popular one: nginx-ingress.\nfollowing this article, choosing helm install, I got the below command to run:\n1 helm install my-release oci://ghcr.io/nginxinc/charts/nginx-ingress --version 1.2.1 If everything works as expected, you should see an ingress-controller service with an IP address from your load-balancer (MetalLB) pool of IP addresses.\nGreat, to expose Gitea, we will change the previous chart file \u0026lsquo;values-gitea.yaml\u0026rsquo; values:\n1 2 3 4 5 6 7 8 ingress: enabled: true className: nginx hosts: - host: gitea.homelab.local paths: - path: / pathType: Prefix here we instructed to create an ingress rule:\n- className is needed if you have multiple ingress controllers installed - the host part tell ingress to accept any request with domain or host header : gitea.homelab.local and forward it to gitea instance To redeploy the release with the new configuration, we run:\n1 helm upgrade gitea gitea-charts/gitea --values values-gitea.yaml If we check the ingresses, we can find Gitea ingress has been created. To test it, we will query the IP address of the ingress, supplying a custom host header: gitea.homelab.local.\n1 curl --header \u0026#39;Host: gitea.homelab.local\u0026#39; ingress_ip_address Deploy bind9 You may notice that accessing Gitea from a browser isn\u0026rsquo;t possible because the local DNS server doesn\u0026rsquo;t have knowledge of the domain: homelab.local. The solution is either to modify the /etc/hosts file or create a CT in Proxmox and host a DNS server there.\nI went for the second option, hosting a DNS server because my homelab may require a variety of services in the future, and I want them to be mapped to a domain for all the connected devices in my network.\nFor the DNS server, Pi-hole may be the most popular option for ad-blocking and adding DNS records, but I experienced a few bugs with serving DNS, so I went with the second option: Bind9.\nI read this article\nI created a CT in proxmox and assigned a static ip 192.168.1.114, don\u0026rsquo;t use dhcp because it may change if CT restarted. So here are my configuration\nmy local ip address: 192.168.1.104, ingress ip address: 192.168.1.148\nfilename: /etc/bind/named.conf.options\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 acl \u0026#34;trusted\u0026#34; { 192.168.1.0/24; }; options { directory \u0026#34;/var/cache/bind\u0026#34;; // If there is a firewall between you and nameservers you want // to talk to, you may need to fix the firewall to allow multiple // ports to talk. See http://www.kb.cert.org/vuls/id/800113 // If your ISP provided one or more IP addresses for stable // nameservers, you probably want to use them as forwarders. // Uncomment the following block, and insert the addresses replacing // the all-0\u0026#39;s placeholder. recursion yes; allow-recursion { trusted; }; listen-on { 192.168.1.114;}; allow-transfer { none; }; // forwarders { // 0.0.0.0; // }; //======================================================================== // If BIND logs error messages about the root key being expired, // you will need to update your keys. See https://www.isc.org/bind-keys //======================================================================== dnssec-validation auto; listen-on-v6 { any; }; }; filename: /etc/bind/named.conf.local\n1 2 3 4 5 6 7 8 zone \u0026#34;homelab.local\u0026#34; { type master; file \u0026#34;/etc/bind/zones/db.homelab.local\u0026#34;; }; zone \u0026#34;168.192.in-addr.arpa\u0026#34; { type primary; file \u0026#34;/etc/bind/zones/db.192.168\u0026#34;; # 192.168.0.0/24 subnet }; filename: zones/db.homelab.local\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 ; ; BIND data file for local loopback interface ; $TTL 604800 @ IN SOA homelab.local. admin.homelab.local. ( 3 ; Serial 604800 ; Refresh 86400 ; Retry 2419200 ; Expire 604800 ) ; Negative Cache TTL ; ; name servers - NS records IN NS ns1.homelab.local. ; name servers - A records ns1.homelab.local. IN A 192.168.1.114 ; name servers - A records gitea.homelab.local. IN A 192.168.1.148 filename: /etc/bind/zones/db.168.192\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ; ; BIND reverse data file for local loopback interface ; $TTL 604800 @ IN SOA ns1.homelab.local. admin.homelab.local. ( 3 ; Serial 604800 ; Refresh 86400 ; Retry 2419200 ; Expire 604800 ) ; Negative Cache TTL ; name servers - NS records IN NS ns1.homelab.local. ; PTR Records 114.1 IN PTR ns1.homelab.local. ; 192.168.1.114 148.1 IN PTR gitea.homelab.local. ; 192.168.1.148 once the the bind9 configured and it\u0026rsquo;s working, we need to add the dns server ip address as an additional one, am using NetworkManager:\nAdd TLS Now, if we try to login into to the Gitea registry using the below command:\n1 docker login gitea.homelab.local It will return an error claiming the registry domain needs a TLS certificate. We can work around that by adding the registry domain to /etc/docker/daemon.json, but it would be more useful if we create a TLS certificate and append it to the domain.\nWe will start first by creating the cert. I chose mkcert because my first search led to it 😄.\n1 mkcert gitea.homelab.local It will generate two PEM files: a public key and a private key.\nWe will create a TLS secret and append the two created files from mkcert:\n1 2 3 kubectl create secret tls gitea-secret \\ --key gitea.homelab.local-key.pem \\ --cert gitea.homelab.local.pem Finally, we append the gitea-secret into the ingress by changing the gitea-values.yaml file:\n1 2 3 4 tls: - secretName: gitea-secret hosts: - gitea.homelab.local Now, we can visit gitea.homelab.local and login to gitea registry without issues.\nChange nginx config for pushing the image We deployed Gitea with one main purpose in mind: pushing containers to the registry. However, if we try building a local image and pushing it, you may face an error saying: \u0026ldquo;413 Request Entity Too Large\u0026rdquo;!\nThis is because by default Nginx imposes a limit of 1MB for uploading media files. To change that, we add an annotation for ingress to remove the limit:\n1 2 annotations: nginx.org/client-max-body-size: \u0026#34;0\u0026#34; then we update the release chart\n1 helm upgrade gitea gitea-charts/gitea --values values-gitea.yaml Now, we can push the image: gitlab.homelab.local/gitea_admin/app:latest\nif you have created another user instead of gitea_admin, you can replace it in the above command.\nAdd Bind9 Server in CoreDNS We have done everything from deploying to adding TLS cert, but if we tried to create a deployment with the deployed image as an example\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion: apps/v1 kind: Deployment metadata: name: app-deployment labels: app: app spec: selector: matchLabels: app: app template: metadata: labels: app: app spec: containers: - name: app image: gitea.homelab.local/gitea_admin/app:latest ports: - containerPort: 80 after applying the yaml, if you run kubectl describe deployment/app_name you may notice in the events section that it\u0026rsquo;s stating pulling the image has failed, that\u0026rsquo;s logical because kubernetes cluster doesn\u0026rsquo;t know about our custom domain: homelab.local.\nSo to let kubernetes DNS server: CoreDNS, acknowledge our domain we gonna need a litle tweak into the CoreDNS config\nwe run the following command to open the editor with configmap:\n1 kubectl edit configmap -n kube-system coredns and then we add the reference to homelab.local\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 apiVersion: v1 kind: ConfigMap metadata: name: coredns namespace: kube-system data: Corefile: | .:53 { errors health kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa } prometheus :9153 forward . 172.16.0.1 cache 30 loop reload loadbalance } homalb.local:53 { errors cache 30 forward . 192.168.1.114 } and for the CoreDNS to take effect, we will restart it with :\n1 kubectl rollout restart -n kube-system deployment/coredns Now, to test things out you can redeploy the previous deployed yaml or just run an alpine with nslookup\n1 kubectl run --image=alpine:3.5 -it alpine-shell-1 -- nslookup gitea.homelab.local it should return the ip address of the ingress.\n","permalink":"//localhost:1313/posts/deploy-gitea-k8s/","summary":"Introduction Hello, lately I have been trying to deploy a custom Docker image into my local Kubernetes cluster. It turned out I needed to host my Docker image on a container registry, either Docker Hub, which is not suitable for my use case, or deploy and use a local registry. During my research, I found Gitea, which I liked as it allows me to deploy all my projects on it and also host the containers.","title":"Deploying gitea into kubernetes with custom domain"},{"content":"HIDING\n","permalink":"//localhost:1313/posts/hide-your-secrets/","summary":"HIDING","title":"GCP -\u003e AWS Migration: Hide Your Secrets"},{"content":"(Knock on the door)\n(door opens)(some background noise of the company like people shatter or talk)\nYeah, Come In\nHey\nHey\nHey, my Junior friend, have a seat.\nThanks\nIt\u0026rsquo;s been 2 weeks since I assigned you the project, how it\u0026rsquo;s going?\nGood, I\u0026rsquo;m doing good, I just finished a few tasks and as you asked about my progress.\nSo, can you tell me how much progress you have made?\nI think am nearly 40%, I somehow finished creating the dev environment, so I can test with developers the performance and durability of the infrastructure.\nWhat about PostgreSQL, Redis, Configuration and Secrets Management? And I saw your email about using AWS AURORA, did you figure out the answer ?\nI deployed the database as an RDS, well I didn\u0026rsquo;t find a real benefit in using Aurora for the dev environment, and also it is somehow expensive, but maybe we can use it on the prod environment as it offers scalability. For Redis, AWS has a fully managed service called Elastic Cache with integrated monitoring service CloudWatch and the last one Configuration Management I read about the service (AWS Secret Manager) that I will use but didn\u0026rsquo;t integrate yet.\nThat seems fair, have you faced some challenges learning AWS, I heard from my fellow developers that there are a few steeping curves in understanding the documentation and the services.\nSomehow yes, as I don\u0026rsquo;t know that much about AWS and the time isn\u0026rsquo;t enough for learning and practising, I choose a tool to be abstract for me the underlying infrastructure for now, but I am willing to learn what happens in the background, so I can customize more.\nWhat\u0026rsquo;s the tool name?\nAWS Copilot (the CTO typing the name, we need to hear the keyboard typing) it\u0026rsquo;s an open source project \u0026hellip;.(the CTO interrupt saying)\nHmm, I see, but wouldn\u0026rsquo;t that add a new layer to learn, because most DevOps developers on the market will stick with IAC?\nYeah, I thought of that, I noticed that copilot works upon CloudFormation, which is an IAC tool for AWS and if you\u0026rsquo;re asking why not use Terraform with built-in modules, I can tell with this method we will narrow in the future our range of search to DevOps with terraform knowledge as with the current situation we can teach our backend developers a few commands and that\u0026rsquo;s enough to monitor the infrastructure.\nInteresting, I\u0026rsquo;m the type of technical guy, can you explain more about the solution?\nOk, can I open my laptop to show you\n(developer unpacking the laptop sound)\nYeah, Sure\nhave you thought of opening a startup (the CTO also) (CTO show some interest in the developer)\n(think few seconds) No, I don\u0026rsquo;t think I am ready, but maybe a small side hustle\nOkay, let me log into my AWS account (while typing)(the developer said) add the MFA code and here we go.\n(the sound of the CTO moving to the chair next to the developer)\nSo, here is the CloudFormation dashboard, those are the stacks that copilot deployed, you can also identify them by tags, generally they have tags copilotenvironment and copilot-application, at the end if somehow copilot didn\u0026rsquo;t meet our expectation or we couldn\u0026rsquo;t customize it further we can just modify, extend and boom. Now let me show the commands to set up and deploy the services\u0026hellip;..\nYou didn\u0026rsquo;t deploy the app Yet, right? (while interrupting)\nNot Yet\nOk (with notification sound on the phone), Continue\nTo set up infrastructure first, we need to run copilot app init then copilot env init after that copilot will ask us a few questions about the public and private subnets and the availability zone of the load balancer after we confirm that we see 3 Cloudformation stack has been created which define a few resources(VPC, Subnets, RouteTables,InternetGateway, and Cluster for Containers), finally I will be writing all the steps on notion.\nAnd the dependencies? (CTO asked)\nYeah, I did my search on deploy RDS and ElasticCache, Copilot doesn\u0026rsquo;t support either of them as a built-in command, but we can extend and create the services by ourselves and this is what I have done, the extended functionality support CloudFormation\u0026rsquo;s files\nGot It, but, hmm(take a few seconds to rephrase ), but I\u0026rsquo;m worrying if we don\u0026rsquo;t understand carefully and let copilot create the infrastructure, it may lead to creating unuseful services and you know we\u0026rsquo;re short on money.\nYeah, I understand your concern but am willing to learn more about the architecture copilot created for us by that time I can see if copilot fits our needs else I remove it and advance my skills to manage by myself the rest of the infrastructure directly with cloud formation\nThat seems fair enough, Ok, here is what you gonna do in the next few days. After you finish deploying that config management thing, deploy the application on ECS, and handle it to the developers to try and check the performance, and what about the CI/CD\nCopilot support it but didn\u0026rsquo;t take a deep look into the documentation, I can deploy the application from my terminal, so let\u0026rsquo;s tell the project manager whenever there is a pull request merged they notify me so I pull the code and push it for review\nNoted, I will tell him\n","permalink":"//localhost:1313/posts/confession/","summary":"(Knock on the door)\n(door opens)(some background noise of the company like people shatter or talk)\nYeah, Come In\nHey\nHey\nHey, my Junior friend, have a seat.\nThanks\nIt\u0026rsquo;s been 2 weeks since I assigned you the project, how it\u0026rsquo;s going?\nGood, I\u0026rsquo;m doing good, I just finished a few tasks and as you asked about my progress.\nSo, can you tell me how much progress you have made?","title":"GCP -\u003e AWS Migration: Confession"},{"content":" 3 days passes, and I\u0026rsquo;m struggling on the same bug, Am I looking at the wrong side of the window, I don\u0026rsquo;t know, I think the best way to understand is going back and examine every command line and line of code I wrote.\nSo at first, After I took the decision to use AWS copilot, I look at the task on JIRA, analyzed it carefully and saw the need to deploy application dependencies first, one of the dependencies is the database . We have been using PostgresSQL V14, so we just need the same version on the dev environment.\nI run through the documentation, to catch any command on how to deploy a database with underlying infrastructure(VPC, Subnet, Route Table, \u0026hellip;). The first command I saw\ncopilot init It fulfils the need of creating the underlying infrastructure, but it requires an application ready to deploy, but this is not the case now. A few minutes after and stumbled upon another command with the description \u0026ldquo;creates a new environment where your services will live.\u0026rdquo;\ncopilot env init When I run the above command, it asked me to run copilot app init first. And here is the output of environment creation\nFrom my understanding of the output and manifest.yml file, it seems after running copilot env deploy it will create two public and private subnets on separate regions. So I proceeded with the command, and here is the output\nI googled a few terms I didn\u0026rsquo;t understand like ECS, security groups, and DNS namespace to have basic knowledge of what happens in the background.\nThe following task was deploying the database and this is when things got trickier, now one of the features of copilot is a command to deploy storage services like database, file system\u0026hellip; etc. It supports two types of databases DynamoDB, Aurora\nAurora seems a great option as it\u0026rsquo;s fully compatible with PostgresSQL, so I tried to deploy a cluster using the following command\ncopilot storage init -n cluster -t Aurora --lifecycle environment --engine PostgreSQL At the same time, I opened Thunderbird and I messaged the CTO asking if it was ok deploying Aurora instead of an RDS. I went back to the command and I found\nCouldn\u0026rsquo;t find any workloads associated with app noteapp, try initializing one: copilot [svc/job] init .\n✘ select a workload from noteapp : no workloads found in noteapp\nWell, the problem is obvious, I cannot deploy a storage service unless I create a service first and by service I mean containerized application.\nI walked through the documentation again, and I found a magical feature that says \u0026ldquo;Modeling Additional Environment Resources with AWS CloudFormation\u0026rdquo;, this feature gave me the ability to deploy resources on an environment based.\nThat gave me goosebumps to understand CloudFormation, as it seems crucial in the next phases. The methodology was deploying a demo architecture to get comfortable with the services and the whole flow, I deployed one of the well-known architectures which is lambda function \u0026amp; DynamoDB and here is my recap\nCloudFormation file structure consists of 3 main blocks Parameters (Optional), Resources (Required), and Outputs (Optional).\nResources block encapsulate the services we need to deploy, Each service requires 2 properties Type, Properties and each service has different Properties, an example of that for Lambda Function there are Handler , Runtime , Code properties while on DynamoDB::Table there are different properties AttributeDefinitions , KeySchema , ProvisionedThroughput Now, AWS CLI has a built-in command for managing CloudFormation files, to create resources the first time the command is\naws cloudformation create-stack --stack-name resource_stack --template-body file://cloudformation.yml --capabilities CAPABILITY_NAMED_IAM and for update\naws cloudformation update-stack --stack-name resource_stack --template-body file://cloudformation.yml --capabilities CAPABILITY_NAMED_IAM An additional cool feature of AWS CloudFormation is the built-in managing dashboard where you can see your stacks and their status\nThen I started to think about integrating CloudFormation with Copilot until I looked through the window, and it was almost dark and my back was hurting, so I took the sign and went for a little bit of social life\n","permalink":"//localhost:1313/posts/stress-swallows-you/","summary":"3 days passes, and I\u0026rsquo;m struggling on the same bug, Am I looking at the wrong side of the window, I don\u0026rsquo;t know, I think the best way to understand is going back and examine every command line and line of code I wrote.\nSo at first, After I took the decision to use AWS copilot, I look at the task on JIRA, analyzed it carefully and saw the need to deploy application dependencies first, one of the dependencies is the database .","title":"GCP -\u003e AWS Migration: Stress Swallows You"},{"content":" You know that feeling ?, when you\u0026rsquo;re escaping a bad documentation, instead crawling around searching for a solution, and you find a snippet of code on Stack Overflow or Reddit, after you copy and paste it, it doesn\u0026rsquo;t work then your mind tells you \u0026ldquo;you need to change it a little bit\u0026rdquo;.\nSo you start changing the code to solve your problem, and guess what? A hell of a lot of new terminology and ideas enter your mind, and you start to get confused. Well, I Hamza Bou Issa am in that state of mind.\nThe last time I was left in \u0026ldquo;Deploying RDS with copilot using CloudFormation\u0026rdquo;, Yeah my approach to solving the problem is the same as before, typing a bunch of keywords and questions into Google, clicking on the first few links, if Stack Overflow then I detect responses with green mark and copy code, if it\u0026rsquo;s an article, I find snippets and copy the ones that have RDS or DB on them\nI took the time to understand CloudFormation file structure and a few resource types\nAt first, I found this snippet\n# Set AWS template version AWSTemplateFormatVersion: \u0026quot;2010-09-09\u0026quot; # Set Parameters Parameters: EngineVersion: Description: PostgreSQL version. Type: String Default: \u0026quot;14.1\u0026quot; SubnetIds: Description: Subnets Type: \u0026quot;List\u0026lt;AWS::EC2::Subnet::Id\u0026gt;\u0026quot; VpcId: Description: Insert your existing VPC id here Type: String Resources: DBSubnetGroup: Type: \u0026quot;AWS::RDS::DBSubnetGroup\u0026quot; Properties: DBSubnetGroupDescription: !Ref \u0026quot;AWS::StackName\u0026quot; SubnetIds: !Ref SubnetIds DatabaseSecurityGroup: Type: \u0026quot;AWS::EC2::SecurityGroup\u0026quot; Properties: GroupDescription: The Security Group for the database instance. VpcId: !Ref VpcId SecurityGroupIngress: - IpProtocol: tcp FromPort: 5432 ToPort: 5432 DBInstance: Type: \u0026quot;AWS::RDS::DBInstance\u0026quot; Properties: AllocatedStorage: \u0026quot;30\u0026quot; DBInstanceClass: db.t4g.medium DBName: \u0026quot;postgres\u0026quot; DBSubnetGroupName: !Ref DBSubnetGroup Engine: postgres EngineVersion: !Ref EngineVersion MasterUsername: username MasterUserPassword: password StorageType: gp2 MonitoringInterval: 0 VPCSecurityGroups: - !Ref DatabaseSecurityGroup The following code will create 3 resources: DbInstance , DatabaseSecurityGroup , DBSubnetGroup , From my understanding the connection between those resources is a Database need to be created on private Subnets(DBSubnetGroup) on the other side for the database to accept connection it needs a security group( DatabaseSecurityGroup ) which should be on the same VPC as the Subnets\nNow before I paste this code into environment/addons/rds.yml, I\u0026rsquo;m going to remove the parameters as we have an alternate method of passing the SubnetIds and VpcId.\nCloudFormation gives the ability to import resources from previously created stacks with Fn::ImportValue function. In this case, after I run copilot env deploy --name test . Copilot create 2 CloudFormation stacks\nThe first stack is the interesting one, after we open on mycompany-app-test stack, we click on the Outputs panel, and it should show us the created resources with export names that can be imported on our RDS stack.\nThe two interesting export names are mycompany-app-test-PrivateSubnets , mycompany-app-test-VpcId , let\u0026rsquo;s refactor our rds.yml file and add them\n# Set AWS template version AWSTemplateFormatVersion: \u0026quot;2010-09-09\u0026quot; # Set Parameters Parameters: App: Type: String Description: Your application's name. Env: Type: String Description: The environment name your service, job, or workflow is being deployed Name: Type: String Description: The name of the service, job, or workflow being deployed. Resources: DBSubnetGroup: Type: \u0026quot;AWS::RDS::DBSubnetGroup\u0026quot; Properties: DBSubnetGroupDescription: !Ref \u0026quot;AWS::StackName\u0026quot; SubnetIds: !Split [',', { 'Fn::ImportValue': !Sub '${App}-${Env}-PrivateSubnets'}] DatabaseSecurityGroup: Type: \u0026quot;AWS::EC2::SecurityGroup\u0026quot; Properties: GroupDescription: The Security Group for the database instance. VpcId: Fn::ImportValue: !Sub '${App}-${Env}-VpcId' SecurityGroupIngress: - IpProtocol: tcp FromPort: 5432 ToPort: 5432 CidrIp: 0.0.0.0/0 DBInstance: Type: \u0026quot;AWS::RDS::DBInstance\u0026quot; Properties: AllocatedStorage: \u0026quot;30\u0026quot; DBInstanceClass: db.t4g.medium DBName: \u0026quot;postgres\u0026quot; DBSubnetGroupName: !Ref DBSubnetGroup Engine: postgres EngineVersion: \u0026quot;14.1\u0026quot; MasterUsername: username MasterUserPassword: password StorageType: gp2 MonitoringInterval: 0 VPCSecurityGroups: - !Ref DatabaseSecurityGroup As you see, I removed the previous parameters and replace them with a few parameters App , Env , Name which is copilot required add-ons parameters. Also for SubnetIds I import PrivateSubnets and split it because it must be passed as separate values\nAfter I run copilot env deploy --name test a nested stack will get created\nBut how I\u0026rsquo;m going to test if the database is working or accepting connection while it\u0026rsquo;s not reachable on the public internet, well it seems I can create an ec2 instance on the public subnet and allow connection with the security group.\nHere is the refactored code\n# Set AWS template version AWSTemplateFormatVersion: \u0026quot;2010-09-09\u0026quot; # Set Parameters Parameters: App: Type: String Description: Your application's name. Env: Type: String Description: The environment name your service, job, or workflow is being deployed to. Resources: DBSubnetGroup: Type: \u0026quot;AWS::RDS::DBSubnetGroup\u0026quot; Properties: DBSubnetGroupDescription: !Ref \u0026quot;AWS::StackName\u0026quot; SubnetIds: !Split [',', { 'Fn::ImportValue': !Sub '${App}-${Env}-PrivateSubnets' }] DatabaseSecurityGroup: Type: \u0026quot;AWS::EC2::SecurityGroup\u0026quot; Properties: GroupDescription: The Security Group for the database instance. VpcId: Fn::ImportValue: !Sub '${App}-${Env}-VpcId' SecurityGroupIngress: - IpProtocol: tcp FromPort: 5432 ToPort: 5432 CidrIp: 0.0.0.0/0 DBInstance: Type: \u0026quot;AWS::RDS::DBInstance\u0026quot; Properties: AllocatedStorage: \u0026quot;30\u0026quot; DBInstanceClass: db.t4g.medium DBName: \u0026quot;postgres\u0026quot; DBSubnetGroupName: !Ref DBSubnetGroup Engine: postgres EngineVersion: \u0026quot;14.1\u0026quot; MasterUsername: username MasterUserPassword: password StorageType: gp2 MonitoringInterval: 0 VPCSecurityGroups: - !Ref DatabaseSecurityGroup Tags: - Key: Name Value: !Sub 'copilot-${App}-${Env}' NewKeyPair: Type: 'AWS::EC2::KeyPair' Properties: KeyName: !Sub ${App}-${Env}-EC2-RDS-KEYPAIR Tags: - Key: Name Value: !Sub 'copilot-${App}-${Env}' EC2SecuityGroup: Type: \u0026quot;AWS::EC2::SecurityGroup\u0026quot; Properties: GroupDescription: The Security Group for the ec2 instance. VpcId: Fn::ImportValue: !Sub '${App}-${Env}-VpcId' SecurityGroupIngress: - IpProtocol: tcp FromPort: 22 ToPort: 22 CidrIp: 0.0.0.0/0 Tags: - Key: Name Value: !Sub 'copilot-${App}-${Env}' Ec2Instance: Type: 'AWS::EC2::Instance' Properties: ImageId: ami-05e8e219ac7e82eba InstanceType: t2.micro KeyName: !Ref NewKeyPair SubnetId: !Select [\u0026quot;0\u0026quot;,!Split [',', { 'Fn::ImportValue': !Sub '${App}-${Env}-PublicSubnets' }]] SecurityGroupIds: - !Ref EC2SecuityGroup UserData: | #!/bin/bash sudo apt update sudo apt upgrade sudo apt install postgresql postgresql-contrib Tags: - Key: Name Value: !Sub 'copilot-${App}-${Env}' Outputs: ServerPublicDNS: Description: \u0026quot;Public DNS of EC2 instance\u0026quot; Value: !GetAtt Ec2Instance.PublicDnsName DatabaseEndpoint: Description: \u0026quot;Connection endpoint for the database\u0026quot; Value: !GetAtt DBInstance.Endpoint.Address A few things to notice, I added an Ec2Instance on a public subnet, a security group that allows only ssh port (22), meanwhile, the ImageId, InstanceType property are more tied to the region you\u0026rsquo;re deploying to, I am using eu-west-3(I\u0026rsquo;m not a French guy -_-). NewKeyPair is the ssh key to log into EC2, and finally the Outputs section for getting the EC2 instance and Database connection URL\nI rerun copilot env deploy --name test , wait for the stack to update, before connecting to ec2 an ssh file must be downloaded, the ssh key pair will be saved on AWS Parameter Store, here are the following steps to download it\naws ec2 describe-key-pairs --filters Name=key-name,Values=mycompany-app-test-E The above command output.\nkey-05abb699beEXAMPLE and to save the ssh key value\naws ssm get-parameter --name /ec2/keypair/key-05abb699beEXAMPLE --with-decrypt Now, After I get the ssh file, try to connect to the server\nssh -i new-key-pair.pem ubuntu@ec2-host I try to connect to RDS\npsql -U username -d postgres -h database_host ","permalink":"//localhost:1313/posts/determination/","summary":"You know that feeling ?, when you\u0026rsquo;re escaping a bad documentation, instead crawling around searching for a solution, and you find a snippet of code on Stack Overflow or Reddit, after you copy and paste it, it doesn\u0026rsquo;t work then your mind tells you \u0026ldquo;you need to change it a little bit\u0026rdquo;.\nSo you start changing the code to solve your problem, and guess what? A hell of a lot of new terminology and ideas enter your mind, and you start to get confused.","title":"GCP -\u003e AWS Migration: Determination"},{"content":"Key points wake up with an email deciding not to go to the workspace read the task description and start searching lay on the bed again, thinking of ways to approach the problem get up and write his thoughts in his journal Search again and find copilot as a solution his alarm bumps for taking a walk It\u0026rsquo;s 8:35 AM, and the phone started vibrating with notifications, he used to receive the \u0026ldquo;Good Morning Babe\u0026rdquo; message from his girlfriend with a few newsletters emails, but this time, it was an email on his Zoho mail with the Subject: [JIRA] Yassine assigned DEV-550 to you\nHe opened his JIRA account from his phone, he was assigned a single task with the title\nHe didn\u0026rsquo;t take a deep look at the description, but he felt he wasn\u0026rsquo;t in the mood to go to the workspace, so he sent a message to the company Slack channel stating he is going to work from home today.\nTook a quick bath, eat breakfast and returned to the desk, he opened again the JIRA task and now taking a better look at the description by catching some keywords: DEV Environment, VPC, Subnet, Route Table, LoadBalancer, Database, Redis, Containers, IAC\nHe wasn\u0026rsquo;t familiar with the majority of keywords, and this was an advantage to stimulate him for learning. 50 min passes and he should take a break for 20 min as he follows The 50-Minute Focus Technique, but it doesn\u0026rsquo;t seem he cares, his eyes are filled with passion for finding the solution, he knew there is no direct tutorial or article to his problem.\nHis browser was filled with tabs of random articles with a side note app to summarise what he understood from each article\nHe lay down on the bed, taking a break and connecting the dots. The task seemed more clear now, but time was a crucial factor, he was telling himself\nI pretty understand the workflow I should take but hmmm,but 3 weeks doesn\u0026rsquo;t seem a lot of time, so let me find a tool to help me speed up the process and doesn\u0026rsquo;t hook me up into much networking details for now\nLet me take a nap for now\nIt was 3:51 PM, and this time he woke up prepared, he took his pc while lying on his bed. The goal is to find a solution to help him speed up building the infrastructure, A quick search leads him to a few tools like Terraform, Ansible, CloudFormation, Puppet, Vagrant\nBut it didn\u0026rsquo;t seem the final result, Terraform \u0026amp; CloudFormation were IAC( Infrastructure as Code) tools an abstraction over cloud provider Rest API, Ansible and Puppet were Configuration Management tools he doesn\u0026rsquo;t need for now and a vagrant is a virtualization tool which was away from his needs.\nIn the same way, he was eager to find a tool to work on top of CloudFormation, he was typing all sorts of keywords around CloudFormation and IAC tools like \u0026ldquo;Tools that use CloudFormation, AWS open source projects for building networking infrastructure, What better than CloudFormation, CloudFormation Cons, CloudFormation Alternatives\u0026rdquo;\nAs a consequence, the results were somehow near his needs, he found a few tools to compare them and decide which one to go with\nAWS CDK HashiCorp WAYPOINT AWS Copilot AWS Amplify Fissaa How extendible the tool is, How much is supported by the community if it\u0026rsquo;s open source(GitHub Starts, pull request, issues), learning curve, have additional and unique features. Those were the criteria he judged upon choosing his next tool, for better clarity he created a table where he assigned each one a score\nAfter calculating the scores, AWS Copilot won the battle by how much is can be extendible into adding CloudFormation files, the community is quite good with 2.7k stars and few issues and on the pull requests side people seems interested in developing features on it, the learning curve scored 6 because documentation was pretty clear, and they have tutorial section and AWS is investing some videos also, it scored the highest on unique features because it helps deploy storage services(S3, Aurora, DynamoDB), customizing the underlying infrastructure like Subnet, NAT, Load Balancer\u0026hellip;etc.\nIt\u0026rsquo;s 7:19 PM, and the alarm rings saying \u0026ldquo;30-min Walk\u0026rdquo;\u0026hellip;\n","permalink":"//localhost:1313/posts/the-search/","summary":"Key points wake up with an email deciding not to go to the workspace read the task description and start searching lay on the bed again, thinking of ways to approach the problem get up and write his thoughts in his journal Search again and find copilot as a solution his alarm bumps for taking a walk It\u0026rsquo;s 8:35 AM, and the phone started vibrating with notifications, he used to receive the \u0026ldquo;Good Morning Babe\u0026rdquo; message from his girlfriend with a few newsletters emails, but this time, it was an email on his Zoho mail with the Subject: [JIRA] Yassine assigned DEV-550 to you","title":"GCP -\u003e AWS Migration: The Search"},{"content":"Key Points: the boy is still an amateur, this is his first experience of him with DevOps the boy lacks some social skills the Startup CEO calls him he got the task of migrating the stack from GCP to AWS he returns confused and scared of not finding a solution then he goes back home, locks the room door and Lay on the bed imagining the way He was standing in front of the Nespresso machine, waiting for his daily caffeine fix, but his ears were inadvertently eavesdropping on the conversation of his two colleagues.\nThe page will take too long if you make that many requests to the backend\nShouldn\u0026rsquo;t we talk to the backend team to create one API to post all JSON data?\u0026quot; one colleague suggested.\nHe didn\u0026rsquo;t keep listening after he smelled the aroma of his coffee. He took his coffee and went back to his desk. As a Backend Developer, he scrolled up and down, examining the project\u0026rsquo;s code and searching for any opportunities for optimization or refactoring.\nAn hour passed, and he grew bored. He played the Spotify playlist \u0026ldquo;Lofi Electronic mix\u0026rdquo; and then slumped in his chair, wondering what life would be like after one year of professional experience. It was the smile on his face that reflected that he was moving closer to his dreams. That moment was the missing key to escaping boredom.\nWhile wandering in his imagination, he felt a hand on his shoulder. He opened his eyes and saw the CTO in front of him. He was embarrassed and suddenly fell off the chair. The CTO laughed and asked if he was okay. He replied\nYeah, all good, thanks. I was thinking about some optimization, and I was closing my eyes to avoid getting distracted\nYep, sure. Sorry for disturbing you. Can I have a moment of your time to discuss something?\u0026quot; the CTO asked.\nHe followed him to the office, sat down, and waited for the CTO to finish sending the email. He couldn\u0026rsquo;t stop asking himself if he was going to be fired or not. His heart jumped out of fear. Then the CTO said,\nI received an email from GCP (Google Cloud Platform) that they will not give us more credit after the next two months. So we thought of migrating to AWS since it gives us one year of free usage on some services. We can benefit from the wide range of services that it provides. I called you because I know you have some sort of Networking knowledge and skills as you worked with some projects on university, and I wanted to ask if you\u0026rsquo;re interested\nTo be honest with you, I dont have that much experience with AWS, but I just did basic configuration of EC2 instances, that\u0026rsquo;s all.\nHmm, Understood, So are you willing to face more challenges and develop yourself on the cloud project ?\nOkey, I think it will be a good experience to prove myself and my skills, I\u0026rsquo;m In\nOkay, sounds good. I will tell the backend senior that you moved to the DevOps team and send you the required tasks and GCP credentials\nCan I ask who is with me on the team ?\nNo one, just you\nHe left the office, feeling a mix of curiosity and fear. He packed his laptop, put on his headphones, and walked back home.\nAs he walked through the door of his apartment, he took off his shoes and fell onto the couch. His mind was racing with thoughts about the upcoming project and the opportunity that lay ahead of him. He was excited about the prospect of working with AWS and learning more about DevOps.\nHe spent the rest of the evening researching AWS services and watching tutorials on how to use them. He was determined to be well-prepared for the task ahead. As the night wore on, he grew tired and went to bed, dreaming about the possibilities that awaited him\n","permalink":"//localhost:1313/posts/the-beginning/","summary":"Key Points: the boy is still an amateur, this is his first experience of him with DevOps the boy lacks some social skills the Startup CEO calls him he got the task of migrating the stack from GCP to AWS he returns confused and scared of not finding a solution then he goes back home, locks the room door and Lay on the bed imagining the way He was standing in front of the Nespresso machine, waiting for his daily caffeine fix, but his ears were inadvertently eavesdropping on the conversation of his two colleagues.","title":"GCP -\u003e AWS Migration: The Beginning"},{"content":"Introduction Deploy Previews allow you and your team to experience changes to any part of your site without having to publish them to production.\nWith a deploy previews feature you and your teammates can see the changes of every pull request you make without merging it, this will reduce the burden of rolling back the environment when bugs happen as you can review the changes before.\nIn this tutorial, you’ll learn about creating a CI pipeline with CodeBuild that gets triggered on every pull request creation or update, for every build we host react build folder on an S3 bucket and serve it with Cloudfront, finally after merging the pull request, we delete the build folder from S3.\nIn the end, this tutorial will expand your knowledge of AWS Services and help you speed up your development and deployment process.\nBefore starting, here is a playlist to enjoy reading with\nTo accomplish this tutorial, you’ll need:\nan AWS account with administrative permissions an AWS CLI with a minimum version of 2.4.6 a runnable React app hosted on GitHub basic knowledge of YAML file structure clone this simple react app Creating CI Pipeline To build a resilient system, you need to add tests on your code then run them locally and on every push to version control. In this step, you will build a CI pipeline that triggers every pull request creation or update on your GitHub repository, the pipeline will run the desired tests and return a badge containing the status of the tests to your GitHub.\nCreate the file pull-request.yml in your text editor:\nnano pull-request.yml Add the following CloudFormation code to the file, which builds a CodeBuildProject that defines a CI pipeline with GitHub integration:\nResources: CodeBuildProject: Type: AWS::CodeBuild::Project Properties: Name: FrontBuildOnPull ServiceRole: !GetAtt BuildProjectRole.Arn LogsConfig: CloudWatchLogs: GroupName: !Ref BuildLogGroup Status: ENABLED StreamName: front_pull_request EncryptionKey: \u0026quot;alias/aws/s3\u0026quot; Artifacts: Type: S3 Location: !Ref ArtifactBucket Name: FrontPullRequest OverrideArtifactName: true EncryptionDisabled: true Environment: Type: LINUX_CONTAINER ComputeType: BUILD_GENERAL1_MEDIUM Image: aws/codebuild/amazonlinux2-x86_64-standard:4.0 Source: Type: GITHUB Location: \u0026quot;https://github.com/projectX/repoY\u0026quot; ReportBuildStatus: true Triggers: BuildType: BUILD Webhook: true FilterGroups: - - Type: EVENT Pattern: PULL_REQUEST_CREATED,PULL_REQUEST_UPDATED Visibility: PUBLIC_READ ResourceAccessRole: !Ref PublicReadRole I will briefly explain the CloudFormation template structure, so you can get a general understanding of CloudFormation files that guides you through the next steps.\nThe majority of CloudFormation template files will contain Parameters(Optional) and Resources(Required), Outputs(Optional), each block on the Resources is a Service, and every Service contains Type and Properties. The type is referred to as AWS Service or a Custom Function and each Type has a different set of properties.\nLet’s analyze the above Code to understand better. Here we have a Resources block that contains three Services. The CodeBuildProject refers to AWS::CodeBuild::Project which is an AWS Service for CodeBuild. The CodeBuild has a set of properties, you can find them here properties, I will explain each property functionality and why we’re adding it.\nGive CodeBuild the required permissions For CodeBuild to work we need to assign the ServiceRole an IAM Role to give CodeBuild the right to access other AWS services like secrets, S3, and logs.\n... ServiceRole: !GetAtt BuildProjectRole.Arn ... ... BuildProjectRole is a role that assumes the principal “codebuild.amazonaws.com” the permission to use a set of services on our behalf of us, more reading about AWS::IAM::ROLE\nAfter, we need to ask ourselves what services CodeBuild must have access to and what we want to access on each of the services:\nCodebuild needs to create and update the test reports. CodeBuild needs to store the CI process logs inside a logs group. CodeBuild needs to access the secrets manager to get some secret credentials ex: CYPRESS_KEY. - CodeBuild needs to get and store React build folder on S3. Now after we defined our requirements, we create BuildProjectPolicy which is an AWS::IAM::Policy for BuildProjectRole that contains the list of statements, and each statement is composed of a set of actions.\nBuildProjectRole: Type: AWS::IAM::Role Properties: AssumeRolePolicyDocument: Version: '2012-10-17' Statement: - Effect: Allow Principal: Service: - codebuild.amazonaws.com Action: - sts:AssumeRole BuildProjectPolicy: Type: AWS::IAM::Policy DependsOn: BuildProjectRole Properties: PolicyName: !Sub ${AWS::StackName}-CodeBuildPolicy PolicyDocument: Version: '2012-10-17' Statement: - Effect: Allow Action: - codebuild:CreateReportGroup - codebuild:CreateReport - codebuild:UpdateReport - codebuild:BatchPutTestCases - codebuild:BatchPutCodeCoverages # Create and update test report Resource: \u0026quot;*\u0026quot; - Effect: Allow Action: - s3:PutObject - s3:GetObject - s3:GetObjectVersion # Get and store React build folder on S3 Resource: \u0026quot;*\u0026quot; - Effect: Allow Action: - logs:CreateLogGroup - logs:CreateLogStream - logs:PutLogEvents # Store the CI process logs inside a logs group Resource: arn:aws:logs:*:*:* - Effect: Allow Action: - secretsmanager:* # Get secret creedentials ex: CYPRESS_KEY Resource: \u0026quot;*\u0026quot; Roles: - !Ref BuildProjectRole Access and Store CodeBuild Logs Logs are an important key of DevOps because it adds the visibility aspect to the infrastructure, so gather logs as much as you can. In our case, to enable logs for CodeBuild project we create LogsConfig property.\n... ... LogsConfig: CloudWatchLogs: GroupName: !Ref BuildLogGroup Status: ENABLED StreamName: front_pull_request ... ... Now, we need to create a LogGroup to which CodeBuild pushes logs to\nBuildLogGroup: Type: AWS::Logs::LogGroup Properties: LogGroupName: frontend_build_on_pull RetentionInDays: 7 RetentionInDays is the period after which the logs expire.\nAccess and Store React Builds After successfully building and testing the project, we should store the build folder on S3 so we can access it again when the developer wants to see his changes. So we create an Artifacts property that stores build on an S3 bucket called “ArtifactBucket”\n... ... Artifacts: Type: S3 Location: !Ref ArtifactBucket Name: FrontPullRequest OverrideArtifactName: true EncryptionDisabled: true ... ... OverrideArtifactName we use to customize the path where we store the builds.\nEncryptionDisabled we disabled it because we don’t need custom encryption in our case, we will be using the default one instead.\nArtifactBucket: Type: 'AWS::S3::Bucket' DeletionPolicy: Delete Properties: BucketName: \u0026quot;some-random-unique-bucket-name\u0026quot; BucketEncryption: ServerSideEncryptionConfiguration: - ServerSideEncryptionByDefault: SSEAlgorithm: AES256 PublicAccessBlockConfiguration: BlockPublicAcls: true BlockPublicPolicy: true IgnorePublicAcls: true RestrictPublicBuckets: true ArtifactBucket is a resource block that refers to AWS::S3::Bucket, also we should add some properties to restrict public access to build folders and define how the data must be encrypted when it gets stored.\nName: you need to change to your unique bucket name, the name must be unique across all AWS buckets.\nBucketEncryption we use Amazon S3 server-side encryption which uses 256-bit Advanced Encryption Standard(AES256).\nPublicAccessBlockConfiguration this property blocks all public access to the public and ignores any policy that allows public visibility of the bucket.\nCodeBuild CI Environment For the CI to work, you must provide information about the CI environment. A build environment represents a combination of the operating system, programming language runtime, and tools that CodeBuild uses to run a build.\n... ... Environment: Type: LINUX_CONTAINER ComputeType: BUILD_GENERAL1_MEDIUM Image: aws/codebuild/amazonlinux2-x86_64-standard:4.0 ... Type this is the OS type, which is Linux for our case.\nComputeType: this provides the CodeBuild with information about the computing resources it’s allowed to use (7 GB Memory,4vCPU,128GB Disk ).\nImage: A docker image to use during the build which provides the environment with a set of tools and runtimes, here is the documentation for better details\nCodeBuild Trigger and Repo Linking To fire up the build process whenever a pull request is created or updated, you need to add two properties: Source, Triggers\n... ... Source: Type: GITHUB Location: \u0026quot;https://github.com/projectX/repoY\u0026quot; ReportBuildStatus: true Triggers: BuildType: BUILD Webhook: true FilterGroups: - - Type: EVENT Pattern: PULL_REQUEST_CREATED,PULL_REQUEST_UPDATED ... ... Source defines the type of version control and the location, you need to change the Location property to your desired repository, on the other side to report the build status back to version control(Github) you turn ReportBuildStatus to true. Triggers specify webhooks that trigger The AWS CodeBuild build.\nTo allow Codebuild access to your repository, you need to create a personal access token and pass it to CodeBuild so it can access your repo on your behalf, This can be done in two steps:\n1- Generate a personal token from your GitHub account:\nVisit the Personal access tokens Github page, then select “Full control of private repositories” and “Full control of repository hooks” and click Generate token.\n2- Pass down the generated token to AWS Codebuild credentials: Run the following command to generate an import-source-credentials.json file on your local :\naws codebuild import-source-credentials --generate-cli-skeleton After, we need to modify import-source-credentials.json and fill it with your credentials:\nauth_type: PERSONAL_ACCESS_TOKEN\nserverType: GITHUB\nusername: your GitHub username\ntoken: the generated token from the previous step.\naws codebuild import-source-credentials \u0026ndash;cli-input-json file://import-source-credentials.json\nTo test that your command runs successfully, run the following command to list your CodeBuild credentials:\naws codebuild list-source-credentials Now when CodeBuild starts running it will search in your source code for a file named buildspec.yml, by definition :\nA buildspec is a collection of build commands and related settings, in YAML format, that CodeBuild uses to run a build.\nHere is an example of buildspec.yml from our demo app :\nversion: 0.2 phases: install: commands: - npm install pre_build: on-failure: ABORT commands: - echo \u0026quot;run some pre-check\u0026quot; #- npm run format:check #- npm run lint:check build: on-failure: ABORT commands: - echo \u0026quot;run tests\u0026quot; # - npm run-script start \u0026amp; npx wait-on http://localhost:3000 # - npx cypress run --record --key ${CYPRESS_KEY} post_build: commands: - | npm run build artifacts: # include all files required to run application # we include only the static build files files: - '**/*' name: $CODEBUILD_WEBHOOK_TRIGGER base-directory: 'dist' Our CI contains different phases (install, pre_build …etc), finally, post_build will run when all the previous steps exited successfully.\npost_build will create a dist folder and then CodeBuild will upload the dist folder to S3 under CODEBUILD_WEBHOOK_TRIGGER path that will be /pr/{pr_number}.\nFor example, if a pull request gets created with the number 1, then after CI builds successfully the build folder will get stored on ArtifactBucket under path /pr/1/.\nCI logs Public Visibility To Users The following property gives developers access to see build logs without having an AWS account :\n... ... Visibility: PUBLIC_READ ResourceAccessRole: !Ref PublicReadRole ... then the PublicReadRole must allow access to logs and S3\nPublicReadRole: Type: AWS::IAM::Role Properties: AssumeRolePolicyDocument: Statement: - Action: ['sts:AssumeRole'] Effect: Allow Principal: Service: [codebuild.amazonaws.com] Version: '2012-10-17' Path: / PublicReadPolicy: Type: 'AWS::IAM::Policy' Properties: PolicyName: PublicBuildPolicy PolicyDocument: Version: \u0026quot;2012-10-17\u0026quot; Statement: - Effect: Allow Action: - \u0026quot;logs:GetLogEvents\u0026quot; Resource: - !Sub \u0026quot;arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:${BuildLogGroup}:*\u0026quot; - Effect: Allow Action: - \u0026quot;s3:GetObject\u0026quot; - \u0026quot;s3:GetObjectVersion\u0026quot; Resource: - Fn::Sub: - \u0026quot;${ArtifcatArn}/*\u0026quot; - {ArtifcatArn: !GetAtt ArtifactBucket.Arn} Roles: - !Ref PublicReadRole Sum It Up After you understand the full picture of CodeBuild Service and its dependency, you can edit pull-request.yml:\nnano pull-request.yml replace its content with the following code\nResources: CodeBuildProject: Type: AWS::CodeBuild::Project Properties: Name: FrontBuildOnPull ServiceRole: !GetAtt BuildProjectRole.Arn LogsConfig: CloudWatchLogs: GroupName: !Ref BuildLogGroup Status: ENABLED StreamName: front_pull_request EncryptionKey: \u0026quot;alias/aws/s3\u0026quot; Artifacts: Type: S3 Location: !Ref ArtifactBucket Name: FrontPullRequest OverrideArtifactName: true EncryptionDisabled: true Environment: Type: LINUX_CONTAINER ComputeType: BUILD_GENERAL1_MEDIUM Image: aws/codebuild/amazonlinux2-x86_64-standard:4.0 Source: Type: GITHUB Location: \u0026quot;https://github.com/projectX/repoY\u0026quot; ReportBuildStatus: true Triggers: BuildType: BUILD Webhook: true FilterGroups: - - Type: EVENT Pattern: PULL_REQUEST_CREATED,PULL_REQUEST_UPDATED Visibility: PUBLIC_READ ResourceAccessRole: !Ref PublicReadRole BuildProjectRole: Type: AWS::IAM::Role Properties: AssumeRolePolicyDocument: Version: '2012-10-17' Statement: - Effect: Allow Principal: Service: - codebuild.amazonaws.com Action: - sts:AssumeRole BuildProjectPolicy: Type: AWS::IAM::Policy DependsOn: BuildProjectRole Properties: PolicyName: !Sub ${AWS::StackName}-CodeBuildPolicy PolicyDocument: Version: '2012-10-17' Statement: - Effect: Allow Action: - codebuild:CreateReportGroup - codebuild:CreateReport - codebuild:UpdateReport - codebuild:BatchPutTestCases - codebuild:BatchPutCodeCoverages # Create and update test report Resource: \u0026quot;*\u0026quot; - Effect: Allow Action: - s3:PutObject - s3:GetObject - s3:GetObjectVersion # Get and store React build folder on S3 Resource: \u0026quot;*\u0026quot; - Effect: Allow Action: - logs:CreateLogGroup - logs:CreateLogStream - logs:PutLogEvents # Store the CI process logs inside a logs group Resource: arn:aws:logs:*:*:* - Effect: Allow Action: - secretsmanager:* # Get secret creedentials ex: CYPRESS_KEY Resource: \u0026quot;*\u0026quot; Roles: - !Ref BuildProjectRole BuildLogGroup: Type: AWS::Logs::LogGroup Properties: LogGroupName: frontend_build_on_pull RetentionInDays: 7 ArtifactBucket: Type: 'AWS::S3::Bucket' DeletionPolicy: Delete Properties: BucketName: \u0026quot;some-random-unique-bucket-name\u0026quot; BucketEncryption: ServerSideEncryptionConfiguration: - ServerSideEncryptionByDefault: SSEAlgorithm: AES256 PublicAccessBlockConfiguration: BlockPublicAcls: true BlockPublicPolicy: true IgnorePublicAcls: true RestrictPublicBuckets: true PublicReadRole: Type: AWS::IAM::Role Properties: AssumeRolePolicyDocument: Statement: - Action: ['sts:AssumeRole'] Effect: Allow Principal: Service: [codebuild.amazonaws.com] Version: '2012-10-17' Path: / PublicReadPolicy: Type: 'AWS::IAM::Policy' Properties: PolicyName: PublicBuildPolicy PolicyDocument: Version: \u0026quot;2012-10-17\u0026quot; Statement: - Effect: Allow Action: - \u0026quot;logs:GetLogEvents\u0026quot; Resource: - !Sub \u0026quot;arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:${BuildLogGroup}:*\u0026quot; - Effect: Allow Action: - \u0026quot;s3:GetObject\u0026quot; - \u0026quot;s3:GetObjectVersion\u0026quot; Resource: - Fn::Sub: - \u0026quot;${ArtifcatArn}/*\u0026quot; - {ArtifcatArn: !GetAtt ArtifactBucket.Arn} Roles: - !Ref PublicReadRole To test our code, move into the pull-request.yml file path and run\naws cloudformation create-stack --stack-name pull-request-preview-stack --template-body file://pull-request.yml --capabilities CAPABILITY_NAMED_IAM Now visit Cloudformation Console and you should see a stack with the name pull-request-preview-stack and its status CREATE_IN_PROGRESS or CREATE_COMPLETE\nServing Content With CloudFront CloudFront is a content delivery network(CDN), CDN can speed up the serving of your static content by caching in the edges that are near you, it is less expensive and it allows you to add a TLS certificate(HTTP) and domain to your static content.\nOn the other side, we don’t need all of these things but what we want from CloudFront is Lambda@Edge which will allow us to redirect requests by the header to right S3 bucket folder. To explain more, let’s suppose we made 2 pull-request by numbers 121,122, after 2 of them are tested and built successfully, they will get stored in a folder named pr on the Artifact Bucket with the following structure:\npr/\n121/\nassets index.html … … 122/\nassets index.html … … Now when Lambda@Edge comes to play, every request that comes to the CloudFront host(which is a random subdomain under CloudFront xxxxx.cloudfront.net) with a pull-request header and value 122 will be redirected to the folder “122” to serve “122” pull request contents.\nIn this section, you will learn how to serve S3 content from a CDN and customize users requests by different criteria like (header,query-params…etc) and also build a trigger whenever a build runs successfully, it clears the cache of a specific pull request on the CDN for the new contents.\nCreating CloudFront Let’s edit pull-request.yml\nnano pull-request.yml and add a CloudFront Distribution Service that will primarily serve index.html from ArtifactBucket\nDistribution: Type: AWS::CloudFront::Distribution Properties: DistributionConfig: Origins: - DomainName: !Sub ${ArtifactBucket}.s3.${AWS::Region}.amazonaws.com Id: S3Origin S3OriginConfig: OriginAccessIdentity: !Sub origin-access-identity/cloudfront/${OriginAccessIdentity} Enabled: true DefaultRootObject: index.html Logging: Bucket: !Sub ${DistributionBucket}.s3.${AWS::Region}.amazonaws.com DefaultCacheBehavior: TargetOriginId: S3Origin CachePolicyId: !Ref DistributionCachingPolicy ViewerProtocolPolicy: https-only PriceClass: PriceClass_100 Cloudfront has 4 major properties that we will explain briefly\nOrigins: This property refers to where CloudFront retrieves the content, we will be adding only Artifact Bucket, also there is a sub-property OriginAccessIdentity that restricts access to ArtifactBucket bucket only from CloudFront.\nDefaultRootObject: The default file that CloudFront will render from the bucket\nPriceClass: To how many regions should our content get cached? we choose PriceClass_100 that’s the minimum value because we don’t care about that.\nLogging: We should collect access logs, So we store them in a bucket “DistributionBucket”\nDefaultCacheBehavior is a required property so we will add it, but we don’t need any caching behavior from CloudFront to test the pull requests. DefaultCacheBehavior contains 3 required attributes:\nTargetOriginId points to one of our Origins which is S3Origin the only origin we have.\nViewerProtocolPolicy controls whether the user is redirected to HTTPS or uses HTTP or both, we chose https-only.\nCachePolicyId , is an important property because we need to allow CloudFront to pass certain headers to Lambda@Edge and the available managed cache policies don’t allow that, so creating a custom cache policy will help. here is the code for the custom cache policy that allows passing the pull-request header to Lambda@Edge.\nDistributionCachingPolicy: Type: AWS::CloudFront::CachePolicy Properties: CachePolicyConfig: Comment: \u0026ldquo;Allow pass of header and query params to cloudfront\u0026rdquo; DefaultTTL: 86400 MaxTTL: 31536000 MinTTL: 80400 Name: CacheForHeaderAndQuery ParametersInCacheKeyAndForwardedToOrigin: CookiesConfig: CookieBehavior: none EnableAcceptEncodingGzip: false HeadersConfig: HeaderBehavior: whitelist Headers: - pull-request QueryStringsConfig: QueryStringBehavior: all\nNow the problem we’re facing is that our pull requests content is stored under the PR folder so we cannot serve them as Cloudfront doesn’t allow dynamic selection of contents. So the solution is rerouting the user request before it reaches the S3 bucket and choosing the right path depending on the custom header(pull-request) he passes on the request.\nChoose The Right Path with Lambda@Edge To add Lambda@Edge to CloudFront we will modify DefaultCacheBehavior to look like this:\nDefaultCacheBehavior: TargetOriginId: S3Origin ViewerProtocolPolicy: redirect-to-https CachePolicyId: !Ref DistributionCachingPolicy LambdaFunctionAssociations: - EventType: 'origin-request' LambdaFunctionARN: !Ref VersionedLambdaFunction Let’s create the lambda function, the programming language will be python :\nLambdaFunction: Type: 'AWS::Lambda::Function' Properties: FunctionName: \u0026quot;cloudfront_lambda\u0026quot; Code: ZipFile: !Sub | import re import boto3 def handler(event, context): client = boto3.client(\u0026quot;s3\u0026quot;) bucket = \u0026quot;front-end-preview-bucket\u0026quot; response = client.list_objects_v2(Bucket=bucket,Prefix=\u0026quot;pr/\u0026quot;,Delimiter=\u0026quot;/\u0026quot;) prs = [p['Prefix'].split('/')[1] for p in response['CommonPrefixes']] request = event['Records'][0]['cf']['request'] print(request) headers = request['headers'] pr = headers.get('pull-request',None) if pr is None: print(f\u0026quot;{pr} not found\u0026quot;) response = { 'status': '404', 'statusDescription': 'No Found', } return response pr = pr[0]['value'] if not pr.isdigit() or not (pr in prs): print(f\u0026quot;{pr} not good\u0026quot;) response = { 'status': '404', 'statusDescription': 'No Found', } return response pr = int(pr) request['uri'] = f\u0026quot;/pr/{pr}{request['uri']}\u0026quot; print(request) return request Handler: 'index.handler' MemorySize: 128 Role: !GetAtt 'LambdaRole.Arn' Runtime: 'python3.9' Timeout: 5 LambdaRole: Type: 'AWS::IAM::Role' Properties: AssumeRolePolicyDocument: Version: '2012-10-17' Statement: - Effect: Allow Principal: Service: - 'lambda.amazonaws.com' - 'edgelambda.amazonaws.com' Action: 'sts:AssumeRole' Policies: - PolicyName: FetchContentFromBucket PolicyDocument: Version: \u0026quot;2012-10-17\u0026quot; Statement: - Effect: Allow Action: - \u0026quot;s3:GetObject\u0026quot; - \u0026quot;s3:ListBucket\u0026quot; - \u0026quot;s3:GetObjectVersion\u0026quot; Resource: - Fn::Sub: - \u0026quot;${ArtifcatArn}/*\u0026quot; - {ArtifcatArn: !GetAtt ArtifactBucket.Arn} ManagedPolicyArns: - 'arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole' - \u0026quot;arn:aws:iam::aws:policy/service-role/AWSLambdaVPCAccessExecutionRole\u0026quot; To summarize what function do, we fetch all current pull requests that are stored on the bucket and whenever a user makes a request, we check the header(pull-request) value and see if it’s available, if yes we rewrite the request URI to “/pr/{pr}{request[‘uri’]}”, if no we return 404 response.\nAlso, we assign Lambda different policies, some are managed policies like :\nAWSLambdaBasicExecutionRole AWSLambdaVPCAccessExecutionRole and others are custom like FetchContentFromBucket to query React build folder from ArtifactBucket.\nA Little Demo After understanding what each service does, we update our pull-request.yml file with the above code. then we update cloud formation with the following command:\naws cloudformation update-stack --stack-name pull-request-preview-stack --template-body file://pull-request.yml --capabilities CAPABILITY_NAMED_IAM Now visit Cloudformation Console and you should see a stack with the name pull-request-preview-stack and its status UPDATE_IN_PROGRESS or UPDATE_COMPLETE.\nTo get the CloudFront endpoint Click on pull-request-preview-stack on Cloudformation Console and Click Outputs\nWhat’s missing now, is creating a pull request in our repository. first, we need a new branch\ngit switch -c feature/test_preview then let’s make some changes and finally add, commit, push\ngit add . git commit -m \u0026quot;it's getting darker :black:\u0026quot; git push --set-upstream origin feature/test_preview Now let’s wait until the tests pass successfully 👀\nIf you visit the CloudFront Endpoint you will get a 404 page, this happens because you didn’t pass a pull-request number as a header. So to achieve this we need to install a chrome extension “mobheader”\nyou replace 271 with your pull request number that you find on the Github pull request page\nRefresh now, Taddaaaa 🎊\nNow, Let\u0026rsquo;s suppose that you made a pull request and requested one of your teammates to make a review for you and he reclaimed something buggy is happening, so you went to investigate the issue and solved it and now you’re pushing the updates. After your CI builds and tests successfully, you visit the CloudFront URL and you find the bug still exists, why ?!.\nWell it’s because CloudFront caches the build folder on its servers and you need to invalidate the cache from the servers then CloudFront will request the files again from S3\nSo the approach will be creating a Lambda Function that gets triggered whenever the builds run successfully, The function takes CloudFront DistributionId as an environment variable and make an invalidation request to /pr/{pr_number} subfolder\nEventCloudFrontLambda: Type: 'AWS::Events::Rule' Properties: Description: Invalidate Cloudfront after a successful build State: ENABLED EventPattern: source: - aws.codebuild detail-type: - CodeBuild Build State Change detail: build-status: - SUCCEEDED project-name: - !Ref CodeBuildProject Targets: - Arn: !GetAtt InvalidateCloudFront.Arn Id: \u0026quot;TargetFunctionV1\u0026quot; InvalidateCloudFront: Type: 'AWS::Lambda::Function' Properties: FunctionName: \u0026quot;invalidate_cloudfront_from_codebuild_lambda\u0026quot; Environment: Variables: DistributionId: !GetAtt Distribution.Id Code: ZipFile: !Sub | import boto3 import uuid import os def handler(event, context): print(event) artifict = event['detail']['additional-information']['artifact']['location'] pr = artifict.split(\u0026quot;/\u0026quot;)[2] distribution_id = os.getenv(\u0026quot;DistributionId\u0026quot;) print(distribution_id) client = boto3.client(\u0026quot;cloudfront\u0026quot;) response = client.create_invalidation( DistributionId=distribution_id, InvalidationBatch={ 'Paths': { 'Quantity': 1, 'Items': [ f'/pr/{pr}/*', ] }, 'CallerReference': str(uuid.uuid4()) } ) return {\u0026quot;status\u0026quot;:200} Handler: 'index.handler' MemorySize: 128 Role: !GetAtt 'LambdaRole.Arn' Runtime: 'python3.9' Timeout: 15 InvalidateCloudFrontLogs: Type: AWS::Logs::LogGroup DependsOn: InvalidateCloudFront Properties: LogGroupName: !Sub \u0026quot;/aws/lambda/${InvalidateCloudFront}\u0026quot; RetentionInDays: 7 PermissionForEventsToInvokeLambda: Type: AWS::Lambda::Permission Properties: FunctionName: !Ref InvalidateCloudFront Action: \u0026quot;lambda:InvokeFunction\u0026quot; Principal: \u0026quot;events.amazonaws.com\u0026quot; SourceArn: !GetAtt EventCloudFrontLambda.Arn Test Updating The Pull-request Code Let’s make some changes, if you cloned my repository you can change the index.json file\nand replace “Hello World War 3! with “Hello World Peace”\nand let’s wait for the builds to run successfully and recheck again our preview\nChallenge For You As our pull request creates a directory on S3, it is a waste of storage and money if we leave the directory on S3 after merging the pull request.\nSo the challenge will be creating a GitHub action workflow that will delete the folder from S3 after merging the pull request. one of the requirements is using AWS OpenID Connect.\nDon’t hesitate to email me, It will be a pleasure for me to review your work 😊\nSummary In this article, we walked into different AWS Services(CodeBuild, Cloudfront,S3), we understand the mechanism of AWS IAM finally we learned how to create and deploy our services with Cloudformation\nThanks for your time, stay tuned for new articles.\n","permalink":"//localhost:1313/posts/deployment-preview-with-aws-cloudfront/","summary":"Introduction Deploy Previews allow you and your team to experience changes to any part of your site without having to publish them to production.\nWith a deploy previews feature you and your teammates can see the changes of every pull request you make without merging it, this will reduce the burden of rolling back the environment when bugs happen as you can review the changes before.\nIn this tutorial, you’ll learn about creating a CI pipeline with CodeBuild that gets triggered on every pull request creation or update, for every build we host react build folder on an S3 bucket and serve it with Cloudfront, finally after merging the pull request, we delete the build folder from S3.","title":"Deployment Preview with AWS CloudFront"}]